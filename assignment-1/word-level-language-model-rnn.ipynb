{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "word-level-language-model-rnn.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alanwuha/ce7455-nlp/blob/master/assignment-1/word-level-language-model-rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIaIZE6es3jc",
        "colab_type": "text"
      },
      "source": [
        "# Question Two\n",
        "\n",
        "Please make sure to run all cells in order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXyEv8NOs-rl",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsyBbtWr0ZaX",
        "colab_type": "text"
      },
      "source": [
        "### Download wikitext-2 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oHSqqxz0c4Q",
        "colab_type": "code",
        "outputId": "b4e6da32-6bb1-47df-8fce-de447578ef2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!rm -r data\n",
        "!mkdir data\n",
        "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip -P /content/data\n",
        "!unzip /content/data/wikitext-2-v1.zip -d /content/data"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-08 09:00:42--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.88.37\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.88.37|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4475746 (4.3M) [application/zip]\n",
            "Saving to: ‘/content/data/wikitext-2-v1.zip’\n",
            "\n",
            "wikitext-2-v1.zip   100%[===================>]   4.27M  2.46MB/s    in 1.7s    \n",
            "\n",
            "2020-02-08 09:00:45 (2.46 MB/s) - ‘/content/data/wikitext-2-v1.zip’ saved [4475746/4475746]\n",
            "\n",
            "Archive:  /content/data/wikitext-2-v1.zip\n",
            "   creating: /content/data/wikitext-2/\n",
            "  inflating: /content/data/wikitext-2/wiki.test.tokens  \n",
            "  inflating: /content/data/wikitext-2/wiki.valid.tokens  \n",
            "  inflating: /content/data/wikitext-2/wiki.train.tokens  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5fcltKuqDoR",
        "colab_type": "text"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqiyNq_4qGkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.onnx\n",
        "import argparse\n",
        "import time\n",
        "import os\n",
        "from io import open"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCOTNafMg_z5",
        "colab_type": "text"
      },
      "source": [
        "### Specify arg variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrEgZw42hCv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = './data/wikitext-2'  # location of the data corpus\n",
        "args_model = 'FNN'          # type of recurrent net\n",
        "emsize = 200                # size of word embeddings\n",
        "nhid = 200                  # number of hidden units per layer\n",
        "nlayers = 2                 # number of layers\n",
        "lr = 20                     # initial learning rate\n",
        "clip = 0.25                 # gradient clipping\n",
        "epochs = 40                 # upper epoch limit\n",
        "batch_size = 20             # batch size\n",
        "bptt = 35                   # sequence length\n",
        "dropout = 0.2               # dropout applied to layers (0 = no dropout)\n",
        "tied = False                # tie the word embedding and softmax weights\n",
        "seed = 1111                 # random seed\n",
        "cuda = True                 # use CUDA\n",
        "log_interval = 200          # report interval\n",
        "save = 'model.pt'           # path to save the final model\n",
        "onnx_export = ''            # path to export the final model in onnx format\n",
        "nhead = 2                   # the number of heads in the encoder/decoder of the transformer model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M0KYJXn0htA",
        "colab_type": "text"
      },
      "source": [
        "### Data Preprocessing Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_krRf2j0kzQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'wiki.train.tokens'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'wiki.valid.tokens'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'wiki.test.tokens'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            idss = []\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                ids = []\n",
        "                for word in words:\n",
        "                    ids.append(self.dictionary.word2idx[word])\n",
        "                idss.append(torch.tensor(ids).type(torch.int64))\n",
        "            ids = torch.cat(idss)\n",
        "\n",
        "        return ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNA9aDE-0VhB",
        "colab_type": "text"
      },
      "source": [
        "### Helper Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVk0qOGJo8dg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "\n",
        "# Starting from sequential data, batchify arranges the dataset into columns.\n",
        "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
        "# ┌ a g m s ┐\n",
        "# │ b h n t │\n",
        "# │ c i o u │\n",
        "# │ d j p v │\n",
        "# │ e k q w │\n",
        "# └ f l r x ┘.\n",
        "# These columns are treated as independent by the model, which means that the\n",
        "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
        "# batch processing.\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "\n",
        "# get_batch subdivides the source data into chunks of length bptt.\n",
        "# If source is equal to the example output of the batchify function, with\n",
        "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
        "# ┌ a g m s ┐ ┌ b h n t ┐\n",
        "# └ b h n t ┘ └ c i o u ┘\n",
        "# Note that despite the name of the function, the subdivison of data is not\n",
        "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
        "# by the batchify function. The chunks are along dimension 0, corresponding\n",
        "# to the seq_len dimension in the LSTM.\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    if args_model not in ['Transformer', 'FNN']:\n",
        "        hidden = model.init_hidden(eval_batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            if args_model in ['Transformer', 'FNN']:\n",
        "                output = model(data)\n",
        "            else:\n",
        "                output, hidden = model(data, hidden)\n",
        "                hidden = repackage_hidden(hidden)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    if args_model not in ['Transformer', 'FNN']:\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        # model.zero_grad()\n",
        "        \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output\n",
        "        if args_model in ['Transformer', 'FNN']:\n",
        "            output = model(data)\n",
        "        else:\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            output, hidden = model(data, hidden)\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        # for p in model.parameters():\n",
        "        #     p.data.add_(-lr, p.grad.data)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.3f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // bptt, optimizer.param_groups[0]['lr'],\n",
        "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "\n",
        "def export_onnx(path, batch_size, seq_len):\n",
        "    print('The model is also exported in ONNX format at {}'.\n",
        "          format(os.path.realpath(args.onnx_export)))\n",
        "    model.eval()\n",
        "    dummy_input = torch.LongTensor(seq_len * batch_size).zero_().view(-1, batch_size).to(device)\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    torch.onnx.export(model, (dummy_input, hidden), path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-w17AZJp2qf",
        "colab_type": "text"
      },
      "source": [
        "### Set torch seed and CUDA device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJGy7JGHqe8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Set cuda device\n",
        "if torch.cuda.is_available():\n",
        "    if not cuda:\n",
        "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GY7McFzqnaR",
        "colab_type": "text"
      },
      "source": [
        "### Load corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig6BmYbWqq0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = Corpus(data)\n",
        "\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(corpus.train, batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "ntokens = len(corpus.dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSxxbklN0moQ",
        "colab_type": "text"
      },
      "source": [
        "## (iii) Write a _class FNNModel(nn.Module)_.\n",
        "\n",
        "The FNNModel should implement a language model with a feed-forward network architecture. It has a hidden layer with tanh architecture and the output layer is a Softmax layer. The output of the model for each input of (n-1) previous word indices are the probabilities of the |_V_| words in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSAjXOz71l80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FNNModel(nn.Module):\n",
        "    \"\"\" Container module with an encoder, a feed forward module, and a decoder. \"\"\"\n",
        "\n",
        "    def __init__(self, ntokens, emsize, hidden_size, tie_weights=False):\n",
        "        super(FNNModel, self).__init__()\n",
        "        self.encoder = nn.Embedding(ntokens, emsize)\n",
        "        self.hidden = nn.Linear(emsize, hidden_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.decoder = nn.Linear(hidden_size, ntokens)\n",
        "\n",
        "        if tie_weights:\n",
        "            if hidden_size != emsize:\n",
        "                raise ValueError('When using the tied flag, hidden_size must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        # self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input):\n",
        "        emb = self.encoder(input)\n",
        "        hid = self.hidden(emb)\n",
        "        output = self.tanh(hid)\n",
        "        decoded = self.decoder(output)\n",
        "        return decoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvvSa_5Sqtyp",
        "colab_type": "text"
      },
      "source": [
        "## (iv-1) Train the model with Adam optimizer (without sharing weights)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipzOYeQy0W0a",
        "colab_type": "code",
        "outputId": "a309aa6f-6f00-4c3b-c986-6eeaf4f582d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tied = False\n",
        "model = FNNModel(ntokens, emsize, nhid, tied).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "# Loop over epochs.\n",
        "best_val_loss = None\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 2983 batches | lr 0.001 | ms/batch 18.42 | loss  8.08 | ppl  3234.78\n",
            "| epoch   1 |   400/ 2983 batches | lr 0.001 | ms/batch 18.14 | loss  6.59 | ppl   728.55\n",
            "| epoch   1 |   600/ 2983 batches | lr 0.001 | ms/batch 18.25 | loss  6.38 | ppl   592.88\n",
            "| epoch   1 |   800/ 2983 batches | lr 0.001 | ms/batch 18.35 | loss  6.31 | ppl   549.91\n",
            "| epoch   1 |  1000/ 2983 batches | lr 0.001 | ms/batch 18.47 | loss  6.23 | ppl   507.14\n",
            "| epoch   1 |  1200/ 2983 batches | lr 0.001 | ms/batch 18.56 | loss  6.22 | ppl   503.70\n",
            "| epoch   1 |  1400/ 2983 batches | lr 0.001 | ms/batch 18.74 | loss  6.20 | ppl   490.41\n",
            "| epoch   1 |  1600/ 2983 batches | lr 0.001 | ms/batch 18.85 | loss  6.18 | ppl   485.13\n",
            "| epoch   1 |  1800/ 2983 batches | lr 0.001 | ms/batch 18.92 | loss  6.07 | ppl   430.87\n",
            "| epoch   1 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.05 | loss  6.10 | ppl   444.95\n",
            "| epoch   1 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.58 | loss  6.00 | ppl   401.94\n",
            "| epoch   1 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.92 | loss  6.03 | ppl   414.26\n",
            "| epoch   1 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.09 | loss  6.01 | ppl   408.67\n",
            "| epoch   1 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  5.95 | ppl   382.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 59.09s | valid loss  5.89 | valid ppl   361.36\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type FNNModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   2 |   200/ 2983 batches | lr 0.001 | ms/batch 20.37 | loss  5.73 | ppl   307.82\n",
            "| epoch   2 |   400/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  5.68 | ppl   292.37\n",
            "| epoch   2 |   600/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.56 | ppl   258.84\n",
            "| epoch   2 |   800/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  5.59 | ppl   268.52\n",
            "| epoch   2 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  5.56 | ppl   259.41\n",
            "| epoch   2 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  5.58 | ppl   264.33\n",
            "| epoch   2 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  5.61 | ppl   272.79\n",
            "| epoch   2 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  5.61 | ppl   271.92\n",
            "| epoch   2 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.23 | loss  5.52 | ppl   250.46\n",
            "| epoch   2 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.09 | loss  5.58 | ppl   264.73\n",
            "| epoch   2 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.61 | loss  5.48 | ppl   239.13\n",
            "| epoch   2 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.83 | loss  5.51 | ppl   247.68\n",
            "| epoch   2 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.99 | loss  5.53 | ppl   251.63\n",
            "| epoch   2 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.18 | loss  5.48 | ppl   239.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 62.20s | valid loss  5.83 | valid ppl   338.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2983 batches | lr 0.001 | ms/batch 19.69 | loss  5.41 | ppl   223.24\n",
            "| epoch   3 |   400/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.38 | ppl   216.74\n",
            "| epoch   3 |   600/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.27 | ppl   193.82\n",
            "| epoch   3 |   800/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  5.32 | ppl   205.20\n",
            "| epoch   3 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  5.30 | ppl   200.52\n",
            "| epoch   3 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.23 | loss  5.32 | ppl   204.58\n",
            "| epoch   3 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.23 | loss  5.36 | ppl   212.01\n",
            "| epoch   3 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.36 | ppl   212.31\n",
            "| epoch   3 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.29 | ppl   198.04\n",
            "| epoch   3 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.34 | ppl   208.98\n",
            "| epoch   3 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.24 | ppl   188.86\n",
            "| epoch   3 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.28 | ppl   195.56\n",
            "| epoch   3 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  5.30 | ppl   200.25\n",
            "| epoch   3 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  5.25 | ppl   190.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 62.35s | valid loss  5.83 | valid ppl   341.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/ 2983 batches | lr 0.001 | ms/batch 20.30 | loss  5.22 | ppl   185.65\n",
            "| epoch   4 |   400/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  5.21 | ppl   183.06\n",
            "| epoch   4 |   600/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.10 | ppl   164.46\n",
            "| epoch   4 |   800/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.16 | ppl   174.76\n",
            "| epoch   4 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  5.15 | ppl   172.37\n",
            "| epoch   4 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  5.17 | ppl   175.62\n",
            "| epoch   4 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  5.20 | ppl   181.77\n",
            "| epoch   4 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.17 | loss  5.21 | ppl   182.61\n",
            "| epoch   4 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.15 | ppl   172.12\n",
            "| epoch   4 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.20 | ppl   180.78\n",
            "| epoch   4 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  5.10 | ppl   163.62\n",
            "| epoch   4 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.23 | loss  5.13 | ppl   169.10\n",
            "| epoch   4 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.23 | loss  5.16 | ppl   174.01\n",
            "| epoch   4 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.24 | loss  5.11 | ppl   165.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 62.48s | valid loss  5.86 | valid ppl   349.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/ 2983 batches | lr 0.001 | ms/batch 20.29 | loss  5.10 | ppl   164.76\n",
            "| epoch   5 |   400/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.10 | ppl   163.65\n",
            "| epoch   5 |   600/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.99 | ppl   147.61\n",
            "| epoch   5 |   800/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  5.06 | ppl   156.90\n",
            "| epoch   5 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  5.05 | ppl   155.55\n",
            "| epoch   5 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  5.07 | ppl   158.41\n",
            "| epoch   5 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.18 | loss  5.10 | ppl   163.67\n",
            "| epoch   5 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  5.10 | ppl   164.63\n",
            "| epoch   5 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  5.05 | ppl   156.43\n",
            "| epoch   5 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  5.10 | ppl   163.64\n",
            "| epoch   5 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.00 | ppl   148.30\n",
            "| epoch   5 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.03 | ppl   153.14\n",
            "| epoch   5 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  5.06 | ppl   158.00\n",
            "| epoch   5 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  5.02 | ppl   150.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 62.45s | valid loss  5.89 | valid ppl   359.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/ 2983 batches | lr 0.001 | ms/batch 20.26 | loss  5.02 | ppl   151.67\n",
            "| epoch   6 |   400/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  5.02 | ppl   150.96\n",
            "| epoch   6 |   600/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.92 | ppl   136.67\n",
            "| epoch   6 |   800/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.98 | ppl   145.26\n",
            "| epoch   6 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.97 | ppl   144.32\n",
            "| epoch   6 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.99 | ppl   147.11\n",
            "| epoch   6 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.02 | ppl   151.65\n",
            "| epoch   6 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  5.03 | ppl   152.65\n",
            "| epoch   6 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.98 | ppl   145.83\n",
            "| epoch   6 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.18 | loss  5.03 | ppl   152.22\n",
            "| epoch   6 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.93 | ppl   138.11\n",
            "| epoch   6 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.96 | ppl   142.50\n",
            "| epoch   6 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.99 | ppl   147.27\n",
            "| epoch   6 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.95 | ppl   140.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 62.47s | valid loss  5.91 | valid ppl   369.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/ 2983 batches | lr 0.001 | ms/batch 20.33 | loss  4.96 | ppl   142.69\n",
            "| epoch   7 |   400/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.96 | ppl   141.96\n",
            "| epoch   7 |   600/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.86 | ppl   128.99\n",
            "| epoch   7 |   800/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.92 | ppl   137.05\n",
            "| epoch   7 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  4.92 | ppl   136.38\n",
            "| epoch   7 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.94 | ppl   139.10\n",
            "| epoch   7 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.96 | ppl   143.08\n",
            "| epoch   7 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.97 | ppl   144.12\n",
            "| epoch   7 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.93 | ppl   138.17\n",
            "| epoch   7 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.97 | ppl   144.06\n",
            "| epoch   7 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.87 | ppl   130.86\n",
            "| epoch   7 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.90 | ppl   134.93\n",
            "| epoch   7 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.94 | ppl   139.63\n",
            "| epoch   7 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.89 | ppl   133.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 62.50s | valid loss  5.94 | valid ppl   380.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/ 2983 batches | lr 0.001 | ms/batch 20.31 | loss  4.91 | ppl   136.14\n",
            "| epoch   8 |   400/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.91 | ppl   135.27\n",
            "| epoch   8 |   600/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.81 | ppl   123.27\n",
            "| epoch   8 |   800/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.87 | ppl   130.93\n",
            "| epoch   8 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.87 | ppl   130.45\n",
            "| epoch   8 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.89 | ppl   133.04\n",
            "| epoch   8 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.92 | ppl   136.74\n",
            "| epoch   8 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.93 | ppl   137.78\n",
            "| epoch   8 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.89 | ppl   132.39\n",
            "| epoch   8 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.93 | ppl   137.84\n",
            "| epoch   8 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.83 | ppl   125.39\n",
            "| epoch   8 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.86 | ppl   129.29\n",
            "| epoch   8 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.90 | ppl   133.87\n",
            "| epoch   8 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  4.85 | ppl   128.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 62.49s | valid loss  5.97 | valid ppl   390.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/ 2983 batches | lr 0.001 | ms/batch 20.32 | loss  4.88 | ppl   131.15\n",
            "| epoch   9 |   400/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.87 | ppl   130.06\n",
            "| epoch   9 |   600/ 2983 batches | lr 0.001 | ms/batch 20.23 | loss  4.78 | ppl   118.86\n",
            "| epoch   9 |   800/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.84 | ppl   126.20\n",
            "| epoch   9 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.84 | ppl   125.85\n",
            "| epoch   9 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  4.85 | ppl   128.31\n",
            "| epoch   9 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.88 | ppl   131.83\n",
            "| epoch   9 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  4.89 | ppl   132.82\n",
            "| epoch   9 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.85 | ppl   127.90\n",
            "| epoch   9 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.89 | ppl   133.01\n",
            "| epoch   9 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.80 | ppl   121.12\n",
            "| epoch   9 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.83 | ppl   124.93\n",
            "| epoch   9 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.86 | ppl   129.39\n",
            "| epoch   9 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.82 | ppl   123.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 62.47s | valid loss  5.99 | valid ppl   399.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/ 2983 batches | lr 0.001 | ms/batch 20.32 | loss  4.85 | ppl   127.23\n",
            "| epoch  10 |   400/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.84 | ppl   125.95\n",
            "| epoch  10 |   600/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.75 | ppl   115.35\n",
            "| epoch  10 |   800/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.81 | ppl   122.43\n",
            "| epoch  10 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.81 | ppl   122.19\n",
            "| epoch  10 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  4.82 | ppl   124.51\n",
            "| epoch  10 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.85 | ppl   127.84\n",
            "| epoch  10 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.86 | ppl   128.92\n",
            "| epoch  10 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  4.82 | ppl   124.31\n",
            "| epoch  10 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.86 | ppl   129.04\n",
            "| epoch  10 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.77 | ppl   117.71\n",
            "| epoch  10 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.80 | ppl   121.46\n",
            "| epoch  10 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.83 | ppl   125.78\n",
            "| epoch  10 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.79 | ppl   120.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 62.47s | valid loss  6.01 | valid ppl   407.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |   200/ 2983 batches | lr 0.001 | ms/batch 20.34 | loss  4.82 | ppl   124.07\n",
            "| epoch  11 |   400/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.81 | ppl   122.61\n",
            "| epoch  11 |   600/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.72 | ppl   112.52\n",
            "| epoch  11 |   800/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.78 | ppl   119.36\n",
            "| epoch  11 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.23 | loss  4.78 | ppl   119.21\n",
            "| epoch  11 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.80 | ppl   121.40\n",
            "| epoch  11 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.82 | ppl   124.56\n",
            "| epoch  11 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.83 | ppl   125.73\n",
            "| epoch  11 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.80 | ppl   121.40\n",
            "| epoch  11 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.84 | ppl   125.84\n",
            "| epoch  11 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  4.74 | ppl   114.88\n",
            "| epoch  11 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.78 | ppl   118.62\n",
            "| epoch  11 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.81 | ppl   122.81\n",
            "| epoch  11 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.77 | ppl   117.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 62.47s | valid loss  6.03 | valid ppl   415.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |   200/ 2983 batches | lr 0.001 | ms/batch 20.31 | loss  4.80 | ppl   121.46\n",
            "| epoch  12 |   400/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.79 | ppl   119.88\n",
            "| epoch  12 |   600/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.70 | ppl   110.14\n",
            "| epoch  12 |   800/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.76 | ppl   116.82\n",
            "| epoch  12 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.76 | ppl   116.73\n",
            "| epoch  12 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.78 | ppl   118.80\n",
            "| epoch  12 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.80 | ppl   121.81\n",
            "| epoch  12 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.18 | loss  4.81 | ppl   123.17\n",
            "| epoch  12 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.78 | ppl   118.98\n",
            "| epoch  12 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  4.81 | ppl   123.16\n",
            "| epoch  12 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.72 | ppl   112.52\n",
            "| epoch  12 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  4.76 | ppl   116.24\n",
            "| epoch  12 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.79 | ppl   120.32\n",
            "| epoch  12 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.75 | ppl   115.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 62.46s | valid loss  6.05 | valid ppl   423.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |   200/ 2983 batches | lr 0.001 | ms/batch 20.28 | loss  4.78 | ppl   119.26\n",
            "| epoch  13 |   400/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.77 | ppl   117.59\n",
            "| epoch  13 |   600/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  4.68 | ppl   108.12\n",
            "| epoch  13 |   800/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  4.74 | ppl   114.70\n",
            "| epoch  13 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.74 | ppl   114.63\n",
            "| epoch  13 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.76 | ppl   116.63\n",
            "| epoch  13 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.78 | ppl   119.49\n",
            "| epoch  13 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.24 | loss  4.80 | ppl   120.97\n",
            "| epoch  13 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.76 | ppl   116.93\n",
            "| epoch  13 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  4.80 | ppl   120.96\n",
            "| epoch  13 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  4.71 | ppl   110.50\n",
            "| epoch  13 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.74 | ppl   114.18\n",
            "| epoch  13 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.77 | ppl   118.19\n",
            "| epoch  13 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.18 | loss  4.73 | ppl   113.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 62.45s | valid loss  6.07 | valid ppl   430.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |   200/ 2983 batches | lr 0.001 | ms/batch 20.30 | loss  4.77 | ppl   117.36\n",
            "| epoch  14 |   400/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.75 | ppl   115.68\n",
            "| epoch  14 |   600/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.67 | ppl   106.35\n",
            "| epoch  14 |   800/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.73 | ppl   112.90\n",
            "| epoch  14 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.73 | ppl   112.82\n",
            "| epoch  14 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  4.74 | ppl   114.78\n",
            "| epoch  14 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.77 | ppl   117.50\n",
            "| epoch  14 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.78 | ppl   119.14\n",
            "| epoch  14 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.75 | ppl   115.16\n",
            "| epoch  14 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.78 | ppl   119.05\n",
            "| epoch  14 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.69 | ppl   108.81\n",
            "| epoch  14 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.72 | ppl   112.38\n",
            "| epoch  14 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.76 | ppl   116.35\n",
            "| epoch  14 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.72 | ppl   111.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 62.49s | valid loss  6.08 | valid ppl   437.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |   200/ 2983 batches | lr 0.001 | ms/batch 20.30 | loss  4.75 | ppl   115.72\n",
            "| epoch  15 |   400/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.74 | ppl   114.02\n",
            "| epoch  15 |   600/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.65 | ppl   104.83\n",
            "| epoch  15 |   800/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.71 | ppl   111.33\n",
            "| epoch  15 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.71 | ppl   111.26\n",
            "| epoch  15 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.18 | loss  4.73 | ppl   113.19\n",
            "| epoch  15 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.11 | loss  4.75 | ppl   115.75\n",
            "| epoch  15 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.77 | ppl   117.49\n",
            "| epoch  15 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.73 | ppl   113.62\n",
            "| epoch  15 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.77 | ppl   117.45\n",
            "| epoch  15 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.68 | ppl   107.32\n",
            "| epoch  15 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  4.71 | ppl   110.79\n",
            "| epoch  15 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.74 | ppl   114.72\n",
            "| epoch  15 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  4.70 | ppl   110.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 62.43s | valid loss  6.10 | valid ppl   444.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |   200/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.74 | ppl   114.25\n",
            "| epoch  16 |   400/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  4.72 | ppl   112.62\n",
            "| epoch  16 |   600/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.64 | ppl   103.47\n",
            "| epoch  16 |   800/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.70 | ppl   109.93\n",
            "| epoch  16 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.70 | ppl   109.89\n",
            "| epoch  16 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.72 | ppl   111.80\n",
            "| epoch  16 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.74 | ppl   114.23\n",
            "| epoch  16 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.23 | loss  4.75 | ppl   116.09\n",
            "| epoch  16 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.72 | ppl   112.24\n",
            "| epoch  16 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.75 | ppl   116.00\n",
            "| epoch  16 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.66 | ppl   106.08\n",
            "| epoch  16 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.69 | ppl   109.40\n",
            "| epoch  16 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.73 | ppl   113.29\n",
            "| epoch  16 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.69 | ppl   108.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 62.46s | valid loss  6.11 | valid ppl   451.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |   200/ 2983 batches | lr 0.001 | ms/batch 20.31 | loss  4.73 | ppl   112.96\n",
            "| epoch  17 |   400/ 2983 batches | lr 0.001 | ms/batch 20.23 | loss  4.71 | ppl   111.38\n",
            "| epoch  17 |   600/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.63 | ppl   102.28\n",
            "| epoch  17 |   800/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.69 | ppl   108.68\n",
            "| epoch  17 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.69 | ppl   108.70\n",
            "| epoch  17 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.71 | ppl   110.59\n",
            "| epoch  17 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.23 | loss  4.73 | ppl   112.89\n",
            "| epoch  17 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.74 | ppl   114.77\n",
            "| epoch  17 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.71 | ppl   111.02\n",
            "| epoch  17 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.74 | ppl   114.78\n",
            "| epoch  17 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.23 | loss  4.65 | ppl   104.97\n",
            "| epoch  17 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.68 | ppl   108.17\n",
            "| epoch  17 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.72 | ppl   112.02\n",
            "| epoch  17 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.68 | ppl   107.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 62.52s | valid loss  6.13 | valid ppl   458.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |   200/ 2983 batches | lr 0.001 | ms/batch 20.26 | loss  4.72 | ppl   111.80\n",
            "| epoch  18 |   400/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.70 | ppl   110.31\n",
            "| epoch  18 |   600/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  4.62 | ppl   101.22\n",
            "| epoch  18 |   800/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.68 | ppl   107.58\n",
            "| epoch  18 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  4.68 | ppl   107.60\n",
            "| epoch  18 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.70 | ppl   109.54\n",
            "| epoch  18 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.72 | ppl   111.72\n",
            "| epoch  18 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.73 | ppl   113.62\n",
            "| epoch  18 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.70 | ppl   109.93\n",
            "| epoch  18 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.73 | ppl   113.61\n",
            "| epoch  18 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.64 | ppl   104.04\n",
            "| epoch  18 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.67 | ppl   107.08\n",
            "| epoch  18 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.71 | ppl   110.91\n",
            "| epoch  18 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.67 | ppl   106.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 62.46s | valid loss  6.14 | valid ppl   464.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |   200/ 2983 batches | lr 0.001 | ms/batch 20.24 | loss  4.71 | ppl   110.79\n",
            "| epoch  19 |   400/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  4.69 | ppl   109.32\n",
            "| epoch  19 |   600/ 2983 batches | lr 0.001 | ms/batch 20.25 | loss  4.61 | ppl   100.29\n",
            "| epoch  19 |   800/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.67 | ppl   106.59\n",
            "| epoch  19 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.67 | ppl   106.63\n",
            "| epoch  19 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  4.69 | ppl   108.65\n",
            "| epoch  19 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.71 | ppl   110.68\n",
            "| epoch  19 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  4.72 | ppl   112.51\n",
            "| epoch  19 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.69 | ppl   108.95\n",
            "| epoch  19 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  4.72 | ppl   112.63\n",
            "| epoch  19 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  4.64 | ppl   103.18\n",
            "| epoch  19 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.66 | ppl   106.13\n",
            "| epoch  19 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.70 | ppl   109.92\n",
            "| epoch  19 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.66 | ppl   105.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "Exiting from training early\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIGOGKJAoC1j",
        "colab_type": "text"
      },
      "source": [
        "## (v-1) Show the perplexity score on the test set. You should select your best model based on the _valid_ set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v-ZA9MVoMsl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0b6f3a42-e6bd-4085-f589-daf220734858"
      },
      "source": [
        "# Load the best saved model.\n",
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  5.73 | test ppl   308.45\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac5xAGvBrtsR",
        "colab_type": "text"
      },
      "source": [
        "## (iv-2) Train the model with Adam optimizer, but now with sharing the input (look-up matrix) and output layer embeddings (final layer weights)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI3kSSpfrxxK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5c2fd9dd-d7f3-4c93-f693-d7e67ff31dca"
      },
      "source": [
        "tied = True\n",
        "model = FNNModel(ntokens, emsize, nhid, tied).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "# Loop over epochs.\n",
        "best_val_loss = None\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 2983 batches | lr 0.001 | ms/batch 17.35 | loss 14.22 | ppl 1502783.63\n",
            "| epoch   1 |   400/ 2983 batches | lr 0.001 | ms/batch 17.25 | loss  8.03 | ppl  3073.87\n",
            "| epoch   1 |   600/ 2983 batches | lr 0.001 | ms/batch 17.44 | loss  7.31 | ppl  1488.76\n",
            "| epoch   1 |   800/ 2983 batches | lr 0.001 | ms/batch 17.68 | loss  7.13 | ppl  1243.80\n",
            "| epoch   1 |  1000/ 2983 batches | lr 0.001 | ms/batch 17.86 | loss  7.00 | ppl  1101.65\n",
            "| epoch   1 |  1200/ 2983 batches | lr 0.001 | ms/batch 18.14 | loss  6.97 | ppl  1067.11\n",
            "| epoch   1 |  1400/ 2983 batches | lr 0.001 | ms/batch 18.32 | loss  6.91 | ppl  1003.37\n",
            "| epoch   1 |  1600/ 2983 batches | lr 0.001 | ms/batch 18.61 | loss  6.89 | ppl   986.13\n",
            "| epoch   1 |  1800/ 2983 batches | lr 0.001 | ms/batch 18.84 | loss  6.79 | ppl   885.45\n",
            "| epoch   1 |  2000/ 2983 batches | lr 0.001 | ms/batch 18.79 | loss  6.80 | ppl   899.18\n",
            "| epoch   1 |  2200/ 2983 batches | lr 0.001 | ms/batch 18.40 | loss  6.71 | ppl   820.16\n",
            "| epoch   1 |  2400/ 2983 batches | lr 0.001 | ms/batch 18.11 | loss  6.71 | ppl   821.80\n",
            "| epoch   1 |  2600/ 2983 batches | lr 0.001 | ms/batch 17.98 | loss  6.70 | ppl   809.18\n",
            "| epoch   1 |  2800/ 2983 batches | lr 0.001 | ms/batch 17.81 | loss  6.62 | ppl   751.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 55.94s | valid loss  6.53 | valid ppl   688.00\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type FNNModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   2 |   200/ 2983 batches | lr 0.001 | ms/batch 17.51 | loss  6.54 | ppl   692.33\n",
            "| epoch   2 |   400/ 2983 batches | lr 0.001 | ms/batch 17.42 | loss  6.47 | ppl   644.98\n",
            "| epoch   2 |   600/ 2983 batches | lr 0.001 | ms/batch 17.42 | loss  6.36 | ppl   579.72\n",
            "| epoch   2 |   800/ 2983 batches | lr 0.001 | ms/batch 17.32 | loss  6.40 | ppl   599.72\n",
            "| epoch   2 |  1000/ 2983 batches | lr 0.001 | ms/batch 17.34 | loss  6.36 | ppl   576.65\n",
            "| epoch   2 |  1200/ 2983 batches | lr 0.001 | ms/batch 17.29 | loss  6.38 | ppl   588.80\n",
            "| epoch   2 |  1400/ 2983 batches | lr 0.001 | ms/batch 17.33 | loss  6.38 | ppl   589.49\n",
            "| epoch   2 |  1600/ 2983 batches | lr 0.001 | ms/batch 17.31 | loss  6.38 | ppl   590.09\n",
            "| epoch   2 |  1800/ 2983 batches | lr 0.001 | ms/batch 17.41 | loss  6.28 | ppl   532.73\n",
            "| epoch   2 |  2000/ 2983 batches | lr 0.001 | ms/batch 17.47 | loss  6.32 | ppl   553.87\n",
            "| epoch   2 |  2200/ 2983 batches | lr 0.001 | ms/batch 17.52 | loss  6.23 | ppl   507.36\n",
            "| epoch   2 |  2400/ 2983 batches | lr 0.001 | ms/batch 17.55 | loss  6.25 | ppl   519.89\n",
            "| epoch   2 |  2600/ 2983 batches | lr 0.001 | ms/batch 17.59 | loss  6.25 | ppl   518.43\n",
            "| epoch   2 |  2800/ 2983 batches | lr 0.001 | ms/batch 17.71 | loss  6.19 | ppl   490.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 54.30s | valid loss  6.26 | valid ppl   524.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2983 batches | lr 0.001 | ms/batch 17.74 | loss  6.21 | ppl   500.05\n",
            "| epoch   3 |   400/ 2983 batches | lr 0.001 | ms/batch 17.86 | loss  6.14 | ppl   464.87\n",
            "| epoch   3 |   600/ 2983 batches | lr 0.001 | ms/batch 17.79 | loss  6.04 | ppl   417.95\n",
            "| epoch   3 |   800/ 2983 batches | lr 0.001 | ms/batch 17.81 | loss  6.08 | ppl   439.21\n",
            "| epoch   3 |  1000/ 2983 batches | lr 0.001 | ms/batch 17.79 | loss  6.06 | ppl   426.59\n",
            "| epoch   3 |  1200/ 2983 batches | lr 0.001 | ms/batch 17.75 | loss  6.08 | ppl   438.70\n",
            "| epoch   3 |  1400/ 2983 batches | lr 0.001 | ms/batch 17.75 | loss  6.11 | ppl   448.97\n",
            "| epoch   3 |  1600/ 2983 batches | lr 0.001 | ms/batch 17.64 | loss  6.11 | ppl   450.33\n",
            "| epoch   3 |  1800/ 2983 batches | lr 0.001 | ms/batch 17.70 | loss  6.02 | ppl   411.08\n",
            "| epoch   3 |  2000/ 2983 batches | lr 0.001 | ms/batch 17.66 | loss  6.07 | ppl   430.78\n",
            "| epoch   3 |  2200/ 2983 batches | lr 0.001 | ms/batch 17.61 | loss  5.98 | ppl   394.87\n",
            "| epoch   3 |  2400/ 2983 batches | lr 0.001 | ms/batch 17.57 | loss  6.01 | ppl   407.36\n",
            "| epoch   3 |  2600/ 2983 batches | lr 0.001 | ms/batch 17.54 | loss  6.02 | ppl   410.71\n",
            "| epoch   3 |  2800/ 2983 batches | lr 0.001 | ms/batch 17.56 | loss  5.96 | ppl   389.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 54.94s | valid loss  6.15 | valid ppl   470.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/ 2983 batches | lr 0.001 | ms/batch 17.63 | loss  6.01 | ppl   407.65\n",
            "| epoch   4 |   400/ 2983 batches | lr 0.001 | ms/batch 17.58 | loss  5.95 | ppl   382.05\n",
            "| epoch   4 |   600/ 2983 batches | lr 0.001 | ms/batch 17.60 | loss  5.84 | ppl   343.48\n",
            "| epoch   4 |   800/ 2983 batches | lr 0.001 | ms/batch 17.61 | loss  5.90 | ppl   363.78\n",
            "| epoch   4 |  1000/ 2983 batches | lr 0.001 | ms/batch 17.63 | loss  5.87 | ppl   355.25\n",
            "| epoch   4 |  1200/ 2983 batches | lr 0.001 | ms/batch 17.65 | loss  5.90 | ppl   365.32\n",
            "| epoch   4 |  1400/ 2983 batches | lr 0.001 | ms/batch 17.64 | loss  5.93 | ppl   377.54\n",
            "| epoch   4 |  1600/ 2983 batches | lr 0.001 | ms/batch 17.72 | loss  5.94 | ppl   378.85\n",
            "| epoch   4 |  1800/ 2983 batches | lr 0.001 | ms/batch 17.69 | loss  5.85 | ppl   348.04\n",
            "| epoch   4 |  2000/ 2983 batches | lr 0.001 | ms/batch 17.68 | loss  5.90 | ppl   364.95\n",
            "| epoch   4 |  2200/ 2983 batches | lr 0.001 | ms/batch 17.71 | loss  5.81 | ppl   334.45\n",
            "| epoch   4 |  2400/ 2983 batches | lr 0.001 | ms/batch 17.76 | loss  5.85 | ppl   346.18\n",
            "| epoch   4 |  2600/ 2983 batches | lr 0.001 | ms/batch 17.75 | loss  5.86 | ppl   351.65\n",
            "| epoch   4 |  2800/ 2983 batches | lr 0.001 | ms/batch 17.75 | loss  5.81 | ppl   332.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 54.94s | valid loss  6.09 | valid ppl   442.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/ 2983 batches | lr 0.001 | ms/batch 17.73 | loss  5.86 | ppl   351.16\n",
            "| epoch   5 |   400/ 2983 batches | lr 0.001 | ms/batch 17.81 | loss  5.80 | ppl   331.85\n",
            "| epoch   5 |   600/ 2983 batches | lr 0.001 | ms/batch 17.75 | loss  5.70 | ppl   298.27\n",
            "| epoch   5 |   800/ 2983 batches | lr 0.001 | ms/batch 17.78 | loss  5.76 | ppl   316.89\n",
            "| epoch   5 |  1000/ 2983 batches | lr 0.001 | ms/batch 17.75 | loss  5.74 | ppl   310.94\n",
            "| epoch   5 |  1200/ 2983 batches | lr 0.001 | ms/batch 17.77 | loss  5.77 | ppl   319.31\n",
            "| epoch   5 |  1400/ 2983 batches | lr 0.001 | ms/batch 17.72 | loss  5.81 | ppl   332.17\n",
            "| epoch   5 |  1600/ 2983 batches | lr 0.001 | ms/batch 17.74 | loss  5.81 | ppl   333.23\n",
            "| epoch   5 |  1800/ 2983 batches | lr 0.001 | ms/batch 17.78 | loss  5.73 | ppl   307.47\n",
            "| epoch   5 |  2000/ 2983 batches | lr 0.001 | ms/batch 17.77 | loss  5.77 | ppl   321.81\n",
            "| epoch   5 |  2200/ 2983 batches | lr 0.001 | ms/batch 17.75 | loss  5.69 | ppl   294.93\n",
            "| epoch   5 |  2400/ 2983 batches | lr 0.001 | ms/batch 17.75 | loss  5.72 | ppl   305.51\n",
            "| epoch   5 |  2600/ 2983 batches | lr 0.001 | ms/batch 17.75 | loss  5.74 | ppl   312.20\n",
            "| epoch   5 |  2800/ 2983 batches | lr 0.001 | ms/batch 17.79 | loss  5.69 | ppl   295.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 55.18s | valid loss  6.06 | valid ppl   426.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/ 2983 batches | lr 0.001 | ms/batch 17.77 | loss  5.74 | ppl   312.31\n",
            "| epoch   6 |   400/ 2983 batches | lr 0.001 | ms/batch 17.83 | loss  5.69 | ppl   296.84\n",
            "| epoch   6 |   600/ 2983 batches | lr 0.001 | ms/batch 17.75 | loss  5.59 | ppl   266.69\n",
            "| epoch   6 |   800/ 2983 batches | lr 0.001 | ms/batch 17.82 | loss  5.65 | ppl   283.90\n",
            "| epoch   6 |  1000/ 2983 batches | lr 0.001 | ms/batch 17.73 | loss  5.63 | ppl   279.81\n",
            "| epoch   6 |  1200/ 2983 batches | lr 0.001 | ms/batch 17.79 | loss  5.66 | ppl   287.14\n",
            "| epoch   6 |  1400/ 2983 batches | lr 0.001 | ms/batch 17.76 | loss  5.70 | ppl   299.75\n",
            "| epoch   6 |  1600/ 2983 batches | lr 0.001 | ms/batch 17.78 | loss  5.71 | ppl   300.59\n",
            "| epoch   6 |  1800/ 2983 batches | lr 0.001 | ms/batch 17.73 | loss  5.63 | ppl   278.31\n",
            "| epoch   6 |  2000/ 2983 batches | lr 0.001 | ms/batch 17.63 | loss  5.67 | ppl   290.72\n",
            "| epoch   6 |  2200/ 2983 batches | lr 0.001 | ms/batch 17.62 | loss  5.58 | ppl   266.38\n",
            "| epoch   6 |  2400/ 2983 batches | lr 0.001 | ms/batch 17.60 | loss  5.62 | ppl   275.77\n",
            "| epoch   6 |  2600/ 2983 batches | lr 0.001 | ms/batch 17.56 | loss  5.65 | ppl   283.18\n",
            "| epoch   6 |  2800/ 2983 batches | lr 0.001 | ms/batch 17.54 | loss  5.59 | ppl   268.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 54.95s | valid loss  6.03 | valid ppl   417.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/ 2983 batches | lr 0.001 | ms/batch 17.56 | loss  5.65 | ppl   283.49\n",
            "| epoch   7 |   400/ 2983 batches | lr 0.001 | ms/batch 17.59 | loss  5.60 | ppl   270.66\n",
            "| epoch   7 |   600/ 2983 batches | lr 0.001 | ms/batch 17.63 | loss  5.49 | ppl   243.15\n",
            "| epoch   7 |   800/ 2983 batches | lr 0.001 | ms/batch 17.70 | loss  5.56 | ppl   258.98\n",
            "| epoch   7 |  1000/ 2983 batches | lr 0.001 | ms/batch 17.70 | loss  5.55 | ppl   256.36\n",
            "| epoch   7 |  1200/ 2983 batches | lr 0.001 | ms/batch 17.71 | loss  5.57 | ppl   262.97\n",
            "| epoch   7 |  1400/ 2983 batches | lr 0.001 | ms/batch 17.76 | loss  5.62 | ppl   275.00\n",
            "| epoch   7 |  1600/ 2983 batches | lr 0.001 | ms/batch 17.75 | loss  5.62 | ppl   275.73\n",
            "| epoch   7 |  1800/ 2983 batches | lr 0.001 | ms/batch 17.79 | loss  5.55 | ppl   256.00\n",
            "| epoch   7 |  2000/ 2983 batches | lr 0.001 | ms/batch 17.80 | loss  5.59 | ppl   266.93\n",
            "| epoch   7 |  2200/ 2983 batches | lr 0.001 | ms/batch 17.77 | loss  5.50 | ppl   244.62\n",
            "| epoch   7 |  2400/ 2983 batches | lr 0.001 | ms/batch 17.81 | loss  5.53 | ppl   252.87\n",
            "| epoch   7 |  2600/ 2983 batches | lr 0.001 | ms/batch 17.78 | loss  5.56 | ppl   260.65\n",
            "| epoch   7 |  2800/ 2983 batches | lr 0.001 | ms/batch 17.74 | loss  5.51 | ppl   247.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 55.06s | valid loss  6.02 | valid ppl   413.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/ 2983 batches | lr 0.001 | ms/batch 17.67 | loss  5.57 | ppl   261.16\n",
            "| epoch   8 |   400/ 2983 batches | lr 0.001 | ms/batch 17.72 | loss  5.52 | ppl   250.16\n",
            "| epoch   8 |   600/ 2983 batches | lr 0.001 | ms/batch 17.59 | loss  5.42 | ppl   224.89\n",
            "| epoch   8 |   800/ 2983 batches | lr 0.001 | ms/batch 17.62 | loss  5.48 | ppl   239.32\n",
            "| epoch   8 |  1000/ 2983 batches | lr 0.001 | ms/batch 17.60 | loss  5.47 | ppl   237.85\n",
            "| epoch   8 |  1200/ 2983 batches | lr 0.001 | ms/batch 17.62 | loss  5.50 | ppl   243.88\n",
            "| epoch   8 |  1400/ 2983 batches | lr 0.001 | ms/batch 17.62 | loss  5.54 | ppl   255.30\n",
            "| epoch   8 |  1600/ 2983 batches | lr 0.001 | ms/batch 17.58 | loss  5.55 | ppl   255.97\n",
            "| epoch   8 |  1800/ 2983 batches | lr 0.001 | ms/batch 17.58 | loss  5.47 | ppl   238.26\n",
            "| epoch   8 |  2000/ 2983 batches | lr 0.001 | ms/batch 17.60 | loss  5.51 | ppl   248.02\n",
            "| epoch   8 |  2200/ 2983 batches | lr 0.001 | ms/batch 17.55 | loss  5.43 | ppl   227.30\n",
            "| epoch   8 |  2400/ 2983 batches | lr 0.001 | ms/batch 17.60 | loss  5.46 | ppl   234.59\n",
            "| epoch   8 |  2600/ 2983 batches | lr 0.001 | ms/batch 17.61 | loss  5.49 | ppl   242.54\n",
            "| epoch   8 |  2800/ 2983 batches | lr 0.001 | ms/batch 17.61 | loss  5.44 | ppl   230.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 54.71s | valid loss  6.02 | valid ppl   411.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/ 2983 batches | lr 0.001 | ms/batch 17.62 | loss  5.49 | ppl   243.23\n",
            "| epoch   9 |   400/ 2983 batches | lr 0.001 | ms/batch 17.61 | loss  5.45 | ppl   233.56\n",
            "| epoch   9 |   600/ 2983 batches | lr 0.001 | ms/batch 17.66 | loss  5.35 | ppl   210.17\n",
            "| epoch   9 |   800/ 2983 batches | lr 0.001 | ms/batch 17.61 | loss  5.41 | ppl   223.43\n",
            "| epoch   9 |  1000/ 2983 batches | lr 0.001 | ms/batch 17.67 | loss  5.41 | ppl   222.78\n",
            "| epoch   9 |  1200/ 2983 batches | lr 0.001 | ms/batch 17.62 | loss  5.43 | ppl   228.31\n",
            "| epoch   9 |  1400/ 2983 batches | lr 0.001 | ms/batch 17.64 | loss  5.48 | ppl   239.10\n",
            "| epoch   9 |  1600/ 2983 batches | lr 0.001 | ms/batch 17.67 | loss  5.48 | ppl   239.81\n",
            "| epoch   9 |  1800/ 2983 batches | lr 0.001 | ms/batch 17.64 | loss  5.41 | ppl   223.71\n",
            "| epoch   9 |  2000/ 2983 batches | lr 0.001 | ms/batch 17.63 | loss  5.45 | ppl   232.58\n",
            "| epoch   9 |  2200/ 2983 batches | lr 0.001 | ms/batch 17.67 | loss  5.36 | ppl   213.08\n",
            "| epoch   9 |  2400/ 2983 batches | lr 0.001 | ms/batch 17.68 | loss  5.39 | ppl   219.59\n",
            "| epoch   9 |  2600/ 2983 batches | lr 0.001 | ms/batch 17.63 | loss  5.43 | ppl   227.60\n",
            "| epoch   9 |  2800/ 2983 batches | lr 0.001 | ms/batch 17.65 | loss  5.38 | ppl   216.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 54.80s | valid loss  6.02 | valid ppl   411.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/ 2983 batches | lr 0.001 | ms/batch 17.72 | loss  5.43 | ppl   228.44\n",
            "| epoch  10 |   400/ 2983 batches | lr 0.001 | ms/batch 17.58 | loss  5.39 | ppl   219.83\n",
            "| epoch  10 |   600/ 2983 batches | lr 0.001 | ms/batch 17.56 | loss  5.29 | ppl   197.99\n",
            "| epoch  10 |   800/ 2983 batches | lr 0.001 | ms/batch 17.60 | loss  5.35 | ppl   210.29\n",
            "| epoch  10 |  1000/ 2983 batches | lr 0.001 | ms/batch 17.63 | loss  5.35 | ppl   210.21\n",
            "| epoch  10 |  1200/ 2983 batches | lr 0.001 | ms/batch 17.61 | loss  5.37 | ppl   215.30\n",
            "| epoch  10 |  1400/ 2983 batches | lr 0.001 | ms/batch 17.58 | loss  5.42 | ppl   225.50\n",
            "| epoch  10 |  1600/ 2983 batches | lr 0.001 | ms/batch 17.60 | loss  5.42 | ppl   226.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "Exiting from training early\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giHtSc80sCcZ",
        "colab_type": "text"
      },
      "source": [
        "## (v-2) Show the perplexity score on the test set. You should select your best model based on the _valid_ set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4cIbY-8r7TB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "63e8c58f-f473-4e32-cea1-60303fa1a979"
      },
      "source": [
        "# Load the best saved model.\n",
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  5.92 | test ppl   372.39\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1C6Z9PgoQox",
        "colab_type": "text"
      },
      "source": [
        "## (vii) Adapt generate.py so that you can generate texts using your language model (FNNModel)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZzhYEfIoWtw",
        "colab_type": "code",
        "outputId": "fe583249-4286-4746-cf2e-d4d4803bdb1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "data = './data/wikitext-2'  # location of the data corpus\n",
        "checkpoint = './model.pt'   # model checkpoint to use\n",
        "outf = 'generated.txt'      # output file for generated text\n",
        "words = 1000                # number of words to generate\n",
        "seed = 1111                 # random seed\n",
        "cuda = True                 # use CUDA\n",
        "temperature = 1.0           # temperature - higher will increase diversity\n",
        "log_interval = 100          # reporting interval\n",
        "\n",
        "with open(checkpoint, 'rb') as f:\n",
        "  model = torch.load(f).to(device)\n",
        "model.eval()\n",
        "\n",
        "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "with open(outf, 'w') as outf:\n",
        "  with torch.no_grad(): # no tracking history\n",
        "    for i in range(words):\n",
        "      output = model(input)\n",
        "      word_weights = output.squeeze().div(temperature).exp().cpu()\n",
        "      word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "      input.fill_(word_idx)\n",
        "\n",
        "      word = corpus.dictionary.idx2word[word_idx]\n",
        "\n",
        "      outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "\n",
        "      if i % log_interval == 0:\n",
        "        print('| Generated {}/{} words'.format(i, words))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-dccb25b3139b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eubAEEWVs5Ei",
        "colab_type": "code",
        "outputId": "72c6c55d-e993-41c1-db12-a90a77c45310",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "!cat generated.txt"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "located at O 'Malley . A direction of The defensive public area . <eos> Dodge Moresby = = <eos> Within\n",
            "a Roman tonnes ( deep convection alone defence of the race with some challenges a game against the second playing\n",
            "power NAACP ) . In active performance of the 1650 Australian deity as a aircraft for the horse @-@ Him\n",
            "on Irish 4th Battle of passes overran the FIA 's most champion the drive of Walpole 's <unk> , along\n",
            "with 12 men to punt and the Augustan History and 11 November 18 @-@ pre @-@ because of nine deeply\n",
            "warm @-@ yard in luminosity for a 4 , as heaviest in the laws , which the Territorial Singles chart\n",
            ". The title in 1899 , what : a forward to a estuary . <eos> = = = <eos> Common\n",
            "starlings , where 550 house had below the 766th Regiment acquired much \" instrumentation , the popularity , the party\n",
            ". Occasionally he allowed another Washington Post Office : \" race and exploration = <eos> = <eos> <eos> <eos> <eos>\n",
            "= = Biography = <eos> <eos> <eos> = Marriage in the final too at least one agent and rise since\n",
            "the bride . The defensive line at that it . moral crystal themes are again by the final seven of\n",
            "Maine . <eos> <eos> Ímar informed about an frame , and 20 years . They represent the attempt to images\n",
            ", the population of the reduction to be able to many junior , husband , and trigger City and Hall\n",
            "\" ) \" incorporated down and the descendant of the new star athletes may have led to break into <unk>\n",
            "Gardner and restriction , conducted hair was the redshirt . It was forced to moderate Mission , affinity <eos> \"\n",
            ". <unk> \" remarked ; similar in the cutter at the early because agriculture , when they began towards creating\n",
            "a member of Ireland for these <unk> and Mantellodon , 2016 . Firstly was supposed to <unk> Society . By\n",
            "19 – 3 @.@ 14 @.@ 5 km / h ) . Oscar Stadium , but the small areas and\n",
            "the flu , suggests the strategy in long bands , but told Gandhi ; the longest play @-@ state has\n",
            "been written from this ; however , and and mb . <eos> Bell considered part of Toulon . <eos> It\n",
            "'s like about the decade of the new other generals , where . Gibson I was incomplete pass to complicating\n",
            "a fox to earn a formal desire to flooding with Michael appearing in <unk> ( died a fitting aspects of\n",
            "11 September 2 : <eos> <eos> <eos> <eos> As early until earlier inspiration for some value of the fruit with\n",
            "1800 – 1998 . The fans in Chains team : the <unk> volunteers from October 1903 to burn . <eos>\n",
            "After a few elections , 2012 , of other virtuoso <unk> Item = = <eos> = = = = 1982\n",
            ", on the pre walks with a pity , for a multiple voices is mentioned in a 4 % and\n",
            "42 runs as Fielding Myers started a major terrain . A figure wind shear , 2009 , to add a\n",
            "insurgency were endured a 7 km / ( an hour before leaving Valley Astronomical Albums Mosley 's upwards to his\n",
            "little offer to win him due to the few , passing are generally told him since the <unk> but have\n",
            "sovereignty . They have a food telescope , and several devolved those stories , and Tim and were questions are\n",
            "also published in the pectoral condoms have formerly the stained . The eastern North Korean troops to punt their shorts\n",
            "had a as an league . On July 1903 . Last series . It was recalled and Ulysses was challenged\n",
            "Mara finished in this proposed progressive deposits in 1924 there was kept a quarrel in the proposals makes a maximum\n",
            "thousand , which is a @-@ game a result . Johnson 's ง and a 5 @.@ 3 . =\n",
            "= <eos> <eos> <eos> <eos> <eos> and two players @-@ long diving isotopes and returned to large division , background\n",
            "and Arab in part as a 000 @,@ 500 men ( Washington Post received a motion 41 ended on July\n",
            "28 @.@ 7 cm ( 4 the town of 1930 to making its inspiration in 1914 and \" as .\n",
            "It is present in the bird who had experimental security and : The second quarter in one lead family usually\n",
            "run out of rugby and Tech received 28 @.@ 9 – 0 lead to Montoursville finished with the east side\n",
            ", Tanzania , a granite in Marines , juniors . Additionally , Pat Grand Prix ( FOCA . The Bruins\n",
            "out of baked alarm area of males determine the Midwest on 5 – 1 @.@ 7 . They become the\n",
            "ball . In 1898 , but , <unk> friend he washed out of John <unk> into helium , peaking at\n",
            "the <unk> . Since he was a vertical fin method of their day , , based on Route 221 yards\n",
            "and favored NC State 's final ( emotion , Hamels = <eos> = <eos> <eos> = = = = =\n",
            "= O 'Malley to <unk> . <eos> = Located 's 1963 . They have been initially signed their uprisings separated\n",
            "from a Order of German rivers and Canadian Department and Game issue . According to the Royal 21st century ,\n",
            "however by two in a king , and people . <eos> <eos> <eos> <eos> <eos> Following the ball had store\n",
            ", <unk> published in the year the general election are taken in many areas with political accustomed to switch to\n",
            "his track to carry out 10 – 41 seconds remaining in theatres in early summer and victory and the attack\n",
            "it was designed to compete . A large general election = <eos> = = = <eos> = = northern Proto\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoOAfzCmuloS",
        "colab_type": "text"
      },
      "source": [
        "## (viii) In your opinion, which computation/operation is the most expensive one in inference or forward pass? Can you think of ways to improve this? Is yea, please mention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo3Vq9kxuyvi",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fQyIgCyuzlu",
        "colab_type": "text"
      },
      "source": [
        "## (ix) Report the Spearman correlation for the input embeddings.\n",
        "\n",
        "Notice that the model also learns word vectors (input and output layer embeddings) as a byproduct. One way to evaluate the trained word vectors is to measure the cosine similarity between pairs of words, and then report the correlation with the similarity scores given by humans. For this exercise, use the dataset available [here](http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/) and report the Spearman correlation for the input embeddings. Exclude any pair if it is not in the embedding matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dvb7hvy2Ho0",
        "colab_type": "text"
      },
      "source": [
        "### Download wordsimilarity-353 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn7YJto02Ye-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "68928599-b46a-4ba1-e236-1edf2da76e8a"
      },
      "source": [
        "!wget http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/wordsim353.zip -P /content/data\n",
        "!unzip /content/data/wordsim353.zip -d /content/data/wordsim353"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-08 09:24:10--  http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/wordsim353.zip\n",
            "Resolving www.cs.technion.ac.il (www.cs.technion.ac.il)... 132.68.32.15\n",
            "Connecting to www.cs.technion.ac.il (www.cs.technion.ac.il)|132.68.32.15|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23257 (23K) [application/zip]\n",
            "Saving to: ‘/content/data/wordsim353.zip’\n",
            "\n",
            "wordsim353.zip      100%[===================>]  22.71K  34.2KB/s    in 0.7s    \n",
            "\n",
            "2020-02-08 09:24:11 (34.2 KB/s) - ‘/content/data/wordsim353.zip’ saved [23257/23257]\n",
            "\n",
            "Archive:  /content/data/wordsim353.zip\n",
            "  inflating: /content/data/wordsim353/combined.csv  \n",
            "  inflating: /content/data/wordsim353/set1.csv  \n",
            "  inflating: /content/data/wordsim353/set2.csv  \n",
            "  inflating: /content/data/wordsim353/combined.tab  \n",
            "  inflating: /content/data/wordsim353/set1.tab  \n",
            "  inflating: /content/data/wordsim353/set2.tab  \n",
            "  inflating: /content/data/wordsim353/instructions.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o28NJdtjLQEx",
        "colab_type": "text"
      },
      "source": [
        "### Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGtEtrWtLQC5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "2262de52-5d97-40c8-d56a-3b961d461447"
      },
      "source": [
        "device = torch.device('cpu')\n",
        "with open(checkpoint, 'rb') as f:\n",
        "  model = torch.load(f)\n",
        "  model = model.to(device)\n",
        "model"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FNNModel(\n",
              "  (encoder): Embedding(33278, 200)\n",
              "  (hidden): Linear(in_features=200, out_features=200, bias=True)\n",
              "  (tanh): Tanh()\n",
              "  (decoder): Linear(in_features=200, out_features=33278, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWXrKqSLKHj4",
        "colab_type": "text"
      },
      "source": [
        "### Prepare Spearman rank-order correlation data table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM0vVQfNHgbJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "0bec9b08-94b4-428a-f11b-89735743eba8"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Read csv\n",
        "df = pd.read_csv('/content/data/wordsim353/combined.csv')\n",
        "\n",
        "# Get word indices for Word 1 column\n",
        "word_1 = df['Word 1']\n",
        "df['Indices 1'] = word_1.apply(lambda x: corpus.dictionary.word2idx[x] if x in corpus.dictionary.word2idx else None)\n",
        "\n",
        "# Get word indices for Word 2 column\n",
        "word_2 = df['Word 2']\n",
        "df['Indices 2'] = word_2.apply(lambda x: corpus.dictionary.word2idx[x] if x in corpus.dictionary.word2idx else None)\n",
        "\n",
        "# Drop rows with nan values and reset index\n",
        "df = df.dropna().reset_index()\n",
        "\n",
        "# Convert columns to integers\n",
        "df['Indices 1'] = df['Indices 1'].astype('int')\n",
        "df['Indices 2'] = df['Indices 2'].astype('int')\n",
        "\n",
        "# Get word embeddings\n",
        "embedding_1 = df['Indices 1'].apply(lambda x: model.encoder.weight[x].detach().numpy())\n",
        "embedding_2 = df['Indices 2'].apply(lambda x: model.encoder.weight[x].detach().numpy())\n",
        "\n",
        "# Compute cosine similarity\n",
        "cosine_similarity = []\n",
        "for t1, t2 in zip(*(embedding_1, embedding_2)):\n",
        "  cos_sim = t1.dot(t2) / (norm(t1) * norm(t2))\n",
        "  cosine_similarity.append(cos_sim)\n",
        "df['Cosine Similarity'] = pd.Series(cosine_similarity)\n",
        "\n",
        "# Reindex dataframe columns\n",
        "df = df.reindex(columns=['Word 1', 'Word 2', 'Human (mean)', 'Cosine Similarity'])\n",
        "\n",
        "print(df)"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           Word 1    Word 2  Human (mean)  Cosine Similarity\n",
            "0            love       sex          6.77           0.096438\n",
            "1           tiger       cat          7.35          -0.106540\n",
            "2           tiger     tiger         10.00           1.000000\n",
            "3            book     paper          7.46          -0.011781\n",
            "4        computer  keyboard          7.62          -0.012144\n",
            "..            ...       ...           ...                ...\n",
            "311        shower     flood          6.03           0.121781\n",
            "312       weather  forecast          8.34           0.082310\n",
            "313      disaster      area          6.25          -0.002650\n",
            "314      governor    office          6.34          -0.063176\n",
            "315  architecture   century          3.78           0.050154\n",
            "\n",
            "[316 rows x 4 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}