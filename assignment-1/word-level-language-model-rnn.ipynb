{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "word-level-language-model-rnn.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alanwuha/ce7455-nlp/blob/master/assignment-1/word-level-language-model-rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsyBbtWr0ZaX",
        "colab_type": "text"
      },
      "source": [
        "### Download wikitext-2 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oHSqqxz0c4Q",
        "colab_type": "code",
        "outputId": "b1d33bcd-147f-4f51-d8c5-50a6e9636c05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!rm -r data\n",
        "!mkdir data\n",
        "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip -P /content/data\n",
        "!unzip /content/data/wikitext-2-v1.zip -d /content/data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'data': No such file or directory\n",
            "--2020-02-07 11:50:23--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.184.165\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.184.165|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4475746 (4.3M) [application/zip]\n",
            "Saving to: ‘/content/data/wikitext-2-v1.zip’\n",
            "\n",
            "wikitext-2-v1.zip   100%[===================>]   4.27M  18.0MB/s    in 0.2s    \n",
            "\n",
            "2020-02-07 11:50:23 (18.0 MB/s) - ‘/content/data/wikitext-2-v1.zip’ saved [4475746/4475746]\n",
            "\n",
            "Archive:  /content/data/wikitext-2-v1.zip\n",
            "   creating: /content/data/wikitext-2/\n",
            "  inflating: /content/data/wikitext-2/wiki.test.tokens  \n",
            "  inflating: /content/data/wikitext-2/wiki.valid.tokens  \n",
            "  inflating: /content/data/wikitext-2/wiki.train.tokens  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCOTNafMg_z5",
        "colab_type": "text"
      },
      "source": [
        "### Specify arg variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrEgZw42hCv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = './data/wikitext-2'  # location of the data corpus\n",
        "args_model = 'FFN'              # type of recurrent net\n",
        "emsize = 200                # size of word embeddings\n",
        "nhid = 200                  # number of hidden units per layer\n",
        "nlayers = 2                 # number of layers\n",
        "lr = 20                     # initial learning rate\n",
        "clip = 0.25                 # gradient clipping\n",
        "epochs = 40                 # upper epoch limit\n",
        "batch_size = 20             # batch size\n",
        "bptt = 35                   # sequence length\n",
        "dropout = 0.2               # dropout applied to layers (0 = no dropout)\n",
        "tied = False                # tie the word embedding and softmax weights\n",
        "seed = 1111                 # random seed\n",
        "cuda = True                 # use CUDA\n",
        "log_interval = 200          # report interval\n",
        "save = 'model.pt'           # path to save the final model\n",
        "onnx_export = ''            # path to export the final model in onnx format\n",
        "nhead = 2                   # the number of heads in the encoder/decoder of the transformer model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M0KYJXn0htA",
        "colab_type": "text"
      },
      "source": [
        "### data.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_krRf2j0kzQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from io import open\n",
        "import torch\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'wiki.train.tokens'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'wiki.valid.tokens'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'wiki.test.tokens'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            idss = []\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                ids = []\n",
        "                for word in words:\n",
        "                    ids.append(self.dictionary.word2idx[word])\n",
        "                idss.append(torch.tensor(ids).type(torch.int64))\n",
        "            ids = torch.cat(idss)\n",
        "\n",
        "        return ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSxxbklN0moQ",
        "colab_type": "text"
      },
      "source": [
        "### model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSAjXOz71l80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FFNModel(nn.Module):\n",
        "    \"\"\" Container module with an encoder, a feed forward module, and a decoder. \"\"\"\n",
        "\n",
        "    def __init__(self, ntokens, emsize, hidden_size, tie_weights=False):\n",
        "        super(FFNModel, self).__init__()\n",
        "        self.encoder = nn.Embedding(ntokens, emsize)\n",
        "        self.hidden = nn.Linear(emsize, hidden_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.decoder = nn.Linear(hidden_size, ntokens)\n",
        "\n",
        "        if tie_weights:\n",
        "            if hidden_size != emsize:\n",
        "                raise ValueError('When using the tied flag, hidden_size must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        # self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input):\n",
        "        emb = self.encoder(input)\n",
        "        hid = self.hidden(emb)\n",
        "        output = self.tanh(hid)\n",
        "        decoded = self.decoder(output)\n",
        "        return decoded\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        if rnn_type in ['LSTM', 'GRU']:\n",
        "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
        "        else:\n",
        "            try:\n",
        "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
        "            except KeyError:\n",
        "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
        "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
        "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        # Optionally tie weights as in:\n",
        "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
        "        # https://arxiv.org/abs/1608.05859\n",
        "        # and\n",
        "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
        "        # https://arxiv.org/abs/1611.01462\n",
        "        if tie_weights:\n",
        "            if nhid != ninp:\n",
        "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output)\n",
        "        return decoded, hidden\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
        "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
        "        else:\n",
        "            return weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
        "\n",
        "# Temporarily leave PositionalEncoding module here. Will be moved somewhere else.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    r\"\"\"Inject some information about the relative or absolute position of the tokens\n",
        "        in the sequence. The positional encodings have the same dimension as\n",
        "        the embeddings, so that the two can be summed. Here, we use sine and cosine\n",
        "        functions of different frequencies.\n",
        "    .. math::\n",
        "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
        "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
        "        \\text{where pos is the word position and i is the embed idx)\n",
        "    Args:\n",
        "        d_model: the embed dim (required).\n",
        "        dropout: the dropout value (default=0.1).\n",
        "        max_len: the max. length of the incoming sequence (default=5000).\n",
        "    Examples:\n",
        "        >>> pos_encoder = PositionalEncoding(d_model)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        r\"\"\"Inputs of forward function\n",
        "        Args:\n",
        "            x: the sequence fed to the positional encoder model (required).\n",
        "        Shape:\n",
        "            x: [sequence length, batch size, embed dim]\n",
        "            output: [sequence length, batch size, embed dim]\n",
        "        Examples:\n",
        "            >>> output = pos_encoder(x)\n",
        "        \"\"\"\n",
        "\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        try:\n",
        "            from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "        except:\n",
        "            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, has_mask=True):\n",
        "        if has_mask:\n",
        "            device = src.device\n",
        "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "                self.src_mask = mask\n",
        "        else:\n",
        "            self.src_mask = None\n",
        "\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return F.log_softmax(output, dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNA9aDE-0VhB",
        "colab_type": "text"
      },
      "source": [
        "### main.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipzOYeQy0W0a",
        "colab_type": "code",
        "outputId": "1d168b0d-0171-43b8-d2a4-7476a35bd1c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# coding: utf-8\n",
        "import argparse\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.onnx\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    if not cuda:\n",
        "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "###############################################################################\n",
        "# Load data\n",
        "###############################################################################\n",
        "\n",
        "# corpus = data.Corpus(data)\n",
        "corpus = Corpus(data)\n",
        "\n",
        "# Starting from sequential data, batchify arranges the dataset into columns.\n",
        "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
        "# ┌ a g m s ┐\n",
        "# │ b h n t │\n",
        "# │ c i o u │\n",
        "# │ d j p v │\n",
        "# │ e k q w │\n",
        "# └ f l r x ┘.\n",
        "# These columns are treated as independent by the model, which means that the\n",
        "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
        "# batch processing.\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(corpus.train, batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "###############################################################################\n",
        "# Build the model\n",
        "###############################################################################\n",
        "\n",
        "ntokens = len(corpus.dictionary)\n",
        "if args_model == 'Transformer':\n",
        "    model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
        "elif args_model == 'FFN':\n",
        "    model = FFNModel(ntokens, emsize, nhid, tied).to(device)\n",
        "else:\n",
        "    model = RNNModel(args_model, ntokens, emsize, nhid, nlayers, dropout, tied).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "###############################################################################\n",
        "# Training code\n",
        "###############################################################################\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "\n",
        "# get_batch subdivides the source data into chunks of length bptt.\n",
        "# If source is equal to the example output of the batchify function, with\n",
        "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
        "# ┌ a g m s ┐ ┌ b h n t ┐\n",
        "# └ b h n t ┘ └ c i o u ┘\n",
        "# Note that despite the name of the function, the subdivison of data is not\n",
        "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
        "# by the batchify function. The chunks are along dimension 0, corresponding\n",
        "# to the seq_len dimension in the LSTM.\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    if args_model not in ['Transformer', 'FFN']:\n",
        "        hidden = model.init_hidden(eval_batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            if args_model in ['Transformer', 'FFN']:\n",
        "                output = model(data)\n",
        "            else:\n",
        "                output, hidden = model(data, hidden)\n",
        "                hidden = repackage_hidden(hidden)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    if args_model not in ['Transformer', 'FFN']:\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        # model.zero_grad()\n",
        "        \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output\n",
        "        if args_model in ['Transformer', 'FFN']:\n",
        "            output = model(data)\n",
        "        else:\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            output, hidden = model(data, hidden)\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        # for p in model.parameters():\n",
        "        #     p.data.add_(-lr, p.grad.data)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.3f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // bptt, optimizer.param_groups[0]['lr'],\n",
        "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "\n",
        "def export_onnx(path, batch_size, seq_len):\n",
        "    print('The model is also exported in ONNX format at {}'.\n",
        "          format(os.path.realpath(onnx_export)))\n",
        "    model.eval()\n",
        "    dummy_input = torch.LongTensor(seq_len * batch_size).zero_().view(-1, batch_size).to(device)\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    torch.onnx.export(model, (dummy_input, hidden), path)\n",
        "\n",
        "\n",
        "# Loop over epochs.\n",
        "lr = lr\n",
        "best_val_loss = None\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "    # after load the rnn params are not a continuous chunk of memory\n",
        "    # this makes them a continuous chunk, and will speed up forward pass\n",
        "    # Currently, only rnn model supports flatten_parameters function.\n",
        "    if model in ['RNN_TANH', 'RNN_RELU', 'LSTM', 'GRU']:\n",
        "        model.rnn.flatten_parameters()\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)\n",
        "\n",
        "if len(onnx_export) > 0:\n",
        "    # Export the model in ONNX format.\n",
        "    export_onnx(onnx_export, batch_size=1, seq_len=bptt)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 2983 batches | lr 0.001 | ms/batch 18.30 | loss  8.08 | ppl  3234.78\n",
            "| epoch   1 |   400/ 2983 batches | lr 0.001 | ms/batch 17.55 | loss  6.59 | ppl   728.55\n",
            "| epoch   1 |   600/ 2983 batches | lr 0.001 | ms/batch 17.62 | loss  6.38 | ppl   592.88\n",
            "| epoch   1 |   800/ 2983 batches | lr 0.001 | ms/batch 17.71 | loss  6.31 | ppl   549.91\n",
            "| epoch   1 |  1000/ 2983 batches | lr 0.001 | ms/batch 17.76 | loss  6.23 | ppl   507.14\n",
            "| epoch   1 |  1200/ 2983 batches | lr 0.001 | ms/batch 17.79 | loss  6.22 | ppl   503.70\n",
            "| epoch   1 |  1400/ 2983 batches | lr 0.001 | ms/batch 17.87 | loss  6.20 | ppl   490.41\n",
            "| epoch   1 |  1600/ 2983 batches | lr 0.001 | ms/batch 17.91 | loss  6.18 | ppl   485.13\n",
            "| epoch   1 |  1800/ 2983 batches | lr 0.001 | ms/batch 18.00 | loss  6.07 | ppl   430.87\n",
            "| epoch   1 |  2000/ 2983 batches | lr 0.001 | ms/batch 18.07 | loss  6.10 | ppl   444.95\n",
            "| epoch   1 |  2200/ 2983 batches | lr 0.001 | ms/batch 18.15 | loss  6.00 | ppl   401.94\n",
            "| epoch   1 |  2400/ 2983 batches | lr 0.001 | ms/batch 18.19 | loss  6.03 | ppl   414.26\n",
            "| epoch   1 |  2600/ 2983 batches | lr 0.001 | ms/batch 18.36 | loss  6.01 | ppl   408.67\n",
            "| epoch   1 |  2800/ 2983 batches | lr 0.001 | ms/batch 18.53 | loss  5.95 | ppl   382.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 55.87s | valid loss  5.89 | valid ppl   361.36\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type FFNModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   2 |   200/ 2983 batches | lr 0.001 | ms/batch 19.00 | loss  5.73 | ppl   307.82\n",
            "| epoch   2 |   400/ 2983 batches | lr 0.001 | ms/batch 19.11 | loss  5.68 | ppl   292.37\n",
            "| epoch   2 |   600/ 2983 batches | lr 0.001 | ms/batch 19.52 | loss  5.56 | ppl   258.84\n",
            "| epoch   2 |   800/ 2983 batches | lr 0.001 | ms/batch 19.66 | loss  5.59 | ppl   268.52\n",
            "| epoch   2 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.03 | loss  5.56 | ppl   259.41\n",
            "| epoch   2 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.17 | loss  5.58 | ppl   264.33\n",
            "| epoch   2 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  5.61 | ppl   272.79\n",
            "| epoch   2 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  5.61 | ppl   271.92\n",
            "| epoch   2 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.52 | ppl   250.46\n",
            "| epoch   2 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  5.58 | ppl   264.73\n",
            "| epoch   2 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  5.48 | ppl   239.13\n",
            "| epoch   2 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  5.51 | ppl   247.68\n",
            "| epoch   2 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  5.53 | ppl   251.63\n",
            "| epoch   2 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  5.48 | ppl   239.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 61.78s | valid loss  5.83 | valid ppl   338.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2983 batches | lr 0.001 | ms/batch 20.47 | loss  5.41 | ppl   223.24\n",
            "| epoch   3 |   400/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  5.38 | ppl   216.74\n",
            "| epoch   3 |   600/ 2983 batches | lr 0.001 | ms/batch 20.23 | loss  5.27 | ppl   193.82\n",
            "| epoch   3 |   800/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.32 | ppl   205.20\n",
            "| epoch   3 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.30 | ppl   200.52\n",
            "| epoch   3 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  5.32 | ppl   204.58\n",
            "| epoch   3 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.37 | loss  5.36 | ppl   212.01\n",
            "| epoch   3 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.57 | loss  5.36 | ppl   212.31\n",
            "| epoch   3 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.78 | loss  5.29 | ppl   198.04\n",
            "| epoch   3 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.96 | loss  5.34 | ppl   208.98\n",
            "| epoch   3 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.94 | loss  5.24 | ppl   188.86\n",
            "| epoch   3 |  2400/ 2983 batches | lr 0.001 | ms/batch 21.11 | loss  5.28 | ppl   195.56\n",
            "| epoch   3 |  2600/ 2983 batches | lr 0.001 | ms/batch 21.01 | loss  5.30 | ppl   200.25\n",
            "| epoch   3 |  2800/ 2983 batches | lr 0.001 | ms/batch 21.07 | loss  5.25 | ppl   190.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 63.97s | valid loss  5.83 | valid ppl   341.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/ 2983 batches | lr 0.001 | ms/batch 21.00 | loss  5.22 | ppl   185.65\n",
            "| epoch   4 |   400/ 2983 batches | lr 0.001 | ms/batch 20.74 | loss  5.21 | ppl   183.06\n",
            "| epoch   4 |   600/ 2983 batches | lr 0.001 | ms/batch 20.60 | loss  5.10 | ppl   164.46\n",
            "| epoch   4 |   800/ 2983 batches | lr 0.001 | ms/batch 20.37 | loss  5.16 | ppl   174.76\n",
            "| epoch   4 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.48 | loss  5.15 | ppl   172.37\n",
            "| epoch   4 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.40 | loss  5.17 | ppl   175.62\n",
            "| epoch   4 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.30 | loss  5.20 | ppl   181.77\n",
            "| epoch   4 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.21 | ppl   182.61\n",
            "| epoch   4 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.28 | loss  5.15 | ppl   172.12\n",
            "| epoch   4 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.32 | loss  5.20 | ppl   180.78\n",
            "| epoch   4 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.35 | loss  5.10 | ppl   163.62\n",
            "| epoch   4 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.41 | loss  5.13 | ppl   169.10\n",
            "| epoch   4 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.42 | loss  5.16 | ppl   174.01\n",
            "| epoch   4 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.44 | loss  5.11 | ppl   165.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 63.43s | valid loss  5.86 | valid ppl   349.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  5.10 | ppl   164.76\n",
            "| epoch   5 |   400/ 2983 batches | lr 0.001 | ms/batch 20.43 | loss  5.10 | ppl   163.65\n",
            "| epoch   5 |   600/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.99 | ppl   147.61\n",
            "| epoch   5 |   800/ 2983 batches | lr 0.001 | ms/batch 20.72 | loss  5.06 | ppl   156.90\n",
            "| epoch   5 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.53 | loss  5.05 | ppl   155.55\n",
            "| epoch   5 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.59 | loss  5.07 | ppl   158.41\n",
            "| epoch   5 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.68 | loss  5.10 | ppl   163.67\n",
            "| epoch   5 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.78 | loss  5.10 | ppl   164.63\n",
            "| epoch   5 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.76 | loss  5.05 | ppl   156.43\n",
            "| epoch   5 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.72 | loss  5.10 | ppl   163.64\n",
            "| epoch   5 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.78 | loss  5.00 | ppl   148.30\n",
            "| epoch   5 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.78 | loss  5.03 | ppl   153.14\n",
            "| epoch   5 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.72 | loss  5.06 | ppl   158.00\n",
            "| epoch   5 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.73 | loss  5.02 | ppl   150.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 64.18s | valid loss  5.89 | valid ppl   359.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/ 2983 batches | lr 0.001 | ms/batch 21.00 | loss  5.02 | ppl   151.67\n",
            "| epoch   6 |   400/ 2983 batches | lr 0.001 | ms/batch 20.85 | loss  5.02 | ppl   150.96\n",
            "| epoch   6 |   600/ 2983 batches | lr 0.001 | ms/batch 20.84 | loss  4.92 | ppl   136.67\n",
            "| epoch   6 |   800/ 2983 batches | lr 0.001 | ms/batch 20.82 | loss  4.98 | ppl   145.26\n",
            "| epoch   6 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.73 | loss  4.97 | ppl   144.32\n",
            "| epoch   6 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.91 | loss  4.99 | ppl   147.11\n",
            "| epoch   6 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.87 | loss  5.02 | ppl   151.65\n",
            "| epoch   6 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.72 | loss  5.03 | ppl   152.65\n",
            "| epoch   6 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.66 | loss  4.98 | ppl   145.83\n",
            "| epoch   6 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.50 | loss  5.03 | ppl   152.22\n",
            "| epoch   6 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.93 | ppl   138.11\n",
            "| epoch   6 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.63 | loss  4.96 | ppl   142.50\n",
            "| epoch   6 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.57 | loss  4.99 | ppl   147.27\n",
            "| epoch   6 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.44 | loss  4.95 | ppl   140.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 64.26s | valid loss  5.91 | valid ppl   369.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/ 2983 batches | lr 0.001 | ms/batch 20.85 | loss  4.96 | ppl   142.69\n",
            "| epoch   7 |   400/ 2983 batches | lr 0.001 | ms/batch 20.56 | loss  4.96 | ppl   141.96\n",
            "| epoch   7 |   600/ 2983 batches | lr 0.001 | ms/batch 20.49 | loss  4.86 | ppl   128.99\n",
            "| epoch   7 |   800/ 2983 batches | lr 0.001 | ms/batch 20.51 | loss  4.92 | ppl   137.05\n",
            "| epoch   7 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.35 | loss  4.92 | ppl   136.38\n",
            "| epoch   7 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.39 | loss  4.94 | ppl   139.10\n",
            "| epoch   7 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  4.96 | ppl   143.08\n",
            "| epoch   7 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.50 | loss  4.97 | ppl   144.12\n",
            "| epoch   7 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.57 | loss  4.93 | ppl   138.17\n",
            "| epoch   7 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.51 | loss  4.97 | ppl   144.06\n",
            "| epoch   7 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.44 | loss  4.87 | ppl   130.86\n",
            "| epoch   7 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.48 | loss  4.90 | ppl   134.93\n",
            "| epoch   7 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.50 | loss  4.94 | ppl   139.63\n",
            "| epoch   7 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.47 | loss  4.89 | ppl   133.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 63.58s | valid loss  5.94 | valid ppl   380.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/ 2983 batches | lr 0.001 | ms/batch 20.80 | loss  4.91 | ppl   136.14\n",
            "| epoch   8 |   400/ 2983 batches | lr 0.001 | ms/batch 20.59 | loss  4.91 | ppl   135.27\n",
            "| epoch   8 |   600/ 2983 batches | lr 0.001 | ms/batch 20.55 | loss  4.81 | ppl   123.27\n",
            "| epoch   8 |   800/ 2983 batches | lr 0.001 | ms/batch 20.56 | loss  4.87 | ppl   130.93\n",
            "| epoch   8 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.49 | loss  4.87 | ppl   130.45\n",
            "| epoch   8 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.59 | loss  4.89 | ppl   133.04\n",
            "| epoch   8 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.50 | loss  4.92 | ppl   136.74\n",
            "| epoch   8 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.42 | loss  4.93 | ppl   137.78\n",
            "| epoch   8 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.51 | loss  4.89 | ppl   132.39\n",
            "| epoch   8 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.58 | loss  4.93 | ppl   137.84\n",
            "| epoch   8 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.65 | loss  4.83 | ppl   125.39\n",
            "| epoch   8 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.86 | ppl   129.29\n",
            "| epoch   8 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.53 | loss  4.90 | ppl   133.87\n",
            "| epoch   8 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.85 | ppl   128.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 63.79s | valid loss  5.97 | valid ppl   390.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/ 2983 batches | lr 0.001 | ms/batch 20.82 | loss  4.88 | ppl   131.15\n",
            "| epoch   9 |   400/ 2983 batches | lr 0.001 | ms/batch 20.63 | loss  4.87 | ppl   130.06\n",
            "| epoch   9 |   600/ 2983 batches | lr 0.001 | ms/batch 20.58 | loss  4.78 | ppl   118.86\n",
            "| epoch   9 |   800/ 2983 batches | lr 0.001 | ms/batch 20.56 | loss  4.84 | ppl   126.20\n",
            "| epoch   9 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.59 | loss  4.84 | ppl   125.85\n",
            "| epoch   9 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.42 | loss  4.85 | ppl   128.31\n",
            "| epoch   9 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.60 | loss  4.88 | ppl   131.83\n",
            "| epoch   9 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.44 | loss  4.89 | ppl   132.82\n",
            "| epoch   9 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.85 | ppl   127.90\n",
            "| epoch   9 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.89 | ppl   133.01\n",
            "| epoch   9 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.51 | loss  4.80 | ppl   121.12\n",
            "| epoch   9 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.44 | loss  4.83 | ppl   124.93\n",
            "| epoch   9 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.60 | loss  4.86 | ppl   129.39\n",
            "| epoch   9 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.82 | ppl   123.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 63.81s | valid loss  5.99 | valid ppl   399.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/ 2983 batches | lr 0.001 | ms/batch 20.90 | loss  4.85 | ppl   127.23\n",
            "| epoch  10 |   400/ 2983 batches | lr 0.001 | ms/batch 20.52 | loss  4.84 | ppl   125.95\n",
            "| epoch  10 |   600/ 2983 batches | lr 0.001 | ms/batch 20.53 | loss  4.75 | ppl   115.35\n",
            "| epoch  10 |   800/ 2983 batches | lr 0.001 | ms/batch 20.53 | loss  4.81 | ppl   122.43\n",
            "| epoch  10 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.56 | loss  4.81 | ppl   122.19\n",
            "| epoch  10 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.82 | ppl   124.51\n",
            "| epoch  10 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.85 | ppl   127.84\n",
            "| epoch  10 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.86 | ppl   128.92\n",
            "| epoch  10 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.82 | ppl   124.31\n",
            "| epoch  10 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.86 | ppl   129.04\n",
            "| epoch  10 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.71 | loss  4.77 | ppl   117.71\n",
            "| epoch  10 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.64 | loss  4.80 | ppl   121.46\n",
            "| epoch  10 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.83 | ppl   125.78\n",
            "| epoch  10 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.79 | ppl   120.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 63.98s | valid loss  6.01 | valid ppl   407.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |   200/ 2983 batches | lr 0.001 | ms/batch 20.82 | loss  4.82 | ppl   124.07\n",
            "| epoch  11 |   400/ 2983 batches | lr 0.001 | ms/batch 20.59 | loss  4.81 | ppl   122.61\n",
            "| epoch  11 |   600/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.72 | ppl   112.52\n",
            "| epoch  11 |   800/ 2983 batches | lr 0.001 | ms/batch 20.69 | loss  4.78 | ppl   119.36\n",
            "| epoch  11 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.67 | loss  4.78 | ppl   119.21\n",
            "| epoch  11 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.52 | loss  4.80 | ppl   121.40\n",
            "| epoch  11 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.63 | loss  4.82 | ppl   124.56\n",
            "| epoch  11 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.83 | ppl   125.73\n",
            "| epoch  11 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.80 | ppl   121.40\n",
            "| epoch  11 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.84 | ppl   125.84\n",
            "| epoch  11 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.66 | loss  4.74 | ppl   114.88\n",
            "| epoch  11 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.58 | loss  4.78 | ppl   118.62\n",
            "| epoch  11 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.67 | loss  4.81 | ppl   122.81\n",
            "| epoch  11 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.66 | loss  4.77 | ppl   117.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 64.01s | valid loss  6.03 | valid ppl   415.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |   200/ 2983 batches | lr 0.001 | ms/batch 20.67 | loss  4.80 | ppl   121.46\n",
            "| epoch  12 |   400/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.79 | ppl   119.88\n",
            "| epoch  12 |   600/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.70 | ppl   110.14\n",
            "| epoch  12 |   800/ 2983 batches | lr 0.001 | ms/batch 20.70 | loss  4.76 | ppl   116.82\n",
            "| epoch  12 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.76 | ppl   116.73\n",
            "| epoch  12 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.56 | loss  4.78 | ppl   118.80\n",
            "| epoch  12 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.68 | loss  4.80 | ppl   121.81\n",
            "| epoch  12 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.81 | ppl   123.17\n",
            "| epoch  12 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.78 | ppl   118.98\n",
            "| epoch  12 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.63 | loss  4.81 | ppl   123.16\n",
            "| epoch  12 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.72 | ppl   112.52\n",
            "| epoch  12 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.76 | ppl   116.24\n",
            "| epoch  12 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.67 | loss  4.79 | ppl   120.32\n",
            "| epoch  12 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.66 | loss  4.75 | ppl   115.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 64.00s | valid loss  6.05 | valid ppl   423.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |   200/ 2983 batches | lr 0.001 | ms/batch 20.73 | loss  4.78 | ppl   119.26\n",
            "| epoch  13 |   400/ 2983 batches | lr 0.001 | ms/batch 20.66 | loss  4.77 | ppl   117.59\n",
            "| epoch  13 |   600/ 2983 batches | lr 0.001 | ms/batch 20.57 | loss  4.68 | ppl   108.12\n",
            "| epoch  13 |   800/ 2983 batches | lr 0.001 | ms/batch 20.55 | loss  4.74 | ppl   114.70\n",
            "| epoch  13 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.59 | loss  4.74 | ppl   114.63\n",
            "| epoch  13 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.57 | loss  4.76 | ppl   116.63\n",
            "| epoch  13 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.53 | loss  4.78 | ppl   119.49\n",
            "| epoch  13 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.74 | loss  4.80 | ppl   120.97\n",
            "| epoch  13 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.63 | loss  4.76 | ppl   116.93\n",
            "| epoch  13 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.64 | loss  4.80 | ppl   120.96\n",
            "| epoch  13 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.71 | loss  4.71 | ppl   110.50\n",
            "| epoch  13 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.69 | loss  4.74 | ppl   114.18\n",
            "| epoch  13 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.60 | loss  4.77 | ppl   118.19\n",
            "| epoch  13 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.56 | loss  4.73 | ppl   113.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 63.99s | valid loss  6.07 | valid ppl   430.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |   200/ 2983 batches | lr 0.001 | ms/batch 20.95 | loss  4.77 | ppl   117.36\n",
            "| epoch  14 |   400/ 2983 batches | lr 0.001 | ms/batch 20.60 | loss  4.75 | ppl   115.68\n",
            "| epoch  14 |   600/ 2983 batches | lr 0.001 | ms/batch 20.60 | loss  4.67 | ppl   106.35\n",
            "| epoch  14 |   800/ 2983 batches | lr 0.001 | ms/batch 20.56 | loss  4.73 | ppl   112.90\n",
            "| epoch  14 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.60 | loss  4.73 | ppl   112.82\n",
            "| epoch  14 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.63 | loss  4.74 | ppl   114.78\n",
            "| epoch  14 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.65 | loss  4.77 | ppl   117.50\n",
            "| epoch  14 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.65 | loss  4.78 | ppl   119.14\n",
            "| epoch  14 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.75 | ppl   115.16\n",
            "| epoch  14 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.65 | loss  4.78 | ppl   119.05\n",
            "| epoch  14 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.69 | ppl   108.81\n",
            "| epoch  14 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.71 | loss  4.72 | ppl   112.38\n",
            "| epoch  14 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.60 | loss  4.76 | ppl   116.35\n",
            "| epoch  14 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.52 | loss  4.72 | ppl   111.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 64.00s | valid loss  6.08 | valid ppl   437.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |   200/ 2983 batches | lr 0.001 | ms/batch 20.79 | loss  4.75 | ppl   115.72\n",
            "| epoch  15 |   400/ 2983 batches | lr 0.001 | ms/batch 20.71 | loss  4.74 | ppl   114.02\n",
            "| epoch  15 |   600/ 2983 batches | lr 0.001 | ms/batch 20.63 | loss  4.65 | ppl   104.83\n",
            "| epoch  15 |   800/ 2983 batches | lr 0.001 | ms/batch 20.70 | loss  4.71 | ppl   111.33\n",
            "| epoch  15 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.65 | loss  4.71 | ppl   111.26\n",
            "| epoch  15 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.44 | loss  4.73 | ppl   113.19\n",
            "| epoch  15 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.70 | loss  4.75 | ppl   115.75\n",
            "| epoch  15 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.60 | loss  4.77 | ppl   117.49\n",
            "| epoch  15 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.46 | loss  4.73 | ppl   113.62\n",
            "| epoch  15 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.69 | loss  4.77 | ppl   117.45\n",
            "| epoch  15 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.53 | loss  4.68 | ppl   107.32\n",
            "| epoch  15 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.71 | ppl   110.79\n",
            "| epoch  15 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.63 | loss  4.74 | ppl   114.72\n",
            "| epoch  15 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.64 | loss  4.70 | ppl   110.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 64.01s | valid loss  6.10 | valid ppl   444.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |   200/ 2983 batches | lr 0.001 | ms/batch 20.85 | loss  4.74 | ppl   114.25\n",
            "| epoch  16 |   400/ 2983 batches | lr 0.001 | ms/batch 20.64 | loss  4.72 | ppl   112.62\n",
            "| epoch  16 |   600/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.64 | ppl   103.47\n",
            "| epoch  16 |   800/ 2983 batches | lr 0.001 | ms/batch 20.54 | loss  4.70 | ppl   109.93\n",
            "| epoch  16 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.80 | loss  4.70 | ppl   109.89\n",
            "| epoch  16 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.64 | loss  4.72 | ppl   111.80\n",
            "| epoch  16 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.72 | loss  4.74 | ppl   114.23\n",
            "| epoch  16 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.65 | loss  4.75 | ppl   116.09\n",
            "| epoch  16 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.48 | loss  4.72 | ppl   112.24\n",
            "| epoch  16 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.40 | loss  4.75 | ppl   116.00\n",
            "| epoch  16 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.65 | loss  4.66 | ppl   106.08\n",
            "| epoch  16 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.69 | ppl   109.40\n",
            "| epoch  16 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.57 | loss  4.73 | ppl   113.29\n",
            "| epoch  16 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.69 | ppl   108.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 63.94s | valid loss  6.11 | valid ppl   451.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |   200/ 2983 batches | lr 0.001 | ms/batch 20.83 | loss  4.73 | ppl   112.96\n",
            "| epoch  17 |   400/ 2983 batches | lr 0.001 | ms/batch 20.57 | loss  4.71 | ppl   111.38\n",
            "| epoch  17 |   600/ 2983 batches | lr 0.001 | ms/batch 20.51 | loss  4.63 | ppl   102.28\n",
            "| epoch  17 |   800/ 2983 batches | lr 0.001 | ms/batch 20.52 | loss  4.69 | ppl   108.68\n",
            "| epoch  17 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.60 | loss  4.69 | ppl   108.70\n",
            "| epoch  17 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.71 | ppl   110.59\n",
            "| epoch  17 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.73 | ppl   112.89\n",
            "| epoch  17 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.56 | loss  4.74 | ppl   114.77\n",
            "| epoch  17 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.63 | loss  4.71 | ppl   111.02\n",
            "| epoch  17 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.74 | ppl   114.78\n",
            "| epoch  17 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.52 | loss  4.65 | ppl   104.97\n",
            "| epoch  17 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.64 | loss  4.68 | ppl   108.17\n",
            "| epoch  17 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.72 | ppl   112.02\n",
            "| epoch  17 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.68 | ppl   107.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 63.93s | valid loss  6.13 | valid ppl   458.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |   200/ 2983 batches | lr 0.001 | ms/batch 20.98 | loss  4.72 | ppl   111.80\n",
            "| epoch  18 |   400/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.70 | ppl   110.31\n",
            "| epoch  18 |   600/ 2983 batches | lr 0.001 | ms/batch 20.57 | loss  4.62 | ppl   101.22\n",
            "| epoch  18 |   800/ 2983 batches | lr 0.001 | ms/batch 20.66 | loss  4.68 | ppl   107.58\n",
            "| epoch  18 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.68 | ppl   107.60\n",
            "| epoch  18 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.64 | loss  4.70 | ppl   109.54\n",
            "| epoch  18 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.50 | loss  4.72 | ppl   111.72\n",
            "| epoch  18 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.55 | loss  4.73 | ppl   113.62\n",
            "| epoch  18 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.59 | loss  4.70 | ppl   109.93\n",
            "| epoch  18 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.73 | loss  4.73 | ppl   113.61\n",
            "| epoch  18 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.66 | loss  4.64 | ppl   104.04\n",
            "| epoch  18 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.65 | loss  4.67 | ppl   107.08\n",
            "| epoch  18 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.67 | loss  4.71 | ppl   110.91\n",
            "| epoch  18 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.37 | loss  4.67 | ppl   106.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 63.98s | valid loss  6.14 | valid ppl   464.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |   200/ 2983 batches | lr 0.001 | ms/batch 20.81 | loss  4.71 | ppl   110.79\n",
            "| epoch  19 |   400/ 2983 batches | lr 0.001 | ms/batch 20.51 | loss  4.69 | ppl   109.32\n",
            "| epoch  19 |   600/ 2983 batches | lr 0.001 | ms/batch 20.52 | loss  4.61 | ppl   100.29\n",
            "| epoch  19 |   800/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.67 | ppl   106.59\n",
            "| epoch  19 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.50 | loss  4.67 | ppl   106.63\n",
            "| epoch  19 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.56 | loss  4.69 | ppl   108.65\n",
            "| epoch  19 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.52 | loss  4.71 | ppl   110.68\n",
            "| epoch  19 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.72 | ppl   112.51\n",
            "| epoch  19 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.57 | loss  4.69 | ppl   108.95\n",
            "| epoch  19 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.51 | loss  4.72 | ppl   112.63\n",
            "| epoch  19 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.51 | loss  4.64 | ppl   103.18\n",
            "| epoch  19 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.52 | loss  4.66 | ppl   106.13\n",
            "| epoch  19 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.57 | loss  4.70 | ppl   109.92\n",
            "| epoch  19 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.66 | ppl   105.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 63.82s | valid loss  6.15 | valid ppl   470.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |   200/ 2983 batches | lr 0.001 | ms/batch 20.75 | loss  4.70 | ppl   109.89\n",
            "| epoch  20 |   400/ 2983 batches | lr 0.001 | ms/batch 20.58 | loss  4.69 | ppl   108.48\n",
            "| epoch  20 |   600/ 2983 batches | lr 0.001 | ms/batch 20.54 | loss  4.60 | ppl    99.47\n",
            "| epoch  20 |   800/ 2983 batches | lr 0.001 | ms/batch 20.57 | loss  4.66 | ppl   105.72\n",
            "| epoch  20 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.60 | loss  4.66 | ppl   105.74\n",
            "| epoch  20 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.51 | loss  4.68 | ppl   107.88\n",
            "| epoch  20 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.51 | loss  4.70 | ppl   109.78\n",
            "| epoch  20 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.52 | loss  4.71 | ppl   111.57\n",
            "| epoch  20 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.51 | loss  4.68 | ppl   108.07\n",
            "| epoch  20 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.53 | loss  4.72 | ppl   111.68\n",
            "| epoch  20 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.52 | loss  4.63 | ppl   102.45\n",
            "| epoch  20 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.55 | loss  4.66 | ppl   105.31\n",
            "| epoch  20 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.69 | ppl   109.08\n",
            "| epoch  20 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.70 | loss  4.65 | ppl   104.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 63.81s | valid loss  6.17 | valid ppl   476.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |   200/ 2983 batches | lr 0.001 | ms/batch 20.78 | loss  4.69 | ppl   109.12\n",
            "| epoch  21 |   400/ 2983 batches | lr 0.001 | ms/batch 20.60 | loss  4.68 | ppl   107.70\n",
            "| epoch  21 |   600/ 2983 batches | lr 0.001 | ms/batch 20.40 | loss  4.59 | ppl    98.76\n",
            "| epoch  21 |   800/ 2983 batches | lr 0.001 | ms/batch 20.58 | loss  4.65 | ppl   104.93\n",
            "| epoch  21 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.53 | loss  4.65 | ppl   104.97\n",
            "| epoch  21 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.51 | loss  4.67 | ppl   107.21\n",
            "| epoch  21 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.69 | loss  4.69 | ppl   108.97\n",
            "| epoch  21 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.46 | loss  4.71 | ppl   110.70\n",
            "| epoch  21 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.52 | loss  4.68 | ppl   107.29\n",
            "| epoch  21 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.58 | loss  4.71 | ppl   110.92\n",
            "| epoch  21 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.44 | loss  4.62 | ppl   101.75\n",
            "| epoch  21 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.53 | loss  4.65 | ppl   104.60\n",
            "| epoch  21 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.54 | loss  4.69 | ppl   108.31\n",
            "| epoch  21 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.45 | loss  4.64 | ppl   103.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 63.71s | valid loss  6.18 | valid ppl   481.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |   200/ 2983 batches | lr 0.001 | ms/batch 20.81 | loss  4.69 | ppl   108.42\n",
            "| epoch  22 |   400/ 2983 batches | lr 0.001 | ms/batch 20.71 | loss  4.67 | ppl   107.05\n",
            "| epoch  22 |   600/ 2983 batches | lr 0.001 | ms/batch 20.64 | loss  4.59 | ppl    98.14\n",
            "| epoch  22 |   800/ 2983 batches | lr 0.001 | ms/batch 20.60 | loss  4.65 | ppl   104.24\n",
            "| epoch  22 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.51 | loss  4.65 | ppl   104.26\n",
            "| epoch  22 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.45 | loss  4.67 | ppl   106.61\n",
            "| epoch  22 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.56 | loss  4.68 | ppl   108.27\n",
            "| epoch  22 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.63 | loss  4.70 | ppl   109.99\n",
            "| epoch  22 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.59 | loss  4.67 | ppl   106.60\n",
            "| epoch  22 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.57 | loss  4.70 | ppl   110.19\n",
            "| epoch  22 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.63 | loss  4.62 | ppl   101.14\n",
            "| epoch  22 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.67 | loss  4.64 | ppl   103.99\n",
            "| epoch  22 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.68 | ppl   107.66\n",
            "| epoch  22 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.50 | loss  4.64 | ppl   103.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 63.95s | valid loss  6.19 | valid ppl   486.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |   200/ 2983 batches | lr 0.001 | ms/batch 20.88 | loss  4.68 | ppl   107.84\n",
            "| epoch  23 |   400/ 2983 batches | lr 0.001 | ms/batch 20.64 | loss  4.67 | ppl   106.46\n",
            "| epoch  23 |   600/ 2983 batches | lr 0.001 | ms/batch 20.81 | loss  4.58 | ppl    97.59\n",
            "| epoch  23 |   800/ 2983 batches | lr 0.001 | ms/batch 20.63 | loss  4.64 | ppl   103.63\n",
            "| epoch  23 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.80 | loss  4.64 | ppl   103.65\n",
            "| epoch  23 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.80 | loss  4.66 | ppl   106.08\n",
            "| epoch  23 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.67 | loss  4.68 | ppl   107.67\n",
            "| epoch  23 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.76 | loss  4.69 | ppl   109.33\n",
            "| epoch  23 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.65 | loss  4.66 | ppl   106.01\n",
            "| epoch  23 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.80 | loss  4.70 | ppl   109.61\n",
            "| epoch  23 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.76 | loss  4.61 | ppl   100.58\n",
            "| epoch  23 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.82 | loss  4.64 | ppl   103.46\n",
            "| epoch  23 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.73 | loss  4.67 | ppl   107.06\n",
            "| epoch  23 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.73 | loss  4.63 | ppl   102.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 64.36s | valid loss  6.20 | valid ppl   491.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |   200/ 2983 batches | lr 0.001 | ms/batch 20.89 | loss  4.68 | ppl   107.31\n",
            "| epoch  24 |   400/ 2983 batches | lr 0.001 | ms/batch 20.81 | loss  4.66 | ppl   105.98\n",
            "| epoch  24 |   600/ 2983 batches | lr 0.001 | ms/batch 20.80 | loss  4.58 | ppl    97.11\n",
            "| epoch  24 |   800/ 2983 batches | lr 0.001 | ms/batch 20.87 | loss  4.64 | ppl   103.13\n",
            "| epoch  24 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.73 | loss  4.64 | ppl   103.09\n",
            "| epoch  24 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.70 | loss  4.66 | ppl   105.59\n",
            "| epoch  24 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.76 | loss  4.67 | ppl   107.15\n",
            "| epoch  24 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.69 | ppl   108.81\n",
            "| epoch  24 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.71 | loss  4.66 | ppl   105.51\n",
            "| epoch  24 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.72 | loss  4.69 | ppl   109.04\n",
            "| epoch  24 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.63 | loss  4.61 | ppl   100.12\n",
            "| epoch  24 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.76 | loss  4.63 | ppl   103.02\n",
            "| epoch  24 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.87 | loss  4.67 | ppl   106.54\n",
            "| epoch  24 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.68 | loss  4.63 | ppl   102.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 64.36s | valid loss  6.21 | valid ppl   496.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |   200/ 2983 batches | lr 0.001 | ms/batch 20.73 | loss  4.67 | ppl   106.87\n",
            "| epoch  25 |   400/ 2983 batches | lr 0.001 | ms/batch 20.80 | loss  4.66 | ppl   105.55\n",
            "| epoch  25 |   600/ 2983 batches | lr 0.001 | ms/batch 20.54 | loss  4.57 | ppl    96.69\n",
            "| epoch  25 |   800/ 2983 batches | lr 0.001 | ms/batch 20.70 | loss  4.63 | ppl   102.69\n",
            "| epoch  25 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.75 | loss  4.63 | ppl   102.62\n",
            "| epoch  25 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.74 | loss  4.66 | ppl   105.17\n",
            "| epoch  25 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.53 | loss  4.67 | ppl   106.70\n",
            "| epoch  25 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.60 | loss  4.68 | ppl   108.30\n",
            "| epoch  25 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.72 | loss  4.65 | ppl   105.08\n",
            "| epoch  25 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.69 | ppl   108.61\n",
            "| epoch  25 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.60 | loss  4.60 | ppl    99.69\n",
            "| epoch  25 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.59 | loss  4.63 | ppl   102.63\n",
            "| epoch  25 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.66 | loss  4.66 | ppl   106.06\n",
            "| epoch  25 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.59 | loss  4.62 | ppl   101.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 64.03s | valid loss  6.22 | valid ppl   501.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |   200/ 2983 batches | lr 0.001 | ms/batch 20.72 | loss  4.67 | ppl   106.47\n",
            "| epoch  26 |   400/ 2983 batches | lr 0.001 | ms/batch 20.54 | loss  4.66 | ppl   105.22\n",
            "| epoch  26 |   600/ 2983 batches | lr 0.001 | ms/batch 20.58 | loss  4.57 | ppl    96.32\n",
            "| epoch  26 |   800/ 2983 batches | lr 0.001 | ms/batch 20.66 | loss  4.63 | ppl   102.32\n",
            "| epoch  26 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.58 | loss  4.63 | ppl   102.20\n",
            "| epoch  26 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.56 | loss  4.65 | ppl   104.77\n",
            "| epoch  26 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.52 | loss  4.67 | ppl   106.32\n",
            "| epoch  26 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.51 | loss  4.68 | ppl   107.92\n",
            "| epoch  26 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.63 | loss  4.65 | ppl   104.72\n",
            "| epoch  26 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.60 | loss  4.68 | ppl   108.18\n",
            "| epoch  26 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.60 | ppl    99.34\n",
            "| epoch  26 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.50 | loss  4.63 | ppl   102.31\n",
            "| epoch  26 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.46 | loss  4.66 | ppl   105.64\n",
            "| epoch  26 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.56 | loss  4.62 | ppl   101.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 63.82s | valid loss  6.23 | valid ppl   505.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |   200/ 2983 batches | lr 0.001 | ms/batch 20.83 | loss  4.66 | ppl   106.16\n",
            "| epoch  27 |   400/ 2983 batches | lr 0.001 | ms/batch 20.59 | loss  4.65 | ppl   104.90\n",
            "| epoch  27 |   600/ 2983 batches | lr 0.001 | ms/batch 20.52 | loss  4.56 | ppl    96.00\n",
            "| epoch  27 |   800/ 2983 batches | lr 0.001 | ms/batch 20.60 | loss  4.62 | ppl   102.00\n",
            "| epoch  27 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.43 | loss  4.62 | ppl   101.86\n",
            "| epoch  27 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.65 | ppl   104.44\n",
            "| epoch  27 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.53 | loss  4.66 | ppl   105.98\n",
            "| epoch  27 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.40 | loss  4.68 | ppl   107.53\n",
            "| epoch  27 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.50 | loss  4.65 | ppl   104.40\n",
            "| epoch  27 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.68 | ppl   107.87\n",
            "| epoch  27 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.60 | ppl    99.02\n",
            "| epoch  27 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.63 | loss  4.63 | ppl   102.00\n",
            "| epoch  27 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.52 | loss  4.66 | ppl   105.25\n",
            "| epoch  27 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.49 | loss  4.62 | ppl   101.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 63.76s | valid loss  6.23 | valid ppl   510.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |   200/ 2983 batches | lr 0.001 | ms/batch 20.78 | loss  4.66 | ppl   105.87\n",
            "| epoch  28 |   400/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.65 | ppl   104.65\n",
            "| epoch  28 |   600/ 2983 batches | lr 0.001 | ms/batch 20.56 | loss  4.56 | ppl    95.73\n",
            "| epoch  28 |   800/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.62 | ppl   101.71\n",
            "| epoch  28 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.75 | loss  4.62 | ppl   101.55\n",
            "| epoch  28 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.60 | loss  4.65 | ppl   104.12\n",
            "| epoch  28 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.65 | loss  4.66 | ppl   105.69\n",
            "| epoch  28 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.81 | loss  4.68 | ppl   107.24\n",
            "| epoch  28 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.74 | loss  4.65 | ppl   104.13\n",
            "| epoch  28 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.72 | loss  4.68 | ppl   107.55\n",
            "| epoch  28 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.64 | loss  4.59 | ppl    98.75\n",
            "| epoch  28 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.62 | ppl   101.75\n",
            "| epoch  28 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.71 | loss  4.65 | ppl   104.92\n",
            "| epoch  28 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.69 | loss  4.61 | ppl   100.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 64.16s | valid loss  6.24 | valid ppl   514.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |   200/ 2983 batches | lr 0.001 | ms/batch 20.77 | loss  4.66 | ppl   105.61\n",
            "| epoch  29 |   400/ 2983 batches | lr 0.001 | ms/batch 20.66 | loss  4.65 | ppl   104.40\n",
            "| epoch  29 |   600/ 2983 batches | lr 0.001 | ms/batch 20.89 | loss  4.56 | ppl    95.49\n",
            "| epoch  29 |   800/ 2983 batches | lr 0.001 | ms/batch 20.81 | loss  4.62 | ppl   101.44\n",
            "| epoch  29 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.89 | loss  4.62 | ppl   101.29\n",
            "| epoch  29 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.81 | loss  4.64 | ppl   103.85\n",
            "| epoch  29 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.82 | loss  4.66 | ppl   105.42\n",
            "| epoch  29 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.74 | loss  4.67 | ppl   106.92\n",
            "| epoch  29 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.91 | loss  4.64 | ppl   103.87\n",
            "| epoch  29 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.89 | loss  4.68 | ppl   107.31\n",
            "| epoch  29 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.87 | loss  4.59 | ppl    98.48\n",
            "| epoch  29 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.83 | loss  4.62 | ppl   101.50\n",
            "| epoch  29 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.78 | loss  4.65 | ppl   104.61\n",
            "| epoch  29 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.76 | loss  4.61 | ppl   100.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 64.57s | valid loss  6.25 | valid ppl   517.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |   200/ 2983 batches | lr 0.001 | ms/batch 20.82 | loss  4.66 | ppl   105.36\n",
            "| epoch  30 |   400/ 2983 batches | lr 0.001 | ms/batch 20.83 | loss  4.65 | ppl   104.19\n",
            "| epoch  30 |   600/ 2983 batches | lr 0.001 | ms/batch 20.73 | loss  4.56 | ppl    95.28\n",
            "| epoch  30 |   800/ 2983 batches | lr 0.001 | ms/batch 20.77 | loss  4.62 | ppl   101.19\n",
            "| epoch  30 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.66 | loss  4.62 | ppl   101.05\n",
            "| epoch  30 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.64 | loss  4.64 | ppl   103.59\n",
            "| epoch  30 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.63 | loss  4.66 | ppl   105.17\n",
            "| epoch  30 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.64 | loss  4.67 | ppl   106.67\n",
            "| epoch  30 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.63 | loss  4.64 | ppl   103.62\n",
            "| epoch  30 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.54 | loss  4.67 | ppl   107.03\n",
            "| epoch  30 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.59 | ppl    98.26\n",
            "| epoch  30 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.55 | loss  4.62 | ppl   101.28\n",
            "| epoch  30 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.73 | loss  4.65 | ppl   104.33\n",
            "| epoch  30 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.64 | loss  4.61 | ppl   100.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 64.12s | valid loss  6.26 | valid ppl   521.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  31 |   200/ 2983 batches | lr 0.001 | ms/batch 20.81 | loss  4.66 | ppl   105.12\n",
            "| epoch  31 |   400/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.64 | ppl   103.95\n",
            "| epoch  31 |   600/ 2983 batches | lr 0.001 | ms/batch 20.64 | loss  4.55 | ppl    95.08\n",
            "| epoch  31 |   800/ 2983 batches | lr 0.001 | ms/batch 20.67 | loss  4.61 | ppl   100.93\n",
            "| epoch  31 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.59 | loss  4.61 | ppl   100.82\n",
            "| epoch  31 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.57 | loss  4.64 | ppl   103.36\n",
            "| epoch  31 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.64 | loss  4.65 | ppl   104.93\n",
            "| epoch  31 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.60 | loss  4.67 | ppl   106.38\n",
            "| epoch  31 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.67 | loss  4.64 | ppl   103.36\n",
            "| epoch  31 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.53 | loss  4.67 | ppl   106.80\n",
            "| epoch  31 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.57 | loss  4.59 | ppl    98.01\n",
            "| epoch  31 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.72 | loss  4.62 | ppl   101.06\n",
            "| epoch  31 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.36 | loss  4.64 | ppl   104.06\n",
            "| epoch  31 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.59 | loss  4.61 | ppl   100.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 63.95s | valid loss  6.26 | valid ppl   524.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  32 |   200/ 2983 batches | lr 0.001 | ms/batch 20.70 | loss  4.65 | ppl   104.86\n",
            "| epoch  32 |   400/ 2983 batches | lr 0.001 | ms/batch 20.58 | loss  4.64 | ppl   103.74\n",
            "| epoch  32 |   600/ 2983 batches | lr 0.001 | ms/batch 20.55 | loss  4.55 | ppl    94.89\n",
            "| epoch  32 |   800/ 2983 batches | lr 0.001 | ms/batch 20.42 | loss  4.61 | ppl   100.68\n",
            "| epoch  32 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.50 | loss  4.61 | ppl   100.59\n",
            "| epoch  32 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.46 | loss  4.64 | ppl   103.11\n",
            "| epoch  32 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.64 | loss  4.65 | ppl   104.71\n",
            "| epoch  32 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.63 | loss  4.66 | ppl   106.13\n",
            "| epoch  32 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.63 | loss  4.64 | ppl   103.11\n",
            "| epoch  32 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.60 | loss  4.67 | ppl   106.52\n",
            "| epoch  32 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.48 | loss  4.58 | ppl    97.78\n",
            "| epoch  32 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.51 | loss  4.61 | ppl   100.84\n",
            "| epoch  32 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.63 | loss  4.64 | ppl   103.79\n",
            "| epoch  32 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.55 | loss  4.61 | ppl   100.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 63.81s | valid loss  6.27 | valid ppl   527.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  33 |   200/ 2983 batches | lr 0.001 | ms/batch 20.76 | loss  4.65 | ppl   104.61\n",
            "| epoch  33 |   400/ 2983 batches | lr 0.001 | ms/batch 20.54 | loss  4.64 | ppl   103.50\n",
            "| epoch  33 |   600/ 2983 batches | lr 0.001 | ms/batch 20.43 | loss  4.55 | ppl    94.70\n",
            "| epoch  33 |   800/ 2983 batches | lr 0.001 | ms/batch 20.43 | loss  4.61 | ppl   100.42\n",
            "| epoch  33 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.64 | loss  4.61 | ppl   100.37\n",
            "| epoch  33 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.55 | loss  4.63 | ppl   102.86\n",
            "| epoch  33 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.51 | loss  4.65 | ppl   104.48\n",
            "| epoch  33 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.52 | loss  4.66 | ppl   105.87\n",
            "| epoch  33 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.55 | loss  4.63 | ppl   102.85\n",
            "| epoch  33 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.47 | loss  4.67 | ppl   106.30\n",
            "| epoch  33 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.46 | loss  4.58 | ppl    97.52\n",
            "| epoch  33 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.53 | loss  4.61 | ppl   100.60\n",
            "| epoch  33 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.52 | loss  4.64 | ppl   103.56\n",
            "| epoch  33 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.60 | ppl    99.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 63.73s | valid loss  6.27 | valid ppl   530.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  34 |   200/ 2983 batches | lr 0.001 | ms/batch 20.76 | loss  4.65 | ppl   104.38\n",
            "| epoch  34 |   400/ 2983 batches | lr 0.001 | ms/batch 20.58 | loss  4.64 | ppl   103.31\n",
            "| epoch  34 |   600/ 2983 batches | lr 0.001 | ms/batch 20.57 | loss  4.55 | ppl    94.51\n",
            "| epoch  34 |   800/ 2983 batches | lr 0.001 | ms/batch 20.67 | loss  4.61 | ppl   100.20\n",
            "| epoch  34 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.69 | loss  4.61 | ppl   100.14\n",
            "| epoch  34 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.47 | loss  4.63 | ppl   102.60\n",
            "| epoch  34 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.65 | ppl   104.26\n",
            "| epoch  34 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.63 | loss  4.66 | ppl   105.66\n",
            "| epoch  34 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.72 | loss  4.63 | ppl   102.63\n",
            "| epoch  34 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.74 | loss  4.66 | ppl   106.02\n",
            "| epoch  34 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.63 | loss  4.58 | ppl    97.31\n",
            "| epoch  34 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.61 | ppl   100.37\n",
            "| epoch  34 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.71 | loss  4.64 | ppl   103.33\n",
            "| epoch  34 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.76 | loss  4.60 | ppl    99.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 64.10s | valid loss  6.28 | valid ppl   534.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  35 |   200/ 2983 batches | lr 0.001 | ms/batch 20.95 | loss  4.65 | ppl   104.17\n",
            "| epoch  35 |   400/ 2983 batches | lr 0.001 | ms/batch 20.86 | loss  4.64 | ppl   103.11\n",
            "| epoch  35 |   600/ 2983 batches | lr 0.001 | ms/batch 20.71 | loss  4.55 | ppl    94.31\n",
            "| epoch  35 |   800/ 2983 batches | lr 0.001 | ms/batch 20.66 | loss  4.61 | ppl   100.00\n",
            "| epoch  35 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.63 | loss  4.60 | ppl    99.95\n",
            "| epoch  35 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.74 | loss  4.63 | ppl   102.37\n",
            "| epoch  35 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.82 | loss  4.64 | ppl   104.05\n",
            "| epoch  35 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.87 | loss  4.66 | ppl   105.45\n",
            "| epoch  35 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.84 | loss  4.63 | ppl   102.43\n",
            "| epoch  35 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.87 | loss  4.66 | ppl   105.84\n",
            "| epoch  35 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.98 | loss  4.58 | ppl    97.09\n",
            "| epoch  35 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.70 | loss  4.61 | ppl   100.14\n",
            "| epoch  35 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.90 | loss  4.64 | ppl   103.16\n",
            "| epoch  35 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.72 | loss  4.60 | ppl    99.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 64.54s | valid loss  6.29 | valid ppl   537.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  36 |   200/ 2983 batches | lr 0.001 | ms/batch 20.94 | loss  4.64 | ppl   103.98\n",
            "| epoch  36 |   400/ 2983 batches | lr 0.001 | ms/batch 20.71 | loss  4.63 | ppl   102.99\n",
            "| epoch  36 |   600/ 2983 batches | lr 0.001 | ms/batch 20.71 | loss  4.55 | ppl    94.17\n",
            "| epoch  36 |   800/ 2983 batches | lr 0.001 | ms/batch 20.78 | loss  4.60 | ppl    99.85\n",
            "| epoch  36 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.74 | loss  4.60 | ppl    99.79\n",
            "| epoch  36 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.91 | loss  4.63 | ppl   102.16\n",
            "| epoch  36 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.85 | loss  4.64 | ppl   103.88\n",
            "| epoch  36 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.64 | loss  4.66 | ppl   105.32\n",
            "| epoch  36 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.83 | loss  4.63 | ppl   102.28\n",
            "| epoch  36 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.91 | loss  4.66 | ppl   105.63\n",
            "| epoch  36 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.87 | loss  4.57 | ppl    96.96\n",
            "| epoch  36 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.82 | loss  4.60 | ppl    99.96\n",
            "| epoch  36 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.83 | loss  4.63 | ppl   103.01\n",
            "| epoch  36 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.84 | loss  4.60 | ppl    99.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 64.57s | valid loss  6.29 | valid ppl   540.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  37 |   200/ 2983 batches | lr 0.001 | ms/batch 20.97 | loss  4.64 | ppl   103.85\n",
            "| epoch  37 |   400/ 2983 batches | lr 0.001 | ms/batch 20.89 | loss  4.63 | ppl   102.89\n",
            "| epoch  37 |   600/ 2983 batches | lr 0.001 | ms/batch 20.85 | loss  4.54 | ppl    94.05\n",
            "| epoch  37 |   800/ 2983 batches | lr 0.001 | ms/batch 20.83 | loss  4.60 | ppl    99.75\n",
            "| epoch  37 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.83 | loss  4.60 | ppl    99.69\n",
            "| epoch  37 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.86 | loss  4.63 | ppl   102.02\n",
            "| epoch  37 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.92 | loss  4.64 | ppl   103.74\n",
            "| epoch  37 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.82 | loss  4.66 | ppl   105.21\n",
            "| epoch  37 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.84 | loss  4.63 | ppl   102.16\n",
            "| epoch  37 |  2000/ 2983 batches | lr 0.001 | ms/batch 21.03 | loss  4.66 | ppl   105.56\n",
            "| epoch  37 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.80 | loss  4.57 | ppl    96.84\n",
            "| epoch  37 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.89 | loss  4.60 | ppl    99.82\n",
            "| epoch  37 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.85 | loss  4.63 | ppl   102.90\n",
            "| epoch  37 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.89 | loss  4.60 | ppl    99.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 64.77s | valid loss  6.30 | valid ppl   542.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  38 |   200/ 2983 batches | lr 0.001 | ms/batch 21.06 | loss  4.64 | ppl   103.75\n",
            "| epoch  38 |   400/ 2983 batches | lr 0.001 | ms/batch 20.77 | loss  4.63 | ppl   102.85\n",
            "| epoch  38 |   600/ 2983 batches | lr 0.001 | ms/batch 20.88 | loss  4.54 | ppl    94.00\n",
            "| epoch  38 |   800/ 2983 batches | lr 0.001 | ms/batch 20.77 | loss  4.60 | ppl    99.69\n",
            "| epoch  38 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.90 | loss  4.60 | ppl    99.61\n",
            "| epoch  38 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.75 | loss  4.62 | ppl   101.92\n",
            "| epoch  38 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.79 | loss  4.64 | ppl   103.64\n",
            "| epoch  38 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.65 | loss  4.66 | ppl   105.17\n",
            "| epoch  38 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.79 | loss  4.63 | ppl   102.08\n",
            "| epoch  38 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.69 | loss  4.66 | ppl   105.42\n",
            "| epoch  38 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.73 | loss  4.57 | ppl    96.80\n",
            "| epoch  38 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.53 | loss  4.60 | ppl    99.74\n",
            "| epoch  38 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.72 | loss  4.63 | ppl   102.81\n",
            "| epoch  38 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.60 | ppl    99.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 64.36s | valid loss  6.30 | valid ppl   545.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  39 |   200/ 2983 batches | lr 0.001 | ms/batch 20.84 | loss  4.64 | ppl   103.68\n",
            "| epoch  39 |   400/ 2983 batches | lr 0.001 | ms/batch 20.56 | loss  4.63 | ppl   102.79\n",
            "| epoch  39 |   600/ 2983 batches | lr 0.001 | ms/batch 20.74 | loss  4.54 | ppl    93.97\n",
            "| epoch  39 |   800/ 2983 batches | lr 0.001 | ms/batch 20.76 | loss  4.60 | ppl    99.66\n",
            "| epoch  39 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.74 | loss  4.60 | ppl    99.57\n",
            "| epoch  39 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.53 | loss  4.62 | ppl   101.88\n",
            "| epoch  39 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.60 | loss  4.64 | ppl   103.56\n",
            "| epoch  39 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.66 | loss  4.66 | ppl   105.12\n",
            "| epoch  39 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.75 | loss  4.63 | ppl   102.04\n",
            "| epoch  39 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.63 | loss  4.66 | ppl   105.45\n",
            "| epoch  39 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.64 | loss  4.57 | ppl    96.74\n",
            "| epoch  39 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.60 | ppl    99.69\n",
            "| epoch  39 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.60 | loss  4.63 | ppl   102.77\n",
            "| epoch  39 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.60 | loss  4.60 | ppl    99.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 64.05s | valid loss  6.31 | valid ppl   547.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  40 |   200/ 2983 batches | lr 0.001 | ms/batch 20.90 | loss  4.64 | ppl   103.62\n",
            "| epoch  40 |   400/ 2983 batches | lr 0.001 | ms/batch 20.72 | loss  4.63 | ppl   102.78\n",
            "| epoch  40 |   600/ 2983 batches | lr 0.001 | ms/batch 20.57 | loss  4.54 | ppl    93.97\n",
            "| epoch  40 |   800/ 2983 batches | lr 0.001 | ms/batch 20.50 | loss  4.60 | ppl    99.66\n",
            "| epoch  40 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.60 | ppl    99.52\n",
            "| epoch  40 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.57 | loss  4.62 | ppl   101.83\n",
            "| epoch  40 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.58 | loss  4.64 | ppl   103.52\n",
            "| epoch  40 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.69 | loss  4.66 | ppl   105.13\n",
            "| epoch  40 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.56 | loss  4.63 | ppl   102.01\n",
            "| epoch  40 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.61 | loss  4.66 | ppl   105.35\n",
            "| epoch  40 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.64 | loss  4.57 | ppl    96.73\n",
            "| epoch  40 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.73 | loss  4.60 | ppl    99.67\n",
            "| epoch  40 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.62 | loss  4.63 | ppl   102.72\n",
            "| epoch  40 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.72 | loss  4.60 | ppl    99.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 64.04s | valid loss  6.31 | valid ppl   550.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  5.73 | test ppl   308.45\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1C6Z9PgoQox",
        "colab_type": "text"
      },
      "source": [
        "### generate.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OVH35EaoiKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = './data/wikitext-2'  # location of the data corpus\n",
        "checkpoint = './model.pt'   # model checkpoint to use\n",
        "outf = 'generated.txt'      # output file for generated text\n",
        "words = 10                # number of words to generate\n",
        "seed = 1111                 # random seed\n",
        "cuda = True                 # use CUDA\n",
        "temperature = 1.0           # temperature - higher will increase diversity\n",
        "log_interval = 100          # reporting interval"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZzhYEfIoWtw",
        "colab_type": "code",
        "outputId": "46462ed3-acca-4ab5-cb26-5da118274ea4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "###############################################################################\n",
        "# Language Modeling on Wikitext-2\n",
        "#\n",
        "# This file generates new sentences sampled from the language model\n",
        "#\n",
        "###############################################################################\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "  if not cuda:\n",
        "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "\n",
        "device = torch.device('cuda' if cuda else 'cpu')\n",
        "\n",
        "# if temperature < 1e-3:\n",
        "#   # parser.error('--temperature has to be greater or equal 1e-3')\n",
        "\n",
        "with open(checkpoint, 'rb') as f:\n",
        "  model = torch.load(f).to(device)\n",
        "model.eval()\n",
        "\n",
        "corpus = Corpus(data)\n",
        "ntokens = len(corpus.dictionary)\n",
        "\n",
        "is_transformer_model = hasattr(model, 'model_type') and model.model_type == 'Transformer'\n",
        "if not is_transformer_model and args_model != 'FFN':\n",
        "  hidden = model.init_hidden(1)\n",
        "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "with open(outf, 'w') as outf:\n",
        "  with torch.no_grad(): # no tracking history\n",
        "    for i in range(words):\n",
        "      if is_transformer_model:\n",
        "        output = model(input, False)\n",
        "        word_weights = output[-1].squeeze().div(temperature).exp().cpu()\n",
        "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "        word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
        "        input = torch.cat([input, word_tensor], 0)\n",
        "      elif args_model == 'FFN':\n",
        "        output = model(input)\n",
        "      else:\n",
        "        output, hidden = model(input, hidden)\n",
        "        word_weights = output.squeeze().div(temperature).exp().cpu()\n",
        "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "        input.fill_(word_idx)\n",
        "\n",
        "      word = corpus.dictionary.idx2word[word_idx]\n",
        "\n",
        "      outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "\n",
        "      if i % log_interval == 0:\n",
        "        print('| Generated {}/{} words'.format(i, words))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-daf0ce126f24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mis_transformer_model\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0margs_model\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'FFN'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mword_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mword_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eubAEEWVs5Ei",
        "colab_type": "code",
        "outputId": "20ec705b-fabb-4edd-cf5b-8ef9369688be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        }
      },
      "source": [
        "!cat generated.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "outside of Max and Chorus for purpose . Wippleman was the only Dutchman known in 1980 to explain his father\n",
            "\" Maroon 3 \" . New of the Philadelphia Provident Paper entered the initial reflection of the book . \"\n",
            "<unk> \" was moved to updating the peace format but compared his endanger of the reality sphere and Nahant to\n",
            "communicate with many veterans of Marquis Turing 's virtual decision . <eos> Keith Lovell stated that early half of the\n",
            "projects were the \" most notable sexually <unk> as 1902 for the time . \" These \" actors carried agents\n",
            "critics , both with a <unk> touched , organized production while Virgil , prone to marsh and green <unk> synths\n",
            ", or a young and repeatedly thin @-@ digit object offered a beer curtain . \" <eos> The track \"\n",
            "notorious \" with a music game was serialized and a female editor he was from Los Angeles to sound from\n",
            "studio art colors , from which the third Guy Fairies of the Association to @-@ East Division followed . In\n",
            "2013 , Anderson invited to fund the Reich 's speech to Computer Entertainment board by American archaeologist Brown that were\n",
            "then plagiarized only by <unk> . Strobel 's Goldie started to audition after a time he presided by having they\n",
            "look before . Also , the portrayal of the camera . Those on but obliged to experience at a happy\n",
            "line is , more obscured together , and often Catherine , abandon the current order and <unk> drastically enforced the\n",
            "mortars . This reflects Lithuanians that he was wanting to enter that happening the \" Good Smoke \" and \"\n",
            "closer to an stood \" . When his father retrieve two in the forthcoming episodes , Lady Bracknell . For\n",
            "instance , he delivers his head and hiring on the ninth @-@ time which selects \" <unk> and lush study\n",
            "\" , so many the locals list the sidelines — and directly it no longer be himself being trying to\n",
            "think , respectively . In the last round , \" and A medium called <unk> which you must 've come\n",
            "up from <unk> \" . The <unk> of Liberty is heavily influenced by other writers from Eurasia , instead of\n",
            "where he said \" To me it had no dramatic physical <unk> . It found another thing ' . \"\n",
            "\" That may have superstitious racial , relaunched TV , is referred to as \" . <eos> <eos> = =\n",
            "= Involvement in Radical Dreamers = = = <eos> <eos> All north of the Red Revue Rose 's , the\n",
            "intermediate face and <unk> of the Fringe riff , reflected to the 45 @-@ page point nine times into the\n",
            "game 's 2008 documentary . The largest obviously taken agaric included various field events , while a week number pre\n",
            "@-@ released scientific features \" scatter \" , and employed a value of sequels for the second week . The\n",
            "director and <unk> scene , into other proponents , included members of \" Elementary Bell \" , its \" only\n",
            "of the show turning admirable positions \" , and \" I Are Must Neighbors ? , save Bell \" .\n",
            "When the initial release was <unk> , he was surprised by his two kids to see , singing God from\n",
            "other characters . <eos> <eos> = = Reception = = <eos> <eos> Although he was written \" , fans didn\n",
            "'t think that in less than game years . \" A messenger accused van Hunt ( triple \" chop @-@\n",
            "UFO @-@ plate elements \" , primarily \" in Australia and bold across Goro \" as they showcased the number\n",
            "of music adventure games , such as two DVDs , stories from the title They , which within later appearances\n",
            "in the series . GamePro praised his remarks , but this Observer expected . When that were not Lucius derives\n",
            "from the character , writers Sarah <unk> called him \" the very best tradition of the stereotypes \" of GameSpot\n",
            "at <unk> . He commented critically songs of a Thunderbirds show series \" Unnatural Selection \" , describing with the\n",
            "same game and were permitted . By the Goodwood Game <unk> , Léon <unk> praised the plot of Nick Johnson\n",
            "'s crew . Towards the end of Game 2 chart Nottinghamshire Josh Sid Sheridan said that this \" behavior can\n",
            "not be vindicated , \" attempts to negotiate <unk> <unk> . 1827 that Mac of the participants would also have\n",
            "page tuning with more easy and indirect bouncers between computers and its performance , including on the existing stage .\n",
            "<eos> On 14 November the preview of the EU director Aerosmith <unk> between Boca Aviv and its United States ,\n",
            "Michigan 's faiths in those released transits . The version has resolved not neither E3 – July 2005 . The\n",
            "film 's main work include Musical Talk On March 25 , 2015 , along with Video NPR . unlike Kilmer\n",
            ", in a four @-@ second DVD of the scenario in All Star Feelings , the \" December , Million\n",
            "@-@ TV of the 1920s \" ( 1999 ) , also praised Jordan 's decision — but added \" When\n",
            "the <unk> numbered the notion we see their marvelous and Logic <unk> to be that <eos> The staff look like\n",
            "the Lim Goldberg @-@ Miranic Season Finds of <unk> , as he can are the most memorable <unk> ever used\n",
            "to tell Senjō for his persuasive the very right , on hopeful the way and \" being the \" [\n",
            "ing ] Madonna BAFTA \" for a variety of player characters over the adhered to any male ; Crash ,\n",
            "and Nottinghamshire , the types of characters and their name . <eos> The work of comic games are the second\n",
            "worn by Mario positions in the United States . Numbers are \" Counterfeit \" , \" stale and padded and\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}