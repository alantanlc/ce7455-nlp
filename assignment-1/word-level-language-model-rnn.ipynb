{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "word-level-language-model-rnn.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alanwuha/ce7455-nlp/blob/master/assignment-1/word-level-language-model-rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIaIZE6es3jc",
        "colab_type": "text"
      },
      "source": [
        "# Question Two\n",
        "\n",
        "Please make sure to run all cells in order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXyEv8NOs-rl",
        "colab_type": "text"
      },
      "source": [
        "## (i - ii) Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsyBbtWr0ZaX",
        "colab_type": "text"
      },
      "source": [
        "### Download wikitext-2 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oHSqqxz0c4Q",
        "colab_type": "code",
        "outputId": "27bac362-c341-409f-803c-0e6bbf2c572b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!rm -r data\n",
        "!mkdir data\n",
        "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip -P /content/data\n",
        "!unzip /content/data/wikitext-2-v1.zip -d /content/data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-09 03:01:57--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.137.54\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.137.54|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4475746 (4.3M) [application/zip]\n",
            "Saving to: ‘/content/data/wikitext-2-v1.zip’\n",
            "\n",
            "\rwikitext-2-v1.zip     0%[                    ]       0  --.-KB/s               \rwikitext-2-v1.zip    48%[========>           ]   2.06M  10.2MB/s               \rwikitext-2-v1.zip   100%[===================>]   4.27M  18.5MB/s    in 0.2s    \n",
            "\n",
            "2020-02-09 03:01:57 (18.5 MB/s) - ‘/content/data/wikitext-2-v1.zip’ saved [4475746/4475746]\n",
            "\n",
            "Archive:  /content/data/wikitext-2-v1.zip\n",
            "   creating: /content/data/wikitext-2/\n",
            "  inflating: /content/data/wikitext-2/wiki.test.tokens  \n",
            "  inflating: /content/data/wikitext-2/wiki.valid.tokens  \n",
            "  inflating: /content/data/wikitext-2/wiki.train.tokens  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5fcltKuqDoR",
        "colab_type": "text"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqiyNq_4qGkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.onnx\n",
        "import argparse\n",
        "import time\n",
        "import os\n",
        "from io import open"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCOTNafMg_z5",
        "colab_type": "text"
      },
      "source": [
        "### Specify arg variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrEgZw42hCv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = './data/wikitext-2'  # location of the data corpus\n",
        "args_model = 'FNN'          # type of recurrent net\n",
        "emsize = 200                # size of word embeddings\n",
        "nhid = 200                  # number of hidden units per layer\n",
        "nlayers = 2                 # number of layers\n",
        "lr = 20                     # initial learning rate\n",
        "clip = 0.25                 # gradient clipping\n",
        "epochs = 40                 # upper epoch limit\n",
        "batch_size = 20             # batch size\n",
        "bptt = 35                   # sequence length\n",
        "dropout = 0.2               # dropout applied to layers (0 = no dropout)\n",
        "tied = False                # tie the word embedding and softmax weights\n",
        "seed = 1111                 # random seed\n",
        "cuda = True                 # use CUDA\n",
        "log_interval = 200          # report interval\n",
        "save = 'model.pt'           # path to save the final model\n",
        "onnx_export = ''            # path to export the final model in onnx format\n",
        "nhead = 2                   # the number of heads in the encoder/decoder of the transformer model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M0KYJXn0htA",
        "colab_type": "text"
      },
      "source": [
        "### Data Preprocessing Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_krRf2j0kzQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'wiki.train.tokens'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'wiki.valid.tokens'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'wiki.test.tokens'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            idss = []\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                ids = []\n",
        "                for word in words:\n",
        "                    ids.append(self.dictionary.word2idx[word])\n",
        "                idss.append(torch.tensor(ids).type(torch.int64))\n",
        "            ids = torch.cat(idss)\n",
        "\n",
        "        return ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNA9aDE-0VhB",
        "colab_type": "text"
      },
      "source": [
        "### Helper Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVk0qOGJo8dg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "\n",
        "# Starting from sequential data, batchify arranges the dataset into columns.\n",
        "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
        "# ┌ a g m s ┐\n",
        "# │ b h n t │\n",
        "# │ c i o u │\n",
        "# │ d j p v │\n",
        "# │ e k q w │\n",
        "# └ f l r x ┘.\n",
        "# These columns are treated as independent by the model, which means that the\n",
        "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
        "# batch processing.\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "\n",
        "# get_batch subdivides the source data into chunks of length bptt.\n",
        "# If source is equal to the example output of the batchify function, with\n",
        "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
        "# ┌ a g m s ┐ ┌ b h n t ┐\n",
        "# └ b h n t ┘ └ c i o u ┘\n",
        "# Note that despite the name of the function, the subdivison of data is not\n",
        "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
        "# by the batchify function. The chunks are along dimension 0, corresponding\n",
        "# to the seq_len dimension in the LSTM.\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    if args_model not in ['Transformer', 'FNN']:\n",
        "        hidden = model.init_hidden(eval_batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            if args_model in ['Transformer', 'FNN']:\n",
        "                output = model(data)\n",
        "            else:\n",
        "                output, hidden = model(data, hidden)\n",
        "                hidden = repackage_hidden(hidden)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    if args_model not in ['Transformer', 'FNN']:\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        # model.zero_grad()\n",
        "        \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output\n",
        "        if args_model in ['Transformer', 'FNN']:\n",
        "            output = model(data)\n",
        "        else:\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            output, hidden = model(data, hidden)\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        # for p in model.parameters():\n",
        "        #     p.data.add_(-lr, p.grad.data)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.3f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // bptt, optimizer.param_groups[0]['lr'],\n",
        "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "\n",
        "def export_onnx(path, batch_size, seq_len):\n",
        "    print('The model is also exported in ONNX format at {}'.\n",
        "          format(os.path.realpath(args.onnx_export)))\n",
        "    model.eval()\n",
        "    dummy_input = torch.LongTensor(seq_len * batch_size).zero_().view(-1, batch_size).to(device)\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    torch.onnx.export(model, (dummy_input, hidden), path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-w17AZJp2qf",
        "colab_type": "text"
      },
      "source": [
        "### Set torch seed and CUDA device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJGy7JGHqe8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Set cuda device\n",
        "if torch.cuda.is_available():\n",
        "    if not cuda:\n",
        "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GY7McFzqnaR",
        "colab_type": "text"
      },
      "source": [
        "### Load corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig6BmYbWqq0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = Corpus(data)\n",
        "\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(corpus.train, batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "ntokens = len(corpus.dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSxxbklN0moQ",
        "colab_type": "text"
      },
      "source": [
        "## (iii) Write a _class FNNModel(nn.Module)_.\n",
        "\n",
        "The FNNModel should implement a language model with a feed-forward network architecture. It has a hidden layer with tanh architecture and the output layer is a Softmax layer. The output of the model for each input of (n-1) previous word indices are the probabilities of the |_V_| words in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSAjXOz71l80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FNNModel(nn.Module):\n",
        "    \"\"\" Container module with an encoder, a feed forward module, and a decoder. \"\"\"\n",
        "\n",
        "    def __init__(self, ntokens, emsize, hidden_size, tie_weights=False):\n",
        "        super(FNNModel, self).__init__()\n",
        "        self.encoder = nn.Embedding(ntokens, emsize)\n",
        "        self.hidden = nn.Linear(emsize, hidden_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.decoder = nn.Linear(hidden_size, ntokens)\n",
        "\n",
        "        if tie_weights:\n",
        "            if hidden_size != emsize:\n",
        "                raise ValueError('When using the tied flag, hidden_size must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        # self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input):\n",
        "        emb = self.encoder(input)\n",
        "        hid = self.hidden(emb)\n",
        "        output = self.tanh(hid)\n",
        "        decoded = self.decoder(output)\n",
        "        return decoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvvSa_5Sqtyp",
        "colab_type": "text"
      },
      "source": [
        "## (iv-1) Train the model with Adam optimizer (without sharing weights)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipzOYeQy0W0a",
        "colab_type": "code",
        "outputId": "247dc719-942c-416c-9644-95568f34f4ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tied = False\n",
        "save = 'model_untied.pt'\n",
        "model = FNNModel(ntokens, emsize, nhid, tied).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "# Loop over epochs.\n",
        "best_val_loss = None\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 2983 batches | lr 0.001 | ms/batch 18.54 | loss  8.08 | ppl  3234.78\n",
            "| epoch   1 |   400/ 2983 batches | lr 0.001 | ms/batch 17.40 | loss  6.59 | ppl   728.55\n",
            "| epoch   1 |   600/ 2983 batches | lr 0.001 | ms/batch 17.45 | loss  6.38 | ppl   592.88\n",
            "| epoch   1 |   800/ 2983 batches | lr 0.001 | ms/batch 17.50 | loss  6.31 | ppl   549.91\n",
            "| epoch   1 |  1000/ 2983 batches | lr 0.001 | ms/batch 17.57 | loss  6.23 | ppl   507.14\n",
            "| epoch   1 |  1200/ 2983 batches | lr 0.001 | ms/batch 17.58 | loss  6.22 | ppl   503.70\n",
            "| epoch   1 |  1400/ 2983 batches | lr 0.001 | ms/batch 17.63 | loss  6.20 | ppl   490.41\n",
            "| epoch   1 |  1600/ 2983 batches | lr 0.001 | ms/batch 17.67 | loss  6.18 | ppl   485.13\n",
            "| epoch   1 |  1800/ 2983 batches | lr 0.001 | ms/batch 17.70 | loss  6.07 | ppl   430.87\n",
            "| epoch   1 |  2000/ 2983 batches | lr 0.001 | ms/batch 17.75 | loss  6.10 | ppl   444.95\n",
            "| epoch   1 |  2200/ 2983 batches | lr 0.001 | ms/batch 17.77 | loss  6.00 | ppl   401.94\n",
            "| epoch   1 |  2400/ 2983 batches | lr 0.001 | ms/batch 17.81 | loss  6.03 | ppl   414.26\n",
            "| epoch   1 |  2600/ 2983 batches | lr 0.001 | ms/batch 17.88 | loss  6.01 | ppl   408.67\n",
            "| epoch   1 |  2800/ 2983 batches | lr 0.001 | ms/batch 17.91 | loss  5.95 | ppl   382.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 54.81s | valid loss  5.89 | valid ppl   361.36\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type FNNModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   2 |   200/ 2983 batches | lr 0.001 | ms/batch 18.07 | loss  5.73 | ppl   307.82\n",
            "| epoch   2 |   400/ 2983 batches | lr 0.001 | ms/batch 18.04 | loss  5.68 | ppl   292.37\n",
            "| epoch   2 |   600/ 2983 batches | lr 0.001 | ms/batch 18.06 | loss  5.56 | ppl   258.84\n",
            "| epoch   2 |   800/ 2983 batches | lr 0.001 | ms/batch 18.12 | loss  5.59 | ppl   268.52\n",
            "| epoch   2 |  1000/ 2983 batches | lr 0.001 | ms/batch 18.15 | loss  5.56 | ppl   259.41\n",
            "| epoch   2 |  1200/ 2983 batches | lr 0.001 | ms/batch 18.20 | loss  5.58 | ppl   264.33\n",
            "| epoch   2 |  1400/ 2983 batches | lr 0.001 | ms/batch 18.25 | loss  5.61 | ppl   272.79\n",
            "| epoch   2 |  1600/ 2983 batches | lr 0.001 | ms/batch 18.27 | loss  5.61 | ppl   271.92\n",
            "| epoch   2 |  1800/ 2983 batches | lr 0.001 | ms/batch 18.29 | loss  5.52 | ppl   250.46\n",
            "| epoch   2 |  2000/ 2983 batches | lr 0.001 | ms/batch 18.36 | loss  5.58 | ppl   264.73\n",
            "| epoch   2 |  2200/ 2983 batches | lr 0.001 | ms/batch 18.40 | loss  5.48 | ppl   239.13\n",
            "| epoch   2 |  2400/ 2983 batches | lr 0.001 | ms/batch 18.43 | loss  5.51 | ppl   247.68\n",
            "| epoch   2 |  2600/ 2983 batches | lr 0.001 | ms/batch 18.46 | loss  5.53 | ppl   251.63\n",
            "| epoch   2 |  2800/ 2983 batches | lr 0.001 | ms/batch 18.52 | loss  5.48 | ppl   239.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 56.51s | valid loss  5.83 | valid ppl   338.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2983 batches | lr 0.001 | ms/batch 18.71 | loss  5.41 | ppl   223.24\n",
            "| epoch   3 |   400/ 2983 batches | lr 0.001 | ms/batch 18.63 | loss  5.38 | ppl   216.74\n",
            "| epoch   3 |   600/ 2983 batches | lr 0.001 | ms/batch 18.65 | loss  5.27 | ppl   193.82\n",
            "| epoch   3 |   800/ 2983 batches | lr 0.001 | ms/batch 18.77 | loss  5.32 | ppl   205.20\n",
            "| epoch   3 |  1000/ 2983 batches | lr 0.001 | ms/batch 18.79 | loss  5.30 | ppl   200.52\n",
            "| epoch   3 |  1200/ 2983 batches | lr 0.001 | ms/batch 18.84 | loss  5.32 | ppl   204.58\n",
            "| epoch   3 |  1400/ 2983 batches | lr 0.001 | ms/batch 18.90 | loss  5.36 | ppl   212.01\n",
            "| epoch   3 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.10 | loss  5.36 | ppl   212.31\n",
            "| epoch   3 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.39 | loss  5.29 | ppl   198.04\n",
            "| epoch   3 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.42 | loss  5.34 | ppl   208.98\n",
            "| epoch   3 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.29 | loss  5.24 | ppl   188.86\n",
            "| epoch   3 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.50 | loss  5.28 | ppl   195.56\n",
            "| epoch   3 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.03 | loss  5.30 | ppl   200.25\n",
            "| epoch   3 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.02 | loss  5.25 | ppl   190.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 58.77s | valid loss  5.83 | valid ppl   341.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/ 2983 batches | lr 0.001 | ms/batch 19.18 | loss  5.22 | ppl   185.65\n",
            "| epoch   4 |   400/ 2983 batches | lr 0.001 | ms/batch 18.93 | loss  5.21 | ppl   183.06\n",
            "| epoch   4 |   600/ 2983 batches | lr 0.001 | ms/batch 19.02 | loss  5.10 | ppl   164.46\n",
            "| epoch   4 |   800/ 2983 batches | lr 0.001 | ms/batch 18.97 | loss  5.16 | ppl   174.76\n",
            "| epoch   4 |  1000/ 2983 batches | lr 0.001 | ms/batch 18.97 | loss  5.15 | ppl   172.37\n",
            "| epoch   4 |  1200/ 2983 batches | lr 0.001 | ms/batch 18.87 | loss  5.17 | ppl   175.62\n",
            "| epoch   4 |  1400/ 2983 batches | lr 0.001 | ms/batch 18.93 | loss  5.20 | ppl   181.77\n",
            "| epoch   4 |  1600/ 2983 batches | lr 0.001 | ms/batch 18.98 | loss  5.21 | ppl   182.61\n",
            "| epoch   4 |  1800/ 2983 batches | lr 0.001 | ms/batch 18.92 | loss  5.15 | ppl   172.12\n",
            "| epoch   4 |  2000/ 2983 batches | lr 0.001 | ms/batch 18.94 | loss  5.20 | ppl   180.78\n",
            "| epoch   4 |  2200/ 2983 batches | lr 0.001 | ms/batch 18.92 | loss  5.10 | ppl   163.62\n",
            "| epoch   4 |  2400/ 2983 batches | lr 0.001 | ms/batch 18.99 | loss  5.13 | ppl   169.10\n",
            "| epoch   4 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.01 | loss  5.16 | ppl   174.01\n",
            "| epoch   4 |  2800/ 2983 batches | lr 0.001 | ms/batch 18.90 | loss  5.11 | ppl   165.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 58.63s | valid loss  5.86 | valid ppl   349.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/ 2983 batches | lr 0.001 | ms/batch 19.15 | loss  5.10 | ppl   164.76\n",
            "| epoch   5 |   400/ 2983 batches | lr 0.001 | ms/batch 19.06 | loss  5.10 | ppl   163.65\n",
            "| epoch   5 |   600/ 2983 batches | lr 0.001 | ms/batch 19.02 | loss  4.99 | ppl   147.61\n",
            "| epoch   5 |   800/ 2983 batches | lr 0.001 | ms/batch 19.01 | loss  5.06 | ppl   156.90\n",
            "| epoch   5 |  1000/ 2983 batches | lr 0.001 | ms/batch 18.91 | loss  5.05 | ppl   155.55\n",
            "| epoch   5 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.02 | loss  5.07 | ppl   158.41\n",
            "| epoch   5 |  1400/ 2983 batches | lr 0.001 | ms/batch 18.94 | loss  5.10 | ppl   163.67\n",
            "| epoch   5 |  1600/ 2983 batches | lr 0.001 | ms/batch 18.92 | loss  5.10 | ppl   164.63\n",
            "| epoch   5 |  1800/ 2983 batches | lr 0.001 | ms/batch 18.96 | loss  5.05 | ppl   156.43\n",
            "| epoch   5 |  2000/ 2983 batches | lr 0.001 | ms/batch 18.91 | loss  5.10 | ppl   163.64\n",
            "| epoch   5 |  2200/ 2983 batches | lr 0.001 | ms/batch 18.96 | loss  5.00 | ppl   148.30\n",
            "| epoch   5 |  2400/ 2983 batches | lr 0.001 | ms/batch 18.93 | loss  5.03 | ppl   153.14\n",
            "| epoch   5 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.02 | loss  5.06 | ppl   158.00\n",
            "| epoch   5 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.06 | loss  5.02 | ppl   150.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 58.72s | valid loss  5.89 | valid ppl   359.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/ 2983 batches | lr 0.001 | ms/batch 19.13 | loss  5.02 | ppl   151.67\n",
            "| epoch   6 |   400/ 2983 batches | lr 0.001 | ms/batch 19.08 | loss  5.02 | ppl   150.96\n",
            "| epoch   6 |   600/ 2983 batches | lr 0.001 | ms/batch 19.23 | loss  4.92 | ppl   136.67\n",
            "| epoch   6 |   800/ 2983 batches | lr 0.001 | ms/batch 18.97 | loss  4.98 | ppl   145.26\n",
            "| epoch   6 |  1000/ 2983 batches | lr 0.001 | ms/batch 19.26 | loss  4.97 | ppl   144.32\n",
            "| epoch   6 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.21 | loss  4.99 | ppl   147.11\n",
            "| epoch   6 |  1400/ 2983 batches | lr 0.001 | ms/batch 18.97 | loss  5.02 | ppl   151.65\n",
            "| epoch   6 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.05 | loss  5.03 | ppl   152.65\n",
            "| epoch   6 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.27 | loss  4.98 | ppl   145.83\n",
            "| epoch   6 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.20 | loss  5.03 | ppl   152.22\n",
            "| epoch   6 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.06 | loss  4.93 | ppl   138.11\n",
            "| epoch   6 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.15 | loss  4.96 | ppl   142.50\n",
            "| epoch   6 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.36 | loss  4.99 | ppl   147.27\n",
            "| epoch   6 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.07 | loss  4.95 | ppl   140.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 59.16s | valid loss  5.91 | valid ppl   369.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/ 2983 batches | lr 0.001 | ms/batch 19.41 | loss  4.96 | ppl   142.69\n",
            "| epoch   7 |   400/ 2983 batches | lr 0.001 | ms/batch 19.13 | loss  4.96 | ppl   141.96\n",
            "| epoch   7 |   600/ 2983 batches | lr 0.001 | ms/batch 19.21 | loss  4.86 | ppl   128.99\n",
            "| epoch   7 |   800/ 2983 batches | lr 0.001 | ms/batch 19.07 | loss  4.92 | ppl   137.05\n",
            "| epoch   7 |  1000/ 2983 batches | lr 0.001 | ms/batch 19.13 | loss  4.92 | ppl   136.38\n",
            "| epoch   7 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.04 | loss  4.94 | ppl   139.10\n",
            "| epoch   7 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.07 | loss  4.96 | ppl   143.08\n",
            "| epoch   7 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.14 | loss  4.97 | ppl   144.12\n",
            "| epoch   7 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.27 | loss  4.93 | ppl   138.17\n",
            "| epoch   7 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.32 | loss  4.97 | ppl   144.06\n",
            "| epoch   7 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.41 | loss  4.87 | ppl   130.86\n",
            "| epoch   7 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.09 | loss  4.90 | ppl   134.93\n",
            "| epoch   7 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.41 | loss  4.94 | ppl   139.63\n",
            "| epoch   7 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.21 | loss  4.89 | ppl   133.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 59.40s | valid loss  5.94 | valid ppl   380.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/ 2983 batches | lr 0.001 | ms/batch 19.47 | loss  4.91 | ppl   136.14\n",
            "| epoch   8 |   400/ 2983 batches | lr 0.001 | ms/batch 18.95 | loss  4.91 | ppl   135.27\n",
            "| epoch   8 |   600/ 2983 batches | lr 0.001 | ms/batch 19.39 | loss  4.81 | ppl   123.27\n",
            "| epoch   8 |   800/ 2983 batches | lr 0.001 | ms/batch 19.28 | loss  4.87 | ppl   130.93\n",
            "| epoch   8 |  1000/ 2983 batches | lr 0.001 | ms/batch 19.15 | loss  4.87 | ppl   130.45\n",
            "| epoch   8 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.16 | loss  4.89 | ppl   133.04\n",
            "| epoch   8 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.35 | loss  4.92 | ppl   136.74\n",
            "| epoch   8 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.03 | loss  4.93 | ppl   137.78\n",
            "| epoch   8 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.15 | loss  4.89 | ppl   132.39\n",
            "| epoch   8 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.00 | loss  4.93 | ppl   137.84\n",
            "| epoch   8 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.02 | loss  4.83 | ppl   125.39\n",
            "| epoch   8 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.09 | loss  4.86 | ppl   129.29\n",
            "| epoch   8 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.24 | loss  4.90 | ppl   133.87\n",
            "| epoch   8 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.39 | loss  4.85 | ppl   128.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 59.33s | valid loss  5.97 | valid ppl   390.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/ 2983 batches | lr 0.001 | ms/batch 19.25 | loss  4.88 | ppl   131.15\n",
            "| epoch   9 |   400/ 2983 batches | lr 0.001 | ms/batch 19.15 | loss  4.87 | ppl   130.06\n",
            "| epoch   9 |   600/ 2983 batches | lr 0.001 | ms/batch 18.98 | loss  4.78 | ppl   118.86\n",
            "| epoch   9 |   800/ 2983 batches | lr 0.001 | ms/batch 19.31 | loss  4.84 | ppl   126.20\n",
            "| epoch   9 |  1000/ 2983 batches | lr 0.001 | ms/batch 19.12 | loss  4.84 | ppl   125.85\n",
            "| epoch   9 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.50 | loss  4.85 | ppl   128.31\n",
            "| epoch   9 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.31 | loss  4.88 | ppl   131.83\n",
            "| epoch   9 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.42 | loss  4.89 | ppl   132.82\n",
            "| epoch   9 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.46 | loss  4.85 | ppl   127.90\n",
            "| epoch   9 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.23 | loss  4.89 | ppl   133.01\n",
            "| epoch   9 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.17 | loss  4.80 | ppl   121.12\n",
            "| epoch   9 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.07 | loss  4.83 | ppl   124.93\n",
            "| epoch   9 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.04 | loss  4.86 | ppl   129.39\n",
            "| epoch   9 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.25 | loss  4.82 | ppl   123.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 59.43s | valid loss  5.99 | valid ppl   399.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/ 2983 batches | lr 0.001 | ms/batch 19.21 | loss  4.85 | ppl   127.23\n",
            "| epoch  10 |   400/ 2983 batches | lr 0.001 | ms/batch 19.29 | loss  4.84 | ppl   125.95\n",
            "| epoch  10 |   600/ 2983 batches | lr 0.001 | ms/batch 19.06 | loss  4.75 | ppl   115.35\n",
            "| epoch  10 |   800/ 2983 batches | lr 0.001 | ms/batch 19.26 | loss  4.81 | ppl   122.43\n",
            "| epoch  10 |  1000/ 2983 batches | lr 0.001 | ms/batch 19.12 | loss  4.81 | ppl   122.19\n",
            "| epoch  10 |  1200/ 2983 batches | lr 0.001 | ms/batch 18.99 | loss  4.82 | ppl   124.51\n",
            "| epoch  10 |  1400/ 2983 batches | lr 0.001 | ms/batch 18.92 | loss  4.85 | ppl   127.84\n",
            "| epoch  10 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.06 | loss  4.86 | ppl   128.92\n",
            "| epoch  10 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.04 | loss  4.82 | ppl   124.31\n",
            "| epoch  10 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.13 | loss  4.86 | ppl   129.04\n",
            "| epoch  10 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.11 | loss  4.77 | ppl   117.71\n",
            "| epoch  10 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.20 | loss  4.80 | ppl   121.46\n",
            "| epoch  10 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.22 | loss  4.83 | ppl   125.78\n",
            "| epoch  10 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.18 | loss  4.79 | ppl   120.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 59.10s | valid loss  6.01 | valid ppl   407.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |   200/ 2983 batches | lr 0.001 | ms/batch 19.33 | loss  4.82 | ppl   124.07\n",
            "| epoch  11 |   400/ 2983 batches | lr 0.001 | ms/batch 19.21 | loss  4.81 | ppl   122.61\n",
            "| epoch  11 |   600/ 2983 batches | lr 0.001 | ms/batch 19.21 | loss  4.72 | ppl   112.52\n",
            "| epoch  11 |   800/ 2983 batches | lr 0.001 | ms/batch 19.18 | loss  4.78 | ppl   119.36\n",
            "| epoch  11 |  1000/ 2983 batches | lr 0.001 | ms/batch 19.05 | loss  4.78 | ppl   119.21\n",
            "| epoch  11 |  1200/ 2983 batches | lr 0.001 | ms/batch 18.99 | loss  4.80 | ppl   121.40\n",
            "| epoch  11 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.08 | loss  4.82 | ppl   124.56\n",
            "| epoch  11 |  1600/ 2983 batches | lr 0.001 | ms/batch 18.99 | loss  4.83 | ppl   125.73\n",
            "| epoch  11 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.01 | loss  4.80 | ppl   121.40\n",
            "| epoch  11 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.08 | loss  4.84 | ppl   125.84\n",
            "| epoch  11 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.19 | loss  4.74 | ppl   114.88\n",
            "| epoch  11 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.07 | loss  4.78 | ppl   118.62\n",
            "| epoch  11 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.07 | loss  4.81 | ppl   122.81\n",
            "| epoch  11 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.09 | loss  4.77 | ppl   117.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 59.05s | valid loss  6.03 | valid ppl   415.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |   200/ 2983 batches | lr 0.001 | ms/batch 19.06 | loss  4.80 | ppl   121.46\n",
            "| epoch  12 |   400/ 2983 batches | lr 0.001 | ms/batch 19.07 | loss  4.79 | ppl   119.88\n",
            "| epoch  12 |   600/ 2983 batches | lr 0.001 | ms/batch 18.94 | loss  4.70 | ppl   110.14\n",
            "| epoch  12 |   800/ 2983 batches | lr 0.001 | ms/batch 18.94 | loss  4.76 | ppl   116.82\n",
            "| epoch  12 |  1000/ 2983 batches | lr 0.001 | ms/batch 18.93 | loss  4.76 | ppl   116.73\n",
            "| epoch  12 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.22 | loss  4.78 | ppl   118.80\n",
            "| epoch  12 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.15 | loss  4.80 | ppl   121.81\n",
            "| epoch  12 |  1600/ 2983 batches | lr 0.001 | ms/batch 18.94 | loss  4.81 | ppl   123.17\n",
            "| epoch  12 |  1800/ 2983 batches | lr 0.001 | ms/batch 18.95 | loss  4.78 | ppl   118.98\n",
            "| epoch  12 |  2000/ 2983 batches | lr 0.001 | ms/batch 18.92 | loss  4.81 | ppl   123.16\n",
            "| epoch  12 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.05 | loss  4.72 | ppl   112.52\n",
            "| epoch  12 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.04 | loss  4.76 | ppl   116.24\n",
            "| epoch  12 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.03 | loss  4.79 | ppl   120.32\n",
            "| epoch  12 |  2800/ 2983 batches | lr 0.001 | ms/batch 18.97 | loss  4.75 | ppl   115.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 58.79s | valid loss  6.05 | valid ppl   423.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |   200/ 2983 batches | lr 0.001 | ms/batch 19.47 | loss  4.78 | ppl   119.26\n",
            "| epoch  13 |   400/ 2983 batches | lr 0.001 | ms/batch 19.00 | loss  4.77 | ppl   117.59\n",
            "| epoch  13 |   600/ 2983 batches | lr 0.001 | ms/batch 19.08 | loss  4.68 | ppl   108.12\n",
            "| epoch  13 |   800/ 2983 batches | lr 0.001 | ms/batch 19.07 | loss  4.74 | ppl   114.70\n",
            "| epoch  13 |  1000/ 2983 batches | lr 0.001 | ms/batch 19.11 | loss  4.74 | ppl   114.63\n",
            "| epoch  13 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.29 | loss  4.76 | ppl   116.63\n",
            "| epoch  13 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.06 | loss  4.78 | ppl   119.49\n",
            "| epoch  13 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.02 | loss  4.80 | ppl   120.97\n",
            "| epoch  13 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.06 | loss  4.76 | ppl   116.93\n",
            "| epoch  13 |  2000/ 2983 batches | lr 0.001 | ms/batch 18.99 | loss  4.80 | ppl   120.96\n",
            "| epoch  13 |  2200/ 2983 batches | lr 0.001 | ms/batch 18.98 | loss  4.71 | ppl   110.50\n",
            "| epoch  13 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.03 | loss  4.74 | ppl   114.18\n",
            "| epoch  13 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.00 | loss  4.77 | ppl   118.19\n",
            "| epoch  13 |  2800/ 2983 batches | lr 0.001 | ms/batch 18.97 | loss  4.73 | ppl   113.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 58.99s | valid loss  6.07 | valid ppl   430.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |   200/ 2983 batches | lr 0.001 | ms/batch 19.20 | loss  4.77 | ppl   117.36\n",
            "| epoch  14 |   400/ 2983 batches | lr 0.001 | ms/batch 18.92 | loss  4.75 | ppl   115.68\n",
            "| epoch  14 |   600/ 2983 batches | lr 0.001 | ms/batch 19.03 | loss  4.67 | ppl   106.35\n",
            "| epoch  14 |   800/ 2983 batches | lr 0.001 | ms/batch 18.94 | loss  4.73 | ppl   112.90\n",
            "| epoch  14 |  1000/ 2983 batches | lr 0.001 | ms/batch 18.95 | loss  4.73 | ppl   112.82\n",
            "| epoch  14 |  1200/ 2983 batches | lr 0.001 | ms/batch 18.91 | loss  4.74 | ppl   114.78\n",
            "| epoch  14 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.04 | loss  4.77 | ppl   117.50\n",
            "| epoch  14 |  1600/ 2983 batches | lr 0.001 | ms/batch 18.97 | loss  4.78 | ppl   119.14\n",
            "| epoch  14 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.01 | loss  4.75 | ppl   115.16\n",
            "| epoch  14 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.15 | loss  4.78 | ppl   119.05\n",
            "| epoch  14 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.16 | loss  4.69 | ppl   108.81\n",
            "| epoch  14 |  2400/ 2983 batches | lr 0.001 | ms/batch 18.94 | loss  4.72 | ppl   112.38\n",
            "| epoch  14 |  2600/ 2983 batches | lr 0.001 | ms/batch 18.91 | loss  4.76 | ppl   116.35\n",
            "| epoch  14 |  2800/ 2983 batches | lr 0.001 | ms/batch 18.93 | loss  4.72 | ppl   111.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 58.79s | valid loss  6.08 | valid ppl   437.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |   200/ 2983 batches | lr 0.001 | ms/batch 19.01 | loss  4.75 | ppl   115.72\n",
            "| epoch  15 |   400/ 2983 batches | lr 0.001 | ms/batch 19.20 | loss  4.74 | ppl   114.02\n",
            "| epoch  15 |   600/ 2983 batches | lr 0.001 | ms/batch 19.02 | loss  4.65 | ppl   104.83\n",
            "| epoch  15 |   800/ 2983 batches | lr 0.001 | ms/batch 18.96 | loss  4.71 | ppl   111.33\n",
            "| epoch  15 |  1000/ 2983 batches | lr 0.001 | ms/batch 18.93 | loss  4.71 | ppl   111.26\n",
            "| epoch  15 |  1200/ 2983 batches | lr 0.001 | ms/batch 18.99 | loss  4.73 | ppl   113.19\n",
            "| epoch  15 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.11 | loss  4.75 | ppl   115.75\n",
            "| epoch  15 |  1600/ 2983 batches | lr 0.001 | ms/batch 18.98 | loss  4.77 | ppl   117.49\n",
            "| epoch  15 |  1800/ 2983 batches | lr 0.001 | ms/batch 18.99 | loss  4.73 | ppl   113.62\n",
            "| epoch  15 |  2000/ 2983 batches | lr 0.001 | ms/batch 18.97 | loss  4.77 | ppl   117.45\n",
            "| epoch  15 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.00 | loss  4.68 | ppl   107.32\n",
            "| epoch  15 |  2400/ 2983 batches | lr 0.001 | ms/batch 18.96 | loss  4.71 | ppl   110.79\n",
            "| epoch  15 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.05 | loss  4.74 | ppl   114.72\n",
            "| epoch  15 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.19 | loss  4.70 | ppl   110.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 58.82s | valid loss  6.10 | valid ppl   444.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |   200/ 2983 batches | lr 0.001 | ms/batch 19.07 | loss  4.74 | ppl   114.25\n",
            "| epoch  16 |   400/ 2983 batches | lr 0.001 | ms/batch 19.13 | loss  4.72 | ppl   112.62\n",
            "| epoch  16 |   600/ 2983 batches | lr 0.001 | ms/batch 18.96 | loss  4.64 | ppl   103.47\n",
            "| epoch  16 |   800/ 2983 batches | lr 0.001 | ms/batch 18.93 | loss  4.70 | ppl   109.93\n",
            "| epoch  16 |  1000/ 2983 batches | lr 0.001 | ms/batch 18.94 | loss  4.70 | ppl   109.89\n",
            "| epoch  16 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.02 | loss  4.72 | ppl   111.80\n",
            "| epoch  16 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.18 | loss  4.74 | ppl   114.23\n",
            "| epoch  16 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.03 | loss  4.75 | ppl   116.09\n",
            "| epoch  16 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.09 | loss  4.72 | ppl   112.24\n",
            "| epoch  16 |  2000/ 2983 batches | lr 0.001 | ms/batch 18.97 | loss  4.75 | ppl   116.00\n",
            "| epoch  16 |  2200/ 2983 batches | lr 0.001 | ms/batch 18.92 | loss  4.66 | ppl   106.08\n",
            "| epoch  16 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.01 | loss  4.69 | ppl   109.40\n",
            "| epoch  16 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.00 | loss  4.73 | ppl   113.29\n",
            "| epoch  16 |  2800/ 2983 batches | lr 0.001 | ms/batch 18.94 | loss  4.69 | ppl   108.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 58.81s | valid loss  6.11 | valid ppl   451.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |   200/ 2983 batches | lr 0.001 | ms/batch 19.21 | loss  4.73 | ppl   112.96\n",
            "| epoch  17 |   400/ 2983 batches | lr 0.001 | ms/batch 19.04 | loss  4.71 | ppl   111.38\n",
            "| epoch  17 |   600/ 2983 batches | lr 0.001 | ms/batch 18.91 | loss  4.63 | ppl   102.28\n",
            "| epoch  17 |   800/ 2983 batches | lr 0.001 | ms/batch 19.00 | loss  4.69 | ppl   108.68\n",
            "| epoch  17 |  1000/ 2983 batches | lr 0.001 | ms/batch 18.93 | loss  4.69 | ppl   108.70\n",
            "| epoch  17 |  1200/ 2983 batches | lr 0.001 | ms/batch 18.96 | loss  4.71 | ppl   110.59\n",
            "| epoch  17 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.15 | loss  4.73 | ppl   112.89\n",
            "| epoch  17 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.05 | loss  4.74 | ppl   114.77\n",
            "| epoch  17 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.06 | loss  4.71 | ppl   111.02\n",
            "| epoch  17 |  2000/ 2983 batches | lr 0.001 | ms/batch 18.95 | loss  4.74 | ppl   114.78\n",
            "| epoch  17 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.08 | loss  4.65 | ppl   104.97\n",
            "| epoch  17 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.13 | loss  4.68 | ppl   108.17\n",
            "| epoch  17 |  2600/ 2983 batches | lr 0.001 | ms/batch 18.97 | loss  4.72 | ppl   112.02\n",
            "| epoch  17 |  2800/ 2983 batches | lr 0.001 | ms/batch 18.99 | loss  4.68 | ppl   107.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 58.87s | valid loss  6.13 | valid ppl   458.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |   200/ 2983 batches | lr 0.001 | ms/batch 19.25 | loss  4.72 | ppl   111.80\n",
            "| epoch  18 |   400/ 2983 batches | lr 0.001 | ms/batch 18.94 | loss  4.70 | ppl   110.31\n",
            "| epoch  18 |   600/ 2983 batches | lr 0.001 | ms/batch 18.97 | loss  4.62 | ppl   101.22\n",
            "| epoch  18 |   800/ 2983 batches | lr 0.001 | ms/batch 19.04 | loss  4.68 | ppl   107.58\n",
            "| epoch  18 |  1000/ 2983 batches | lr 0.001 | ms/batch 19.04 | loss  4.68 | ppl   107.60\n",
            "| epoch  18 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.13 | loss  4.70 | ppl   109.54\n",
            "| epoch  18 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.00 | loss  4.72 | ppl   111.72\n",
            "| epoch  18 |  1600/ 2983 batches | lr 0.001 | ms/batch 18.96 | loss  4.73 | ppl   113.62\n",
            "| epoch  18 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.13 | loss  4.70 | ppl   109.93\n",
            "| epoch  18 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.05 | loss  4.73 | ppl   113.61\n",
            "| epoch  18 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.09 | loss  4.64 | ppl   104.04\n",
            "| epoch  18 |  2400/ 2983 batches | lr 0.001 | ms/batch 18.94 | loss  4.67 | ppl   107.08\n",
            "| epoch  18 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.18 | loss  4.71 | ppl   110.91\n",
            "| epoch  18 |  2800/ 2983 batches | lr 0.001 | ms/batch 18.96 | loss  4.67 | ppl   106.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 58.85s | valid loss  6.14 | valid ppl   464.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |   200/ 2983 batches | lr 0.001 | ms/batch 19.05 | loss  4.71 | ppl   110.79\n",
            "| epoch  19 |   400/ 2983 batches | lr 0.001 | ms/batch 19.05 | loss  4.69 | ppl   109.32\n",
            "| epoch  19 |   600/ 2983 batches | lr 0.001 | ms/batch 19.00 | loss  4.61 | ppl   100.29\n",
            "| epoch  19 |   800/ 2983 batches | lr 0.001 | ms/batch 19.40 | loss  4.67 | ppl   106.59\n",
            "| epoch  19 |  1000/ 2983 batches | lr 0.001 | ms/batch 18.94 | loss  4.67 | ppl   106.63\n",
            "| epoch  19 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.00 | loss  4.69 | ppl   108.65\n",
            "| epoch  19 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.06 | loss  4.71 | ppl   110.68\n",
            "| epoch  19 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.13 | loss  4.72 | ppl   112.51\n",
            "| epoch  19 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.28 | loss  4.69 | ppl   108.95\n",
            "| epoch  19 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.22 | loss  4.72 | ppl   112.63\n",
            "| epoch  19 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.59 | loss  4.64 | ppl   103.18\n",
            "| epoch  19 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.49 | loss  4.66 | ppl   106.13\n",
            "| epoch  19 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.26 | loss  4.70 | ppl   109.92\n",
            "| epoch  19 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.00 | loss  4.66 | ppl   105.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 59.28s | valid loss  6.15 | valid ppl   470.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |   200/ 2983 batches | lr 0.001 | ms/batch 19.11 | loss  4.70 | ppl   109.89\n",
            "| epoch  20 |   400/ 2983 batches | lr 0.001 | ms/batch 18.99 | loss  4.69 | ppl   108.48\n",
            "| epoch  20 |   600/ 2983 batches | lr 0.001 | ms/batch 19.04 | loss  4.60 | ppl    99.47\n",
            "| epoch  20 |   800/ 2983 batches | lr 0.001 | ms/batch 19.30 | loss  4.66 | ppl   105.72\n",
            "| epoch  20 |  1000/ 2983 batches | lr 0.001 | ms/batch 19.03 | loss  4.66 | ppl   105.74\n",
            "| epoch  20 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.06 | loss  4.68 | ppl   107.88\n",
            "| epoch  20 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.06 | loss  4.70 | ppl   109.78\n",
            "| epoch  20 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.19 | loss  4.71 | ppl   111.57\n",
            "| epoch  20 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.05 | loss  4.68 | ppl   108.07\n",
            "| epoch  20 |  2000/ 2983 batches | lr 0.001 | ms/batch 18.94 | loss  4.72 | ppl   111.68\n",
            "| epoch  20 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.03 | loss  4.63 | ppl   102.45\n",
            "| epoch  20 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.11 | loss  4.66 | ppl   105.31\n",
            "| epoch  20 |  2600/ 2983 batches | lr 0.001 | ms/batch 18.95 | loss  4.69 | ppl   109.08\n",
            "| epoch  20 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.02 | loss  4.65 | ppl   104.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 58.92s | valid loss  6.17 | valid ppl   476.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |   200/ 2983 batches | lr 0.001 | ms/batch 19.22 | loss  4.69 | ppl   109.12\n",
            "| epoch  21 |   400/ 2983 batches | lr 0.001 | ms/batch 18.97 | loss  4.68 | ppl   107.70\n",
            "| epoch  21 |   600/ 2983 batches | lr 0.001 | ms/batch 19.06 | loss  4.59 | ppl    98.76\n",
            "| epoch  21 |   800/ 2983 batches | lr 0.001 | ms/batch 19.03 | loss  4.65 | ppl   104.93\n",
            "| epoch  21 |  1000/ 2983 batches | lr 0.001 | ms/batch 18.96 | loss  4.65 | ppl   104.97\n",
            "| epoch  21 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.00 | loss  4.67 | ppl   107.21\n",
            "| epoch  21 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.10 | loss  4.69 | ppl   108.97\n",
            "| epoch  21 |  1600/ 2983 batches | lr 0.001 | ms/batch 18.97 | loss  4.71 | ppl   110.70\n",
            "| epoch  21 |  1800/ 2983 batches | lr 0.001 | ms/batch 18.97 | loss  4.68 | ppl   107.29\n",
            "| epoch  21 |  2000/ 2983 batches | lr 0.001 | ms/batch 18.98 | loss  4.71 | ppl   110.92\n",
            "| epoch  21 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.19 | loss  4.62 | ppl   101.75\n",
            "| epoch  21 |  2400/ 2983 batches | lr 0.001 | ms/batch 18.95 | loss  4.65 | ppl   104.60\n",
            "| epoch  21 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.45 | loss  4.69 | ppl   108.31\n",
            "| epoch  21 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.25 | loss  4.64 | ppl   103.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 59.08s | valid loss  6.18 | valid ppl   481.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |   200/ 2983 batches | lr 0.001 | ms/batch 19.26 | loss  4.69 | ppl   108.42\n",
            "| epoch  22 |   400/ 2983 batches | lr 0.001 | ms/batch 19.05 | loss  4.67 | ppl   107.05\n",
            "| epoch  22 |   600/ 2983 batches | lr 0.001 | ms/batch 19.18 | loss  4.59 | ppl    98.14\n",
            "| epoch  22 |   800/ 2983 batches | lr 0.001 | ms/batch 19.31 | loss  4.65 | ppl   104.24\n",
            "| epoch  22 |  1000/ 2983 batches | lr 0.001 | ms/batch 19.13 | loss  4.65 | ppl   104.26\n",
            "| epoch  22 |  1200/ 2983 batches | lr 0.001 | ms/batch 18.95 | loss  4.67 | ppl   106.61\n",
            "| epoch  22 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.09 | loss  4.68 | ppl   108.27\n",
            "| epoch  22 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.42 | loss  4.70 | ppl   109.99\n",
            "| epoch  22 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.14 | loss  4.67 | ppl   106.60\n",
            "| epoch  22 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.18 | loss  4.70 | ppl   110.19\n",
            "| epoch  22 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.23 | loss  4.62 | ppl   101.14\n",
            "| epoch  22 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.13 | loss  4.64 | ppl   103.99\n",
            "| epoch  22 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.39 | loss  4.68 | ppl   107.66\n",
            "| epoch  22 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.30 | loss  4.64 | ppl   103.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 59.34s | valid loss  6.19 | valid ppl   486.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |   200/ 2983 batches | lr 0.001 | ms/batch 19.34 | loss  4.68 | ppl   107.84\n",
            "| epoch  23 |   400/ 2983 batches | lr 0.001 | ms/batch 19.29 | loss  4.67 | ppl   106.46\n",
            "| epoch  23 |   600/ 2983 batches | lr 0.001 | ms/batch 19.09 | loss  4.58 | ppl    97.59\n",
            "| epoch  23 |   800/ 2983 batches | lr 0.001 | ms/batch 19.10 | loss  4.64 | ppl   103.63\n",
            "| epoch  23 |  1000/ 2983 batches | lr 0.001 | ms/batch 19.08 | loss  4.64 | ppl   103.65\n",
            "| epoch  23 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.12 | loss  4.66 | ppl   106.08\n",
            "| epoch  23 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.17 | loss  4.68 | ppl   107.67\n",
            "| epoch  23 |  1600/ 2983 batches | lr 0.001 | ms/batch 18.99 | loss  4.69 | ppl   109.33\n",
            "| epoch  23 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.26 | loss  4.66 | ppl   106.01\n",
            "| epoch  23 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.46 | loss  4.70 | ppl   109.61\n",
            "| epoch  23 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.14 | loss  4.61 | ppl   100.58\n",
            "| epoch  23 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.45 | loss  4.64 | ppl   103.46\n",
            "| epoch  23 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.15 | loss  4.67 | ppl   107.06\n",
            "| epoch  23 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.26 | loss  4.63 | ppl   102.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 59.33s | valid loss  6.20 | valid ppl   491.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |   200/ 2983 batches | lr 0.001 | ms/batch 19.37 | loss  4.68 | ppl   107.31\n",
            "| epoch  24 |   400/ 2983 batches | lr 0.001 | ms/batch 19.20 | loss  4.66 | ppl   105.98\n",
            "| epoch  24 |   600/ 2983 batches | lr 0.001 | ms/batch 19.33 | loss  4.58 | ppl    97.11\n",
            "| epoch  24 |   800/ 2983 batches | lr 0.001 | ms/batch 19.03 | loss  4.64 | ppl   103.13\n",
            "| epoch  24 |  1000/ 2983 batches | lr 0.001 | ms/batch 19.24 | loss  4.64 | ppl   103.09\n",
            "| epoch  24 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.36 | loss  4.66 | ppl   105.59\n",
            "| epoch  24 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.19 | loss  4.67 | ppl   107.15\n",
            "| epoch  24 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.10 | loss  4.69 | ppl   108.81\n",
            "| epoch  24 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.42 | loss  4.66 | ppl   105.51\n",
            "| epoch  24 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.26 | loss  4.69 | ppl   109.04\n",
            "| epoch  24 |  2200/ 2983 batches | lr 0.001 | ms/batch 18.96 | loss  4.61 | ppl   100.12\n",
            "| epoch  24 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.22 | loss  4.63 | ppl   103.02\n",
            "| epoch  24 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.23 | loss  4.67 | ppl   106.54\n",
            "| epoch  24 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.31 | loss  4.63 | ppl   102.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 59.41s | valid loss  6.21 | valid ppl   496.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |   200/ 2983 batches | lr 0.001 | ms/batch 19.68 | loss  4.67 | ppl   106.87\n",
            "| epoch  25 |   400/ 2983 batches | lr 0.001 | ms/batch 18.95 | loss  4.66 | ppl   105.55\n",
            "| epoch  25 |   600/ 2983 batches | lr 0.001 | ms/batch 19.12 | loss  4.57 | ppl    96.69\n",
            "| epoch  25 |   800/ 2983 batches | lr 0.001 | ms/batch 19.13 | loss  4.63 | ppl   102.69\n",
            "| epoch  25 |  1000/ 2983 batches | lr 0.001 | ms/batch 19.32 | loss  4.63 | ppl   102.62\n",
            "| epoch  25 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.06 | loss  4.66 | ppl   105.17\n",
            "| epoch  25 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.13 | loss  4.67 | ppl   106.70\n",
            "| epoch  25 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.25 | loss  4.68 | ppl   108.30\n",
            "| epoch  25 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.32 | loss  4.65 | ppl   105.08\n",
            "| epoch  25 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.26 | loss  4.69 | ppl   108.61\n",
            "| epoch  25 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.19 | loss  4.60 | ppl    99.69\n",
            "| epoch  25 |  2400/ 2983 batches | lr 0.001 | ms/batch 18.95 | loss  4.63 | ppl   102.63\n",
            "| epoch  25 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.41 | loss  4.66 | ppl   106.06\n",
            "| epoch  25 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.29 | loss  4.62 | ppl   101.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 59.44s | valid loss  6.22 | valid ppl   501.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |   200/ 2983 batches | lr 0.001 | ms/batch 19.21 | loss  4.67 | ppl   106.47\n",
            "| epoch  26 |   400/ 2983 batches | lr 0.001 | ms/batch 19.08 | loss  4.66 | ppl   105.22\n",
            "| epoch  26 |   600/ 2983 batches | lr 0.001 | ms/batch 19.31 | loss  4.57 | ppl    96.32\n",
            "| epoch  26 |   800/ 2983 batches | lr 0.001 | ms/batch 19.20 | loss  4.63 | ppl   102.32\n",
            "| epoch  26 |  1000/ 2983 batches | lr 0.001 | ms/batch 19.31 | loss  4.63 | ppl   102.20\n",
            "| epoch  26 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.30 | loss  4.65 | ppl   104.77\n",
            "| epoch  26 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.30 | loss  4.67 | ppl   106.32\n",
            "| epoch  26 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.19 | loss  4.68 | ppl   107.92\n",
            "| epoch  26 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.04 | loss  4.65 | ppl   104.72\n",
            "| epoch  26 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.04 | loss  4.68 | ppl   108.18\n",
            "| epoch  26 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.20 | loss  4.60 | ppl    99.34\n",
            "| epoch  26 |  2400/ 2983 batches | lr 0.001 | ms/batch 18.99 | loss  4.63 | ppl   102.31\n",
            "| epoch  26 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.16 | loss  4.66 | ppl   105.64\n",
            "| epoch  26 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.14 | loss  4.62 | ppl   101.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 59.30s | valid loss  6.23 | valid ppl   505.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |   200/ 2983 batches | lr 0.001 | ms/batch 19.40 | loss  4.66 | ppl   106.16\n",
            "| epoch  27 |   400/ 2983 batches | lr 0.001 | ms/batch 19.24 | loss  4.65 | ppl   104.90\n",
            "| epoch  27 |   600/ 2983 batches | lr 0.001 | ms/batch 19.16 | loss  4.56 | ppl    96.00\n",
            "| epoch  27 |   800/ 2983 batches | lr 0.001 | ms/batch 19.20 | loss  4.62 | ppl   102.00\n",
            "| epoch  27 |  1000/ 2983 batches | lr 0.001 | ms/batch 18.94 | loss  4.62 | ppl   101.86\n",
            "| epoch  27 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.29 | loss  4.65 | ppl   104.44\n",
            "| epoch  27 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.33 | loss  4.66 | ppl   105.98\n",
            "| epoch  27 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.31 | loss  4.68 | ppl   107.53\n",
            "| epoch  27 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.17 | loss  4.65 | ppl   104.40\n",
            "| epoch  27 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.25 | loss  4.68 | ppl   107.87\n",
            "| epoch  27 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.22 | loss  4.60 | ppl    99.02\n",
            "| epoch  27 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.40 | loss  4.63 | ppl   102.00\n",
            "| epoch  27 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.12 | loss  4.66 | ppl   105.25\n",
            "| epoch  27 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.42 | loss  4.62 | ppl   101.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 59.49s | valid loss  6.23 | valid ppl   510.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |   200/ 2983 batches | lr 0.001 | ms/batch 19.46 | loss  4.66 | ppl   105.87\n",
            "| epoch  28 |   400/ 2983 batches | lr 0.001 | ms/batch 19.20 | loss  4.65 | ppl   104.65\n",
            "| epoch  28 |   600/ 2983 batches | lr 0.001 | ms/batch 19.17 | loss  4.56 | ppl    95.73\n",
            "| epoch  28 |   800/ 2983 batches | lr 0.001 | ms/batch 19.09 | loss  4.62 | ppl   101.71\n",
            "| epoch  28 |  1000/ 2983 batches | lr 0.001 | ms/batch 19.05 | loss  4.62 | ppl   101.55\n",
            "| epoch  28 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.06 | loss  4.65 | ppl   104.12\n",
            "| epoch  28 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.20 | loss  4.66 | ppl   105.69\n",
            "| epoch  28 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.28 | loss  4.68 | ppl   107.24\n",
            "| epoch  28 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.05 | loss  4.65 | ppl   104.13\n",
            "| epoch  28 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.59 | loss  4.68 | ppl   107.55\n",
            "| epoch  28 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.06 | loss  4.59 | ppl    98.75\n",
            "| epoch  28 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.30 | loss  4.62 | ppl   101.75\n",
            "| epoch  28 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.34 | loss  4.65 | ppl   104.92\n",
            "| epoch  28 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.31 | loss  4.61 | ppl   100.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 59.39s | valid loss  6.24 | valid ppl   514.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |   200/ 2983 batches | lr 0.001 | ms/batch 19.15 | loss  4.66 | ppl   105.61\n",
            "| epoch  29 |   400/ 2983 batches | lr 0.001 | ms/batch 19.12 | loss  4.65 | ppl   104.40\n",
            "| epoch  29 |   600/ 2983 batches | lr 0.001 | ms/batch 19.27 | loss  4.56 | ppl    95.49\n",
            "| epoch  29 |   800/ 2983 batches | lr 0.001 | ms/batch 19.18 | loss  4.62 | ppl   101.44\n",
            "| epoch  29 |  1000/ 2983 batches | lr 0.001 | ms/batch 19.24 | loss  4.62 | ppl   101.29\n",
            "| epoch  29 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.23 | loss  4.64 | ppl   103.85\n",
            "| epoch  29 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.17 | loss  4.66 | ppl   105.42\n",
            "| epoch  29 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.14 | loss  4.67 | ppl   106.92\n",
            "| epoch  29 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.29 | loss  4.64 | ppl   103.87\n",
            "| epoch  29 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.11 | loss  4.68 | ppl   107.31\n",
            "| epoch  29 |  2200/ 2983 batches | lr 0.001 | ms/batch 18.94 | loss  4.59 | ppl    98.48\n",
            "| epoch  29 |  2400/ 2983 batches | lr 0.001 | ms/batch 18.97 | loss  4.62 | ppl   101.50\n",
            "| epoch  29 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.04 | loss  4.65 | ppl   104.61\n",
            "| epoch  29 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.18 | loss  4.61 | ppl   100.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 59.22s | valid loss  6.25 | valid ppl   517.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |   200/ 2983 batches | lr 0.001 | ms/batch 19.21 | loss  4.66 | ppl   105.36\n",
            "| epoch  30 |   400/ 2983 batches | lr 0.001 | ms/batch 19.21 | loss  4.65 | ppl   104.19\n",
            "| epoch  30 |   600/ 2983 batches | lr 0.001 | ms/batch 19.52 | loss  4.56 | ppl    95.28\n",
            "| epoch  30 |   800/ 2983 batches | lr 0.001 | ms/batch 19.18 | loss  4.62 | ppl   101.19\n",
            "| epoch  30 |  1000/ 2983 batches | lr 0.001 | ms/batch 19.10 | loss  4.62 | ppl   101.05\n",
            "| epoch  30 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.05 | loss  4.64 | ppl   103.59\n",
            "| epoch  30 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.42 | loss  4.66 | ppl   105.17\n",
            "| epoch  30 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.14 | loss  4.67 | ppl   106.67\n",
            "| epoch  30 |  1800/ 2983 batches | lr 0.001 | ms/batch 18.99 | loss  4.64 | ppl   103.62\n",
            "| epoch  30 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.12 | loss  4.67 | ppl   107.03\n",
            "| epoch  30 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.20 | loss  4.59 | ppl    98.26\n",
            "| epoch  30 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.13 | loss  4.62 | ppl   101.28\n",
            "| epoch  30 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.12 | loss  4.65 | ppl   104.33\n",
            "| epoch  30 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.29 | loss  4.61 | ppl   100.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 59.30s | valid loss  6.26 | valid ppl   521.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  31 |   200/ 2983 batches | lr 0.001 | ms/batch 19.10 | loss  4.66 | ppl   105.12\n",
            "| epoch  31 |   400/ 2983 batches | lr 0.001 | ms/batch 19.42 | loss  4.64 | ppl   103.95\n",
            "| epoch  31 |   600/ 2983 batches | lr 0.001 | ms/batch 19.10 | loss  4.55 | ppl    95.08\n",
            "| epoch  31 |   800/ 2983 batches | lr 0.001 | ms/batch 18.95 | loss  4.61 | ppl   100.93\n",
            "| epoch  31 |  1000/ 2983 batches | lr 0.001 | ms/batch 19.33 | loss  4.61 | ppl   100.82\n",
            "| epoch  31 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.04 | loss  4.64 | ppl   103.36\n",
            "| epoch  31 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.08 | loss  4.65 | ppl   104.93\n",
            "| epoch  31 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.10 | loss  4.67 | ppl   106.38\n",
            "| epoch  31 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.32 | loss  4.64 | ppl   103.36\n",
            "| epoch  31 |  2000/ 2983 batches | lr 0.001 | ms/batch 18.98 | loss  4.67 | ppl   106.80\n",
            "| epoch  31 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.34 | loss  4.59 | ppl    98.01\n",
            "| epoch  31 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.17 | loss  4.62 | ppl   101.06\n",
            "| epoch  31 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.27 | loss  4.64 | ppl   104.06\n",
            "| epoch  31 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.23 | loss  4.61 | ppl   100.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 59.29s | valid loss  6.26 | valid ppl   524.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  32 |   200/ 2983 batches | lr 0.001 | ms/batch 19.58 | loss  4.65 | ppl   104.86\n",
            "| epoch  32 |   400/ 2983 batches | lr 0.001 | ms/batch 19.08 | loss  4.64 | ppl   103.74\n",
            "| epoch  32 |   600/ 2983 batches | lr 0.001 | ms/batch 19.35 | loss  4.55 | ppl    94.89\n",
            "| epoch  32 |   800/ 2983 batches | lr 0.001 | ms/batch 19.14 | loss  4.61 | ppl   100.68\n",
            "| epoch  32 |  1000/ 2983 batches | lr 0.001 | ms/batch 19.39 | loss  4.61 | ppl   100.59\n",
            "| epoch  32 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.30 | loss  4.64 | ppl   103.11\n",
            "| epoch  32 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.21 | loss  4.65 | ppl   104.71\n",
            "| epoch  32 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.20 | loss  4.66 | ppl   106.13\n",
            "| epoch  32 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.26 | loss  4.64 | ppl   103.11\n",
            "| epoch  32 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.10 | loss  4.67 | ppl   106.52\n",
            "| epoch  32 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.05 | loss  4.58 | ppl    97.78\n",
            "| epoch  32 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.37 | loss  4.61 | ppl   100.84\n",
            "| epoch  32 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.12 | loss  4.64 | ppl   103.79\n",
            "| epoch  32 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.53 | loss  4.61 | ppl   100.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 59.61s | valid loss  6.27 | valid ppl   527.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  33 |   200/ 2983 batches | lr 0.001 | ms/batch 19.41 | loss  4.65 | ppl   104.61\n",
            "| epoch  33 |   400/ 2983 batches | lr 0.001 | ms/batch 19.18 | loss  4.64 | ppl   103.50\n",
            "| epoch  33 |   600/ 2983 batches | lr 0.001 | ms/batch 19.22 | loss  4.55 | ppl    94.70\n",
            "| epoch  33 |   800/ 2983 batches | lr 0.001 | ms/batch 19.29 | loss  4.61 | ppl   100.42\n",
            "| epoch  33 |  1000/ 2983 batches | lr 0.001 | ms/batch 19.00 | loss  4.61 | ppl   100.37\n",
            "| epoch  33 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.42 | loss  4.63 | ppl   102.86\n",
            "| epoch  33 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.35 | loss  4.65 | ppl   104.48\n",
            "| epoch  33 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.27 | loss  4.66 | ppl   105.87\n",
            "| epoch  33 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.40 | loss  4.63 | ppl   102.85\n",
            "| epoch  33 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.19 | loss  4.67 | ppl   106.30\n",
            "| epoch  33 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.00 | loss  4.58 | ppl    97.52\n",
            "| epoch  33 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.08 | loss  4.61 | ppl   100.60\n",
            "| epoch  33 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.02 | loss  4.64 | ppl   103.56\n",
            "| epoch  33 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.50 | loss  4.60 | ppl    99.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 59.45s | valid loss  6.27 | valid ppl   530.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  34 |   200/ 2983 batches | lr 0.001 | ms/batch 19.44 | loss  4.65 | ppl   104.38\n",
            "| epoch  34 |   400/ 2983 batches | lr 0.001 | ms/batch 19.53 | loss  4.64 | ppl   103.31\n",
            "| epoch  34 |   600/ 2983 batches | lr 0.001 | ms/batch 19.31 | loss  4.55 | ppl    94.51\n",
            "| epoch  34 |   800/ 2983 batches | lr 0.001 | ms/batch 19.21 | loss  4.61 | ppl   100.20\n",
            "| epoch  34 |  1000/ 2983 batches | lr 0.001 | ms/batch 19.23 | loss  4.61 | ppl   100.14\n",
            "| epoch  34 |  1200/ 2983 batches | lr 0.001 | ms/batch 18.98 | loss  4.63 | ppl   102.60\n",
            "| epoch  34 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.18 | loss  4.65 | ppl   104.26\n",
            "| epoch  34 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.32 | loss  4.66 | ppl   105.66\n",
            "| epoch  34 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.02 | loss  4.63 | ppl   102.63\n",
            "| epoch  34 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.14 | loss  4.66 | ppl   106.02\n",
            "| epoch  34 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.23 | loss  4.58 | ppl    97.31\n",
            "| epoch  34 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.13 | loss  4.61 | ppl   100.37\n",
            "| epoch  34 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.02 | loss  4.64 | ppl   103.33\n",
            "| epoch  34 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.00 | loss  4.60 | ppl    99.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 59.36s | valid loss  6.28 | valid ppl   534.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  35 |   200/ 2983 batches | lr 0.001 | ms/batch 19.38 | loss  4.65 | ppl   104.17\n",
            "| epoch  35 |   400/ 2983 batches | lr 0.001 | ms/batch 19.14 | loss  4.64 | ppl   103.11\n",
            "| epoch  35 |   600/ 2983 batches | lr 0.001 | ms/batch 19.19 | loss  4.55 | ppl    94.31\n",
            "| epoch  35 |   800/ 2983 batches | lr 0.001 | ms/batch 19.15 | loss  4.61 | ppl   100.00\n",
            "| epoch  35 |  1000/ 2983 batches | lr 0.001 | ms/batch 19.05 | loss  4.60 | ppl    99.95\n",
            "| epoch  35 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.35 | loss  4.63 | ppl   102.37\n",
            "| epoch  35 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.04 | loss  4.64 | ppl   104.05\n",
            "| epoch  35 |  1600/ 2983 batches | lr 0.001 | ms/batch 18.99 | loss  4.66 | ppl   105.45\n",
            "| epoch  35 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.01 | loss  4.63 | ppl   102.43\n",
            "| epoch  35 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.31 | loss  4.66 | ppl   105.84\n",
            "| epoch  35 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.14 | loss  4.58 | ppl    97.09\n",
            "| epoch  35 |  2400/ 2983 batches | lr 0.001 | ms/batch 18.96 | loss  4.61 | ppl   100.14\n",
            "| epoch  35 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.14 | loss  4.64 | ppl   103.16\n",
            "| epoch  35 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.26 | loss  4.60 | ppl    99.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 59.21s | valid loss  6.29 | valid ppl   537.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  36 |   200/ 2983 batches | lr 0.001 | ms/batch 19.23 | loss  4.64 | ppl   103.98\n",
            "| epoch  36 |   400/ 2983 batches | lr 0.001 | ms/batch 18.95 | loss  4.63 | ppl   102.99\n",
            "| epoch  36 |   600/ 2983 batches | lr 0.001 | ms/batch 19.07 | loss  4.55 | ppl    94.17\n",
            "| epoch  36 |   800/ 2983 batches | lr 0.001 | ms/batch 18.95 | loss  4.60 | ppl    99.85\n",
            "| epoch  36 |  1000/ 2983 batches | lr 0.001 | ms/batch 19.23 | loss  4.60 | ppl    99.79\n",
            "| epoch  36 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.06 | loss  4.63 | ppl   102.16\n",
            "| epoch  36 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.14 | loss  4.64 | ppl   103.88\n",
            "| epoch  36 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.04 | loss  4.66 | ppl   105.32\n",
            "| epoch  36 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.11 | loss  4.63 | ppl   102.28\n",
            "| epoch  36 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.11 | loss  4.66 | ppl   105.63\n",
            "| epoch  36 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.14 | loss  4.57 | ppl    96.96\n",
            "| epoch  36 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.09 | loss  4.60 | ppl    99.96\n",
            "| epoch  36 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.00 | loss  4.63 | ppl   103.01\n",
            "| epoch  36 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.01 | loss  4.60 | ppl    99.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 58.95s | valid loss  6.29 | valid ppl   540.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  37 |   200/ 2983 batches | lr 0.001 | ms/batch 19.15 | loss  4.64 | ppl   103.85\n",
            "| epoch  37 |   400/ 2983 batches | lr 0.001 | ms/batch 18.94 | loss  4.63 | ppl   102.89\n",
            "| epoch  37 |   600/ 2983 batches | lr 0.001 | ms/batch 19.19 | loss  4.54 | ppl    94.05\n",
            "| epoch  37 |   800/ 2983 batches | lr 0.001 | ms/batch 18.96 | loss  4.60 | ppl    99.75\n",
            "| epoch  37 |  1000/ 2983 batches | lr 0.001 | ms/batch 18.98 | loss  4.60 | ppl    99.69\n",
            "| epoch  37 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.18 | loss  4.63 | ppl   102.02\n",
            "| epoch  37 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.22 | loss  4.64 | ppl   103.74\n",
            "| epoch  37 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.20 | loss  4.66 | ppl   105.21\n",
            "| epoch  37 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.02 | loss  4.63 | ppl   102.16\n",
            "| epoch  37 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.07 | loss  4.66 | ppl   105.56\n",
            "| epoch  37 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.08 | loss  4.57 | ppl    96.84\n",
            "| epoch  37 |  2400/ 2983 batches | lr 0.001 | ms/batch 18.90 | loss  4.60 | ppl    99.82\n",
            "| epoch  37 |  2600/ 2983 batches | lr 0.001 | ms/batch 18.94 | loss  4.63 | ppl   102.90\n",
            "| epoch  37 |  2800/ 2983 batches | lr 0.001 | ms/batch 18.99 | loss  4.60 | ppl    99.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 58.93s | valid loss  6.30 | valid ppl   542.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  38 |   200/ 2983 batches | lr 0.001 | ms/batch 19.04 | loss  4.64 | ppl   103.75\n",
            "| epoch  38 |   400/ 2983 batches | lr 0.001 | ms/batch 19.05 | loss  4.63 | ppl   102.85\n",
            "| epoch  38 |   600/ 2983 batches | lr 0.001 | ms/batch 19.11 | loss  4.54 | ppl    94.00\n",
            "| epoch  38 |   800/ 2983 batches | lr 0.001 | ms/batch 19.04 | loss  4.60 | ppl    99.69\n",
            "| epoch  38 |  1000/ 2983 batches | lr 0.001 | ms/batch 19.12 | loss  4.60 | ppl    99.61\n",
            "| epoch  38 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.16 | loss  4.62 | ppl   101.92\n",
            "| epoch  38 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.19 | loss  4.64 | ppl   103.64\n",
            "| epoch  38 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.26 | loss  4.66 | ppl   105.17\n",
            "| epoch  38 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.07 | loss  4.63 | ppl   102.08\n",
            "| epoch  38 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.23 | loss  4.66 | ppl   105.42\n",
            "| epoch  38 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.20 | loss  4.57 | ppl    96.80\n",
            "| epoch  38 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.15 | loss  4.60 | ppl    99.74\n",
            "| epoch  38 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.16 | loss  4.63 | ppl   102.81\n",
            "| epoch  38 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.25 | loss  4.60 | ppl    99.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 59.18s | valid loss  6.30 | valid ppl   545.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  39 |   200/ 2983 batches | lr 0.001 | ms/batch 19.20 | loss  4.64 | ppl   103.68\n",
            "| epoch  39 |   400/ 2983 batches | lr 0.001 | ms/batch 19.29 | loss  4.63 | ppl   102.79\n",
            "| epoch  39 |   600/ 2983 batches | lr 0.001 | ms/batch 18.93 | loss  4.54 | ppl    93.97\n",
            "| epoch  39 |   800/ 2983 batches | lr 0.001 | ms/batch 19.04 | loss  4.60 | ppl    99.66\n",
            "| epoch  39 |  1000/ 2983 batches | lr 0.001 | ms/batch 19.13 | loss  4.60 | ppl    99.57\n",
            "| epoch  39 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.04 | loss  4.62 | ppl   101.88\n",
            "| epoch  39 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.08 | loss  4.64 | ppl   103.56\n",
            "| epoch  39 |  1600/ 2983 batches | lr 0.001 | ms/batch 18.91 | loss  4.66 | ppl   105.12\n",
            "| epoch  39 |  1800/ 2983 batches | lr 0.001 | ms/batch 19.34 | loss  4.63 | ppl   102.04\n",
            "| epoch  39 |  2000/ 2983 batches | lr 0.001 | ms/batch 19.06 | loss  4.66 | ppl   105.45\n",
            "| epoch  39 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.03 | loss  4.57 | ppl    96.74\n",
            "| epoch  39 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.12 | loss  4.60 | ppl    99.69\n",
            "| epoch  39 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.23 | loss  4.63 | ppl   102.77\n",
            "| epoch  39 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.11 | loss  4.60 | ppl    99.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 59.12s | valid loss  6.31 | valid ppl   547.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  40 |   200/ 2983 batches | lr 0.001 | ms/batch 19.04 | loss  4.64 | ppl   103.62\n",
            "| epoch  40 |   400/ 2983 batches | lr 0.001 | ms/batch 18.95 | loss  4.63 | ppl   102.78\n",
            "| epoch  40 |   600/ 2983 batches | lr 0.001 | ms/batch 18.94 | loss  4.54 | ppl    93.97\n",
            "| epoch  40 |   800/ 2983 batches | lr 0.001 | ms/batch 18.93 | loss  4.60 | ppl    99.66\n",
            "| epoch  40 |  1000/ 2983 batches | lr 0.001 | ms/batch 19.20 | loss  4.60 | ppl    99.52\n",
            "| epoch  40 |  1200/ 2983 batches | lr 0.001 | ms/batch 19.31 | loss  4.62 | ppl   101.83\n",
            "| epoch  40 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.26 | loss  4.64 | ppl   103.52\n",
            "| epoch  40 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.01 | loss  4.66 | ppl   105.13\n",
            "| epoch  40 |  1800/ 2983 batches | lr 0.001 | ms/batch 18.96 | loss  4.63 | ppl   102.01\n",
            "| epoch  40 |  2000/ 2983 batches | lr 0.001 | ms/batch 18.96 | loss  4.66 | ppl   105.35\n",
            "| epoch  40 |  2200/ 2983 batches | lr 0.001 | ms/batch 19.16 | loss  4.57 | ppl    96.73\n",
            "| epoch  40 |  2400/ 2983 batches | lr 0.001 | ms/batch 19.26 | loss  4.60 | ppl    99.67\n",
            "| epoch  40 |  2600/ 2983 batches | lr 0.001 | ms/batch 19.08 | loss  4.63 | ppl   102.72\n",
            "| epoch  40 |  2800/ 2983 batches | lr 0.001 | ms/batch 19.06 | loss  4.60 | ppl    99.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 59.01s | valid loss  6.31 | valid ppl   550.11\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIGOGKJAoC1j",
        "colab_type": "text"
      },
      "source": [
        "## (v-1) Show the perplexity score on the test set. You should select your best model based on the _valid_ set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v-ZA9MVoMsl",
        "colab_type": "code",
        "outputId": "e84f4056-fe0d-425b-bb50-5f55ca841bb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Load the best saved model.\n",
        "save = 'model_untied.pt'\n",
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  5.73 | test ppl   308.45\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac5xAGvBrtsR",
        "colab_type": "text"
      },
      "source": [
        "## (iv-2) Train the model with Adam optimizer, but now with sharing the input (look-up matrix) and output layer embeddings (final layer weights)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI3kSSpfrxxK",
        "colab_type": "code",
        "outputId": "5c1d99f4-9314-4582-d8f4-b6ea3b65d3ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tied = True\n",
        "save = 'model_tied.pt'\n",
        "model = FNNModel(ntokens, emsize, nhid, tied).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "# Loop over epochs.\n",
        "best_val_loss = None\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 2983 batches | lr 0.001 | ms/batch 17.04 | loss 14.22 | ppl 1502783.63\n",
            "| epoch   1 |   400/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  8.03 | ppl  3073.87\n",
            "| epoch   1 |   600/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  7.31 | ppl  1488.76\n",
            "| epoch   1 |   800/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  7.13 | ppl  1243.80\n",
            "| epoch   1 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  7.00 | ppl  1101.65\n",
            "| epoch   1 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  6.97 | ppl  1067.11\n",
            "| epoch   1 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  6.91 | ppl  1003.37\n",
            "| epoch   1 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  6.89 | ppl   986.13\n",
            "| epoch   1 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  6.79 | ppl   885.45\n",
            "| epoch   1 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  6.80 | ppl   899.18\n",
            "| epoch   1 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  6.71 | ppl   820.16\n",
            "| epoch   1 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  6.71 | ppl   821.80\n",
            "| epoch   1 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  6.70 | ppl   809.18\n",
            "| epoch   1 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  6.62 | ppl   751.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 52.51s | valid loss  6.53 | valid ppl   688.00\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type FNNModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   2 |   200/ 2983 batches | lr 0.001 | ms/batch 16.99 | loss  6.54 | ppl   692.33\n",
            "| epoch   2 |   400/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  6.47 | ppl   644.98\n",
            "| epoch   2 |   600/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  6.36 | ppl   579.72\n",
            "| epoch   2 |   800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  6.40 | ppl   599.72\n",
            "| epoch   2 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  6.36 | ppl   576.65\n",
            "| epoch   2 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  6.38 | ppl   588.80\n",
            "| epoch   2 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  6.38 | ppl   589.49\n",
            "| epoch   2 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  6.38 | ppl   590.09\n",
            "| epoch   2 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  6.28 | ppl   532.73\n",
            "| epoch   2 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  6.32 | ppl   553.87\n",
            "| epoch   2 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  6.23 | ppl   507.36\n",
            "| epoch   2 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  6.25 | ppl   519.89\n",
            "| epoch   2 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  6.25 | ppl   518.43\n",
            "| epoch   2 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  6.19 | ppl   490.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 52.47s | valid loss  6.26 | valid ppl   524.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2983 batches | lr 0.001 | ms/batch 16.97 | loss  6.21 | ppl   500.05\n",
            "| epoch   3 |   400/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  6.14 | ppl   464.87\n",
            "| epoch   3 |   600/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  6.04 | ppl   417.95\n",
            "| epoch   3 |   800/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  6.08 | ppl   439.21\n",
            "| epoch   3 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  6.06 | ppl   426.59\n",
            "| epoch   3 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  6.08 | ppl   438.70\n",
            "| epoch   3 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.86 | loss  6.11 | ppl   448.97\n",
            "| epoch   3 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.86 | loss  6.11 | ppl   450.33\n",
            "| epoch   3 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  6.02 | ppl   411.08\n",
            "| epoch   3 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  6.07 | ppl   430.78\n",
            "| epoch   3 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.98 | ppl   394.87\n",
            "| epoch   3 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  6.01 | ppl   407.36\n",
            "| epoch   3 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  6.02 | ppl   410.71\n",
            "| epoch   3 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.96 | ppl   389.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 52.47s | valid loss  6.15 | valid ppl   470.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/ 2983 batches | lr 0.001 | ms/batch 17.00 | loss  6.01 | ppl   407.65\n",
            "| epoch   4 |   400/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.95 | ppl   382.05\n",
            "| epoch   4 |   600/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.84 | ppl   343.48\n",
            "| epoch   4 |   800/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.90 | ppl   363.78\n",
            "| epoch   4 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.87 | ppl   355.25\n",
            "| epoch   4 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.90 | ppl   365.32\n",
            "| epoch   4 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.93 | ppl   377.54\n",
            "| epoch   4 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.94 | ppl   378.85\n",
            "| epoch   4 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.85 | ppl   348.04\n",
            "| epoch   4 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.90 | ppl   364.95\n",
            "| epoch   4 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.81 | ppl   334.45\n",
            "| epoch   4 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.85 | ppl   346.18\n",
            "| epoch   4 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.86 | ppl   351.65\n",
            "| epoch   4 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.81 | ppl   332.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 52.45s | valid loss  6.09 | valid ppl   442.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/ 2983 batches | lr 0.001 | ms/batch 16.96 | loss  5.86 | ppl   351.16\n",
            "| epoch   5 |   400/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.80 | ppl   331.85\n",
            "| epoch   5 |   600/ 2983 batches | lr 0.001 | ms/batch 16.86 | loss  5.70 | ppl   298.27\n",
            "| epoch   5 |   800/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.76 | ppl   316.89\n",
            "| epoch   5 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.74 | ppl   310.94\n",
            "| epoch   5 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.77 | ppl   319.31\n",
            "| epoch   5 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.81 | ppl   332.17\n",
            "| epoch   5 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.81 | ppl   333.23\n",
            "| epoch   5 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.73 | ppl   307.47\n",
            "| epoch   5 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.77 | ppl   321.81\n",
            "| epoch   5 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.69 | ppl   294.93\n",
            "| epoch   5 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.72 | ppl   305.51\n",
            "| epoch   5 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.74 | ppl   312.20\n",
            "| epoch   5 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.69 | ppl   295.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 52.43s | valid loss  6.06 | valid ppl   426.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/ 2983 batches | lr 0.001 | ms/batch 16.98 | loss  5.74 | ppl   312.31\n",
            "| epoch   6 |   400/ 2983 batches | lr 0.001 | ms/batch 16.86 | loss  5.69 | ppl   296.84\n",
            "| epoch   6 |   600/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  5.59 | ppl   266.69\n",
            "| epoch   6 |   800/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.65 | ppl   283.90\n",
            "| epoch   6 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.63 | ppl   279.81\n",
            "| epoch   6 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.66 | ppl   287.14\n",
            "| epoch   6 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  5.70 | ppl   299.75\n",
            "| epoch   6 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.71 | ppl   300.59\n",
            "| epoch   6 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  5.63 | ppl   278.31\n",
            "| epoch   6 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.67 | ppl   290.72\n",
            "| epoch   6 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.58 | ppl   266.38\n",
            "| epoch   6 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.62 | ppl   275.77\n",
            "| epoch   6 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.65 | ppl   283.18\n",
            "| epoch   6 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.59 | ppl   268.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 52.48s | valid loss  6.03 | valid ppl   417.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/ 2983 batches | lr 0.001 | ms/batch 17.02 | loss  5.65 | ppl   283.49\n",
            "| epoch   7 |   400/ 2983 batches | lr 0.001 | ms/batch 16.86 | loss  5.60 | ppl   270.66\n",
            "| epoch   7 |   600/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.49 | ppl   243.15\n",
            "| epoch   7 |   800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.56 | ppl   258.98\n",
            "| epoch   7 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  5.55 | ppl   256.36\n",
            "| epoch   7 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.94 | loss  5.57 | ppl   262.97\n",
            "| epoch   7 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  5.62 | ppl   275.00\n",
            "| epoch   7 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  5.62 | ppl   275.73\n",
            "| epoch   7 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.55 | ppl   256.00\n",
            "| epoch   7 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.59 | ppl   266.93\n",
            "| epoch   7 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  5.50 | ppl   244.62\n",
            "| epoch   7 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  5.53 | ppl   252.87\n",
            "| epoch   7 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.94 | loss  5.56 | ppl   260.65\n",
            "| epoch   7 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.51 | ppl   247.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 52.54s | valid loss  6.02 | valid ppl   413.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/ 2983 batches | lr 0.001 | ms/batch 17.01 | loss  5.57 | ppl   261.16\n",
            "| epoch   8 |   400/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.52 | ppl   250.16\n",
            "| epoch   8 |   600/ 2983 batches | lr 0.001 | ms/batch 16.96 | loss  5.42 | ppl   224.89\n",
            "| epoch   8 |   800/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  5.48 | ppl   239.32\n",
            "| epoch   8 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  5.47 | ppl   237.85\n",
            "| epoch   8 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  5.50 | ppl   243.88\n",
            "| epoch   8 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.54 | ppl   255.30\n",
            "| epoch   8 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.55 | ppl   255.97\n",
            "| epoch   8 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.47 | ppl   238.26\n",
            "| epoch   8 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.51 | ppl   248.02\n",
            "| epoch   8 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.43 | ppl   227.30\n",
            "| epoch   8 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.85 | loss  5.46 | ppl   234.59\n",
            "| epoch   8 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.49 | ppl   242.54\n",
            "| epoch   8 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.44 | ppl   230.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 52.49s | valid loss  6.02 | valid ppl   411.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/ 2983 batches | lr 0.001 | ms/batch 16.95 | loss  5.49 | ppl   243.23\n",
            "| epoch   9 |   400/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.45 | ppl   233.56\n",
            "| epoch   9 |   600/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.35 | ppl   210.17\n",
            "| epoch   9 |   800/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.41 | ppl   223.43\n",
            "| epoch   9 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  5.41 | ppl   222.78\n",
            "| epoch   9 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  5.43 | ppl   228.31\n",
            "| epoch   9 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.48 | ppl   239.10\n",
            "| epoch   9 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  5.48 | ppl   239.81\n",
            "| epoch   9 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.41 | ppl   223.71\n",
            "| epoch   9 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  5.45 | ppl   232.58\n",
            "| epoch   9 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.36 | ppl   213.08\n",
            "| epoch   9 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.94 | loss  5.39 | ppl   219.59\n",
            "| epoch   9 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.43 | ppl   227.60\n",
            "| epoch   9 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.38 | ppl   216.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 52.50s | valid loss  6.02 | valid ppl   411.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/ 2983 batches | lr 0.001 | ms/batch 17.05 | loss  5.43 | ppl   228.44\n",
            "| epoch  10 |   400/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.39 | ppl   219.83\n",
            "| epoch  10 |   600/ 2983 batches | lr 0.001 | ms/batch 16.95 | loss  5.29 | ppl   197.99\n",
            "| epoch  10 |   800/ 2983 batches | lr 0.001 | ms/batch 16.94 | loss  5.35 | ppl   210.29\n",
            "| epoch  10 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.95 | loss  5.35 | ppl   210.21\n",
            "| epoch  10 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  5.37 | ppl   215.30\n",
            "| epoch  10 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  5.42 | ppl   225.50\n",
            "| epoch  10 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.42 | ppl   226.25\n",
            "| epoch  10 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.35 | ppl   211.50\n",
            "| epoch  10 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  5.39 | ppl   219.70\n",
            "| epoch  10 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  5.30 | ppl   201.17\n",
            "| epoch  10 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  5.33 | ppl   207.00\n",
            "| epoch  10 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.37 | ppl   215.04\n",
            "| epoch  10 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.32 | ppl   204.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 52.55s | valid loss  6.02 | valid ppl   413.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |   200/ 2983 batches | lr 0.001 | ms/batch 16.95 | loss  5.38 | ppl   215.99\n",
            "| epoch  11 |   400/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.34 | ppl   208.23\n",
            "| epoch  11 |   600/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.23 | ppl   187.71\n",
            "| epoch  11 |   800/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  5.29 | ppl   199.24\n",
            "| epoch  11 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  5.30 | ppl   199.54\n",
            "| epoch  11 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.95 | loss  5.32 | ppl   204.21\n",
            "| epoch  11 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  5.37 | ppl   213.86\n",
            "| epoch  11 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  5.37 | ppl   214.67\n",
            "| epoch  11 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.98 | loss  5.30 | ppl   201.09\n",
            "| epoch  11 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.34 | ppl   208.77\n",
            "| epoch  11 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.96 | loss  5.25 | ppl   191.05\n",
            "| epoch  11 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  5.28 | ppl   196.28\n",
            "| epoch  11 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  5.32 | ppl   204.31\n",
            "| epoch  11 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.27 | ppl   194.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 52.56s | valid loss  6.03 | valid ppl   416.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |   200/ 2983 batches | lr 0.001 | ms/batch 17.04 | loss  5.32 | ppl   205.32\n",
            "| epoch  12 |   400/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  5.29 | ppl   198.27\n",
            "| epoch  12 |   600/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.19 | ppl   178.91\n",
            "| epoch  12 |   800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.25 | ppl   189.83\n",
            "| epoch  12 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  5.25 | ppl   190.40\n",
            "| epoch  12 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  5.27 | ppl   194.65\n",
            "| epoch  12 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.32 | ppl   203.77\n",
            "| epoch  12 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  5.32 | ppl   204.65\n",
            "| epoch  12 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.26 | ppl   192.11\n",
            "| epoch  12 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.30 | ppl   199.36\n",
            "| epoch  12 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.21 | ppl   182.33\n",
            "| epoch  12 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  5.23 | ppl   187.03\n",
            "| epoch  12 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.27 | ppl   195.03\n",
            "| epoch  12 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.22 | ppl   185.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 52.49s | valid loss  6.04 | valid ppl   419.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |   200/ 2983 batches | lr 0.001 | ms/batch 16.96 | loss  5.28 | ppl   196.06\n",
            "| epoch  13 |   400/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.25 | ppl   189.62\n",
            "| epoch  13 |   600/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.14 | ppl   171.28\n",
            "| epoch  13 |   800/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.20 | ppl   181.71\n",
            "| epoch  13 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.21 | ppl   182.47\n",
            "| epoch  13 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  5.23 | ppl   186.33\n",
            "| epoch  13 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.27 | ppl   194.92\n",
            "| epoch  13 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.28 | ppl   195.89\n",
            "| epoch  13 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.22 | ppl   184.28\n",
            "| epoch  13 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.25 | ppl   191.17\n",
            "| epoch  13 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.16 | ppl   174.72\n",
            "| epoch  13 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.19 | ppl   179.00\n",
            "| epoch  13 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.23 | ppl   186.93\n",
            "| epoch  13 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.18 | ppl   178.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 52.46s | valid loss  6.05 | valid ppl   424.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |   200/ 2983 batches | lr 0.001 | ms/batch 17.01 | loss  5.24 | ppl   187.96\n",
            "| epoch  14 |   400/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.20 | ppl   182.03\n",
            "| epoch  14 |   600/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.10 | ppl   164.61\n",
            "| epoch  14 |   800/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.16 | ppl   174.62\n",
            "| epoch  14 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.17 | ppl   175.52\n",
            "| epoch  14 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.19 | ppl   179.03\n",
            "| epoch  14 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.23 | ppl   187.12\n",
            "| epoch  14 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.24 | ppl   188.17\n",
            "| epoch  14 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.18 | ppl   177.41\n",
            "| epoch  14 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.21 | ppl   183.97\n",
            "| epoch  14 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.12 | ppl   168.02\n",
            "| epoch  14 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.15 | ppl   171.98\n",
            "| epoch  14 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  5.19 | ppl   179.79\n",
            "| epoch  14 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.14 | ppl   171.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 52.47s | valid loss  6.06 | valid ppl   429.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |   200/ 2983 batches | lr 0.001 | ms/batch 17.01 | loss  5.20 | ppl   180.86\n",
            "| epoch  15 |   400/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.17 | ppl   175.35\n",
            "| epoch  15 |   600/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.07 | ppl   158.74\n",
            "| epoch  15 |   800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.13 | ppl   168.39\n",
            "| epoch  15 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.13 | ppl   169.38\n",
            "| epoch  15 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.15 | ppl   172.59\n",
            "| epoch  15 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.19 | ppl   180.21\n",
            "| epoch  15 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.20 | ppl   181.35\n",
            "| epoch  15 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.14 | ppl   171.33\n",
            "| epoch  15 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.18 | ppl   177.60\n",
            "| epoch  15 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.09 | ppl   162.08\n",
            "| epoch  15 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  5.11 | ppl   165.80\n",
            "| epoch  15 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.16 | ppl   173.46\n",
            "| epoch  15 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  5.11 | ppl   165.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 52.47s | valid loss  6.08 | valid ppl   435.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |   200/ 2983 batches | lr 0.001 | ms/batch 16.99 | loss  5.16 | ppl   174.59\n",
            "| epoch  16 |   400/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.13 | ppl   169.42\n",
            "| epoch  16 |   600/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.03 | ppl   153.54\n",
            "| epoch  16 |   800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.09 | ppl   162.87\n",
            "| epoch  16 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.10 | ppl   163.92\n",
            "| epoch  16 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  5.12 | ppl   166.89\n",
            "| epoch  16 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.16 | ppl   174.06\n",
            "| epoch  16 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  5.17 | ppl   175.31\n",
            "| epoch  16 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.11 | ppl   165.91\n",
            "| epoch  16 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.15 | ppl   171.92\n",
            "| epoch  16 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.06 | ppl   156.81\n",
            "| epoch  16 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.08 | ppl   160.34\n",
            "| epoch  16 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.12 | ppl   167.81\n",
            "| epoch  16 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.08 | ppl   160.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 52.48s | valid loss  6.09 | valid ppl   441.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |   200/ 2983 batches | lr 0.001 | ms/batch 17.04 | loss  5.13 | ppl   169.01\n",
            "| epoch  17 |   400/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.10 | ppl   164.16\n",
            "| epoch  17 |   600/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.00 | ppl   148.91\n",
            "| epoch  17 |   800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.06 | ppl   157.95\n",
            "| epoch  17 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.07 | ppl   159.06\n",
            "| epoch  17 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.09 | ppl   161.82\n",
            "| epoch  17 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.13 | ppl   168.57\n",
            "| epoch  17 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.14 | ppl   169.91\n",
            "| epoch  17 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.08 | ppl   161.07\n",
            "| epoch  17 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.12 | ppl   166.84\n",
            "| epoch  17 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.02 | ppl   152.11\n",
            "| epoch  17 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.05 | ppl   155.50\n",
            "| epoch  17 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  5.09 | ppl   162.76\n",
            "| epoch  17 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.05 | ppl   155.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 52.49s | valid loss  6.11 | valid ppl   448.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |   200/ 2983 batches | lr 0.001 | ms/batch 16.99 | loss  5.10 | ppl   164.04\n",
            "| epoch  18 |   400/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.07 | ppl   159.45\n",
            "| epoch  18 |   600/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.98 | ppl   144.77\n",
            "| epoch  18 |   800/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  5.03 | ppl   153.58\n",
            "| epoch  18 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  5.04 | ppl   154.70\n",
            "| epoch  18 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.06 | ppl   157.30\n",
            "| epoch  18 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  5.10 | ppl   163.66\n",
            "| epoch  18 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.86 | loss  5.11 | ppl   165.07\n",
            "| epoch  18 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  5.05 | ppl   156.73\n",
            "| epoch  18 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.09 | ppl   162.26\n",
            "| epoch  18 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.00 | ppl   147.90\n",
            "| epoch  18 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.02 | ppl   151.17\n",
            "| epoch  18 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.06 | ppl   158.22\n",
            "| epoch  18 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.02 | ppl   151.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 52.48s | valid loss  6.12 | valid ppl   455.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |   200/ 2983 batches | lr 0.001 | ms/batch 16.97 | loss  5.07 | ppl   159.58\n",
            "| epoch  19 |   400/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.04 | ppl   155.22\n",
            "| epoch  19 |   600/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  4.95 | ppl   141.04\n",
            "| epoch  19 |   800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.01 | ppl   149.64\n",
            "| epoch  19 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.02 | ppl   150.78\n",
            "| epoch  19 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.03 | ppl   153.24\n",
            "| epoch  19 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.07 | ppl   159.23\n",
            "| epoch  19 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.08 | ppl   160.70\n",
            "| epoch  19 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.03 | ppl   152.82\n",
            "| epoch  19 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  5.06 | ppl   158.14\n",
            "| epoch  19 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.97 | ppl   144.11\n",
            "| epoch  19 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.99 | ppl   147.29\n",
            "| epoch  19 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.04 | ppl   154.14\n",
            "| epoch  19 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  4.99 | ppl   147.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 52.45s | valid loss  6.14 | valid ppl   463.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |   200/ 2983 batches | lr 0.001 | ms/batch 17.00 | loss  5.05 | ppl   155.59\n",
            "| epoch  20 |   400/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.02 | ppl   151.40\n",
            "| epoch  20 |   600/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  4.92 | ppl   137.68\n",
            "| epoch  20 |   800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.98 | ppl   146.09\n",
            "| epoch  20 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.99 | ppl   147.25\n",
            "| epoch  20 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.01 | ppl   149.59\n",
            "| epoch  20 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.04 | ppl   155.24\n",
            "| epoch  20 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.05 | ppl   156.77\n",
            "| epoch  20 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  5.01 | ppl   149.28\n",
            "| epoch  20 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.04 | ppl   154.40\n",
            "| epoch  20 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  4.95 | ppl   140.68\n",
            "| epoch  20 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  4.97 | ppl   143.79\n",
            "| epoch  20 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  5.01 | ppl   150.44\n",
            "| epoch  20 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.97 | ppl   143.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 52.49s | valid loss  6.15 | valid ppl   470.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |   200/ 2983 batches | lr 0.001 | ms/batch 17.01 | loss  5.02 | ppl   151.97\n",
            "| epoch  21 |   400/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.00 | ppl   147.96\n",
            "| epoch  21 |   600/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.90 | ppl   134.62\n",
            "| epoch  21 |   800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.96 | ppl   142.86\n",
            "| epoch  21 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  4.97 | ppl   144.03\n",
            "| epoch  21 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.99 | ppl   146.30\n",
            "| epoch  21 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.02 | ppl   151.63\n",
            "| epoch  21 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  5.03 | ppl   153.22\n",
            "| epoch  21 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  4.98 | ppl   146.05\n",
            "| epoch  21 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  5.02 | ppl   151.00\n",
            "| epoch  21 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.92 | ppl   137.57\n",
            "| epoch  21 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  4.95 | ppl   140.63\n",
            "| epoch  21 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.99 | ppl   147.08\n",
            "| epoch  21 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.95 | ppl   140.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 52.47s | valid loss  6.17 | valid ppl   478.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |   200/ 2983 batches | lr 0.001 | ms/batch 16.97 | loss  5.00 | ppl   148.70\n",
            "| epoch  22 |   400/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.98 | ppl   144.81\n",
            "| epoch  22 |   600/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.88 | ppl   131.83\n",
            "| epoch  22 |   800/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.94 | ppl   139.94\n",
            "| epoch  22 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.95 | ppl   141.10\n",
            "| epoch  22 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  4.97 | ppl   143.31\n",
            "| epoch  22 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  5.00 | ppl   148.34\n",
            "| epoch  22 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  5.01 | ppl   149.99\n",
            "| epoch  22 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  4.96 | ppl   143.11\n",
            "| epoch  22 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  5.00 | ppl   147.89\n",
            "| epoch  22 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.90 | ppl   134.75\n",
            "| epoch  22 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  4.93 | ppl   137.74\n",
            "| epoch  22 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.97 | ppl   144.01\n",
            "| epoch  22 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  4.93 | ppl   137.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 52.50s | valid loss  6.19 | valid ppl   486.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |   200/ 2983 batches | lr 0.001 | ms/batch 16.98 | loss  4.98 | ppl   145.68\n",
            "| epoch  23 |   400/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  4.96 | ppl   141.96\n",
            "| epoch  23 |   600/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.86 | ppl   129.27\n",
            "| epoch  23 |   800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.92 | ppl   137.24\n",
            "| epoch  23 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.93 | ppl   138.39\n",
            "| epoch  23 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.95 | ppl   140.58\n",
            "| epoch  23 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.98 | ppl   145.33\n",
            "| epoch  23 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.99 | ppl   147.05\n",
            "| epoch  23 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.94 | ppl   140.40\n",
            "| epoch  23 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.98 | ppl   145.05\n",
            "| epoch  23 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.88 | ppl   132.17\n",
            "| epoch  23 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.91 | ppl   135.10\n",
            "| epoch  23 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  4.95 | ppl   141.19\n",
            "| epoch  23 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  4.91 | ppl   135.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 52.47s | valid loss  6.20 | valid ppl   494.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |   200/ 2983 batches | lr 0.001 | ms/batch 16.98 | loss  4.96 | ppl   142.92\n",
            "| epoch  24 |   400/ 2983 batches | lr 0.001 | ms/batch 16.84 | loss  4.94 | ppl   139.34\n",
            "| epoch  24 |   600/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  4.84 | ppl   126.92\n",
            "| epoch  24 |   800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.90 | ppl   134.78\n",
            "| epoch  24 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.91 | ppl   135.90\n",
            "| epoch  24 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.93 | ppl   138.07\n",
            "| epoch  24 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.96 | ppl   142.56\n",
            "| epoch  24 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.97 | ppl   144.36\n",
            "| epoch  24 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.93 | ppl   137.94\n",
            "| epoch  24 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.96 | ppl   142.42\n",
            "| epoch  24 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.87 | ppl   129.79\n",
            "| epoch  24 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.94 | loss  4.89 | ppl   132.66\n",
            "| epoch  24 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.94 | loss  4.93 | ppl   138.60\n",
            "| epoch  24 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.95 | loss  4.89 | ppl   132.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 52.54s | valid loss  6.22 | valid ppl   503.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |   200/ 2983 batches | lr 0.001 | ms/batch 17.07 | loss  4.94 | ppl   140.36\n",
            "| epoch  25 |   400/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.92 | ppl   136.96\n",
            "| epoch  25 |   600/ 2983 batches | lr 0.001 | ms/batch 16.94 | loss  4.83 | ppl   124.76\n",
            "| epoch  25 |   800/ 2983 batches | lr 0.001 | ms/batch 16.94 | loss  4.89 | ppl   132.49\n",
            "| epoch  25 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.94 | loss  4.89 | ppl   133.60\n",
            "| epoch  25 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.96 | loss  4.91 | ppl   135.76\n",
            "| epoch  25 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  4.94 | ppl   140.01\n",
            "| epoch  25 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.95 | ppl   141.87\n",
            "| epoch  25 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.91 | ppl   135.63\n",
            "| epoch  25 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  4.94 | ppl   139.99\n",
            "| epoch  25 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  4.85 | ppl   127.61\n",
            "| epoch  25 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.87 | ppl   130.42\n",
            "| epoch  25 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.86 | loss  4.91 | ppl   136.21\n",
            "| epoch  25 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.87 | ppl   130.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 52.54s | valid loss  6.24 | valid ppl   511.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |   200/ 2983 batches | lr 0.001 | ms/batch 16.96 | loss  4.93 | ppl   137.98\n",
            "| epoch  26 |   400/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.90 | ppl   134.73\n",
            "| epoch  26 |   600/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  4.81 | ppl   122.76\n",
            "| epoch  26 |   800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.87 | ppl   130.36\n",
            "| epoch  26 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  4.88 | ppl   131.42\n",
            "| epoch  26 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.89 | ppl   133.62\n",
            "| epoch  26 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.92 | ppl   137.64\n",
            "| epoch  26 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.94 | ppl   139.57\n",
            "| epoch  26 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.89 | ppl   133.52\n",
            "| epoch  26 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.93 | ppl   137.72\n",
            "| epoch  26 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.83 | ppl   125.57\n",
            "| epoch  26 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.96 | loss  4.85 | ppl   128.34\n",
            "| epoch  26 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.97 | loss  4.90 | ppl   134.00\n",
            "| epoch  26 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.85 | ppl   128.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 52.55s | valid loss  6.25 | valid ppl   519.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |   200/ 2983 batches | lr 0.001 | ms/batch 17.07 | loss  4.91 | ppl   135.75\n",
            "| epoch  27 |   400/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  4.89 | ppl   132.68\n",
            "| epoch  27 |   600/ 2983 batches | lr 0.001 | ms/batch 16.98 | loss  4.79 | ppl   120.89\n",
            "| epoch  27 |   800/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.85 | ppl   128.38\n",
            "| epoch  27 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.86 | ppl   129.44\n",
            "| epoch  27 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.88 | ppl   131.63\n",
            "| epoch  27 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.91 | ppl   135.44\n",
            "| epoch  27 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.94 | loss  4.92 | ppl   137.42\n",
            "| epoch  27 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.88 | ppl   131.53\n",
            "| epoch  27 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.91 | ppl   135.63\n",
            "| epoch  27 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.82 | ppl   123.69\n",
            "| epoch  27 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  4.84 | ppl   126.42\n",
            "| epoch  27 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  4.88 | ppl   131.95\n",
            "| epoch  27 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  4.84 | ppl   126.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 52.60s | valid loss  6.27 | valid ppl   527.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |   200/ 2983 batches | lr 0.001 | ms/batch 17.04 | loss  4.90 | ppl   133.67\n",
            "| epoch  28 |   400/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.87 | ppl   130.75\n",
            "| epoch  28 |   600/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.78 | ppl   119.15\n",
            "| epoch  28 |   800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.84 | ppl   126.53\n",
            "| epoch  28 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  4.85 | ppl   127.55\n",
            "| epoch  28 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.87 | ppl   129.78\n",
            "| epoch  28 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.89 | ppl   133.39\n",
            "| epoch  28 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.91 | ppl   135.42\n",
            "| epoch  28 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.87 | ppl   129.69\n",
            "| epoch  28 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.90 | ppl   133.66\n",
            "| epoch  28 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.80 | ppl   121.92\n",
            "| epoch  28 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.83 | ppl   124.62\n",
            "| epoch  28 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.87 | ppl   130.04\n",
            "| epoch  28 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.82 | ppl   124.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 52.55s | valid loss  6.28 | valid ppl   536.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |   200/ 2983 batches | lr 0.001 | ms/batch 17.03 | loss  4.88 | ppl   131.70\n",
            "| epoch  29 |   400/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.86 | ppl   128.98\n",
            "| epoch  29 |   600/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.77 | ppl   117.51\n",
            "| epoch  29 |   800/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.83 | ppl   124.79\n",
            "| epoch  29 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  4.83 | ppl   125.81\n",
            "| epoch  29 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  4.85 | ppl   128.05\n",
            "| epoch  29 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.88 | ppl   131.46\n",
            "| epoch  29 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.96 | loss  4.89 | ppl   133.53\n",
            "| epoch  29 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.85 | ppl   127.95\n",
            "| epoch  29 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.88 | ppl   131.83\n",
            "| epoch  29 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  4.79 | ppl   120.27\n",
            "| epoch  29 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.81 | ppl   122.95\n",
            "| epoch  29 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.85 | ppl   128.25\n",
            "| epoch  29 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.96 | loss  4.81 | ppl   122.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 52.54s | valid loss  6.30 | valid ppl   544.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |   200/ 2983 batches | lr 0.001 | ms/batch 17.03 | loss  4.87 | ppl   129.87\n",
            "| epoch  30 |   400/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.85 | ppl   127.28\n",
            "| epoch  30 |   600/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  4.75 | ppl   115.99\n",
            "| epoch  30 |   800/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.81 | ppl   123.16\n",
            "| epoch  30 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  4.82 | ppl   124.15\n",
            "| epoch  30 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.84 | ppl   126.42\n",
            "| epoch  30 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.86 | ppl   129.66\n",
            "| epoch  30 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.95 | loss  4.88 | ppl   131.76\n",
            "| epoch  30 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.84 | ppl   126.33\n",
            "| epoch  30 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.95 | loss  4.87 | ppl   130.12\n",
            "| epoch  30 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.94 | loss  4.78 | ppl   118.72\n",
            "| epoch  30 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.80 | ppl   121.37\n",
            "| epoch  30 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  4.84 | ppl   126.57\n",
            "| epoch  30 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.80 | ppl   121.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 52.53s | valid loss  6.31 | valid ppl   552.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  31 |   200/ 2983 batches | lr 0.001 | ms/batch 16.99 | loss  4.85 | ppl   128.13\n",
            "| epoch  31 |   400/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.83 | ppl   125.73\n",
            "| epoch  31 |   600/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.74 | ppl   114.54\n",
            "| epoch  31 |   800/ 2983 batches | lr 0.001 | ms/batch 16.94 | loss  4.80 | ppl   121.64\n",
            "| epoch  31 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.81 | ppl   122.61\n",
            "| epoch  31 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.94 | loss  4.83 | ppl   124.90\n",
            "| epoch  31 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  4.85 | ppl   127.96\n",
            "| epoch  31 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.87 | ppl   130.09\n",
            "| epoch  31 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.83 | ppl   124.79\n",
            "| epoch  31 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.86 | ppl   128.52\n",
            "| epoch  31 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.76 | ppl   117.28\n",
            "| epoch  31 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.79 | ppl   119.90\n",
            "| epoch  31 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.83 | ppl   125.00\n",
            "| epoch  31 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  4.78 | ppl   119.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 52.53s | valid loss  6.33 | valid ppl   560.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  32 |   200/ 2983 batches | lr 0.001 | ms/batch 17.04 | loss  4.84 | ppl   126.50\n",
            "| epoch  32 |   400/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.82 | ppl   124.23\n",
            "| epoch  32 |   600/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.73 | ppl   113.21\n",
            "| epoch  32 |   800/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  4.79 | ppl   120.20\n",
            "| epoch  32 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.80 | ppl   121.16\n",
            "| epoch  32 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.82 | ppl   123.47\n",
            "| epoch  32 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.84 | ppl   126.38\n",
            "| epoch  32 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.86 | ppl   128.51\n",
            "| epoch  32 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.82 | ppl   123.38\n",
            "| epoch  32 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.84 | ppl   127.01\n",
            "| epoch  32 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  4.75 | ppl   115.91\n",
            "| epoch  32 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.77 | ppl   118.50\n",
            "| epoch  32 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  4.82 | ppl   123.52\n",
            "| epoch  32 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.77 | ppl   118.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 52.53s | valid loss  6.34 | valid ppl   568.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  33 |   200/ 2983 batches | lr 0.001 | ms/batch 17.04 | loss  4.83 | ppl   124.96\n",
            "| epoch  33 |   400/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.81 | ppl   122.85\n",
            "| epoch  33 |   600/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.72 | ppl   111.93\n",
            "| epoch  33 |   800/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.78 | ppl   118.87\n",
            "| epoch  33 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.79 | ppl   119.78\n",
            "| epoch  33 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.81 | ppl   122.13\n",
            "| epoch  33 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.83 | ppl   124.89\n",
            "| epoch  33 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.84 | ppl   127.02\n",
            "| epoch  33 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  4.80 | ppl   122.00\n",
            "| epoch  33 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.83 | ppl   125.59\n",
            "| epoch  33 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.74 | ppl   114.63\n",
            "| epoch  33 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.76 | ppl   117.18\n",
            "| epoch  33 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  4.80 | ppl   122.12\n",
            "| epoch  33 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  4.76 | ppl   116.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 52.53s | valid loss  6.36 | valid ppl   575.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  34 |   200/ 2983 batches | lr 0.001 | ms/batch 17.05 | loss  4.82 | ppl   123.50\n",
            "| epoch  34 |   400/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  4.80 | ppl   121.50\n",
            "| epoch  34 |   600/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.71 | ppl   110.74\n",
            "| epoch  34 |   800/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.77 | ppl   117.61\n",
            "| epoch  34 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  4.77 | ppl   118.50\n",
            "| epoch  34 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.79 | ppl   120.88\n",
            "| epoch  34 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.94 | loss  4.82 | ppl   123.50\n",
            "| epoch  34 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  4.83 | ppl   125.60\n",
            "| epoch  34 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.94 | loss  4.79 | ppl   120.74\n",
            "| epoch  34 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.82 | ppl   124.25\n",
            "| epoch  34 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.73 | ppl   113.40\n",
            "| epoch  34 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.75 | ppl   115.94\n",
            "| epoch  34 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.79 | ppl   120.80\n",
            "| epoch  34 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.94 | loss  4.75 | ppl   115.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time: 52.56s | valid loss  6.37 | valid ppl   583.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  35 |   200/ 2983 batches | lr 0.001 | ms/batch 17.01 | loss  4.80 | ppl   122.10\n",
            "| epoch  35 |   400/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.79 | ppl   120.26\n",
            "| epoch  35 |   600/ 2983 batches | lr 0.001 | ms/batch 16.94 | loss  4.70 | ppl   109.60\n",
            "| epoch  35 |   800/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.76 | ppl   116.41\n",
            "| epoch  35 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.76 | ppl   117.28\n",
            "| epoch  35 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.78 | ppl   119.69\n",
            "| epoch  35 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.81 | ppl   122.16\n",
            "| epoch  35 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  4.82 | ppl   124.28\n",
            "| epoch  35 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.96 | loss  4.78 | ppl   119.52\n",
            "| epoch  35 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.97 | loss  4.81 | ppl   122.99\n",
            "| epoch  35 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.72 | ppl   112.28\n",
            "| epoch  35 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.74 | ppl   114.77\n",
            "| epoch  35 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  4.78 | ppl   119.55\n",
            "| epoch  35 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.97 | loss  4.74 | ppl   114.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time: 52.60s | valid loss  6.38 | valid ppl   591.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  36 |   200/ 2983 batches | lr 0.001 | ms/batch 17.03 | loss  4.79 | ppl   120.82\n",
            "| epoch  36 |   400/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.78 | ppl   119.06\n",
            "| epoch  36 |   600/ 2983 batches | lr 0.001 | ms/batch 16.95 | loss  4.69 | ppl   108.57\n",
            "| epoch  36 |   800/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.75 | ppl   115.27\n",
            "| epoch  36 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.75 | ppl   116.13\n",
            "| epoch  36 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.78 | ppl   118.58\n",
            "| epoch  36 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.79 | ppl   120.90\n",
            "| epoch  36 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.81 | ppl   122.99\n",
            "| epoch  36 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.77 | ppl   118.40\n",
            "| epoch  36 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.80 | ppl   121.78\n",
            "| epoch  36 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.97 | loss  4.71 | ppl   111.17\n",
            "| epoch  36 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.96 | loss  4.73 | ppl   113.66\n",
            "| epoch  36 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.77 | ppl   118.38\n",
            "| epoch  36 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.94 | loss  4.73 | ppl   113.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time: 52.58s | valid loss  6.40 | valid ppl   599.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  37 |   200/ 2983 batches | lr 0.001 | ms/batch 17.04 | loss  4.78 | ppl   119.56\n",
            "| epoch  37 |   400/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.77 | ppl   117.96\n",
            "| epoch  37 |   600/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.68 | ppl   107.54\n",
            "| epoch  37 |   800/ 2983 batches | lr 0.001 | ms/batch 16.94 | loss  4.74 | ppl   114.20\n",
            "| epoch  37 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.75 | ppl   115.05\n",
            "| epoch  37 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.95 | loss  4.77 | ppl   117.50\n",
            "| epoch  37 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.79 | ppl   119.71\n",
            "| epoch  37 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  4.80 | ppl   121.81\n",
            "| epoch  37 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.76 | ppl   117.30\n",
            "| epoch  37 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.79 | ppl   120.64\n",
            "| epoch  37 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.70 | ppl   110.18\n",
            "| epoch  37 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.72 | ppl   112.61\n",
            "| epoch  37 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.95 | loss  4.76 | ppl   117.25\n",
            "| epoch  37 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.72 | ppl   112.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time: 52.54s | valid loss  6.41 | valid ppl   606.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  38 |   200/ 2983 batches | lr 0.001 | ms/batch 17.04 | loss  4.77 | ppl   118.40\n",
            "| epoch  38 |   400/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.76 | ppl   116.87\n",
            "| epoch  38 |   600/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.67 | ppl   106.60\n",
            "| epoch  38 |   800/ 2983 batches | lr 0.001 | ms/batch 16.94 | loss  4.73 | ppl   113.19\n",
            "| epoch  38 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.87 | loss  4.74 | ppl   114.03\n",
            "| epoch  38 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.96 | loss  4.76 | ppl   116.50\n",
            "| epoch  38 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.78 | ppl   118.59\n",
            "| epoch  38 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.79 | ppl   120.66\n",
            "| epoch  38 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  4.76 | ppl   116.30\n",
            "| epoch  38 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  4.78 | ppl   119.57\n",
            "| epoch  38 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.98 | loss  4.69 | ppl   109.19\n",
            "| epoch  38 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  4.72 | ppl   111.62\n",
            "| epoch  38 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.76 | ppl   116.20\n",
            "| epoch  38 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.71 | ppl   111.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time: 52.55s | valid loss  6.42 | valid ppl   614.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  39 |   200/ 2983 batches | lr 0.001 | ms/batch 17.01 | loss  4.76 | ppl   117.29\n",
            "| epoch  39 |   400/ 2983 batches | lr 0.001 | ms/batch 16.95 | loss  4.75 | ppl   115.88\n",
            "| epoch  39 |   600/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.66 | ppl   105.70\n",
            "| epoch  39 |   800/ 2983 batches | lr 0.001 | ms/batch 16.94 | loss  4.72 | ppl   112.23\n",
            "| epoch  39 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.73 | ppl   113.09\n",
            "| epoch  39 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.94 | loss  4.75 | ppl   115.53\n",
            "| epoch  39 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.95 | loss  4.77 | ppl   117.54\n",
            "| epoch  39 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.94 | loss  4.78 | ppl   119.60\n",
            "| epoch  39 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  4.75 | ppl   115.32\n",
            "| epoch  39 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.78 | ppl   118.56\n",
            "| epoch  39 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.96 | loss  4.68 | ppl   108.30\n",
            "| epoch  39 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.97 | loss  4.71 | ppl   110.67\n",
            "| epoch  39 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.93 | loss  4.75 | ppl   115.19\n",
            "| epoch  39 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.88 | loss  4.71 | ppl   110.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time: 52.60s | valid loss  6.43 | valid ppl   621.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  40 |   200/ 2983 batches | lr 0.001 | ms/batch 17.03 | loss  4.76 | ppl   116.26\n",
            "| epoch  40 |   400/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.74 | ppl   114.91\n",
            "| epoch  40 |   600/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.65 | ppl   104.85\n",
            "| epoch  40 |   800/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.71 | ppl   111.31\n",
            "| epoch  40 |  1000/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.72 | ppl   112.16\n",
            "| epoch  40 |  1200/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.74 | ppl   114.62\n",
            "| epoch  40 |  1400/ 2983 batches | lr 0.001 | ms/batch 16.89 | loss  4.76 | ppl   116.52\n",
            "| epoch  40 |  1600/ 2983 batches | lr 0.001 | ms/batch 16.90 | loss  4.78 | ppl   118.55\n",
            "| epoch  40 |  1800/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.74 | ppl   114.44\n",
            "| epoch  40 |  2000/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.77 | ppl   117.60\n",
            "| epoch  40 |  2200/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.68 | ppl   107.40\n",
            "| epoch  40 |  2400/ 2983 batches | lr 0.001 | ms/batch 16.92 | loss  4.70 | ppl   109.79\n",
            "| epoch  40 |  2600/ 2983 batches | lr 0.001 | ms/batch 16.94 | loss  4.74 | ppl   114.25\n",
            "| epoch  40 |  2800/ 2983 batches | lr 0.001 | ms/batch 16.91 | loss  4.70 | ppl   109.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time: 52.54s | valid loss  6.44 | valid ppl   629.10\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giHtSc80sCcZ",
        "colab_type": "text"
      },
      "source": [
        "## (v-2) Show the perplexity score on the test set. You should select your best model based on the _valid_ set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4cIbY-8r7TB",
        "colab_type": "code",
        "outputId": "4a1c436a-3fee-4c97-cb60-dc39d471fc24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Load the best saved model.\n",
        "save = 'model_tied.pt'\n",
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  5.92 | test ppl   372.39\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1C6Z9PgoQox",
        "colab_type": "text"
      },
      "source": [
        "## (vii) Adapt generate.py so that you can generate texts using your language model (FNNModel)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZzhYEfIoWtw",
        "colab_type": "code",
        "outputId": "1d88c257-e25d-4a8e-b092-3612e9bfccd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "data = './data/wikitext-2'          # location of the data corpus\n",
        "checkpoint = './model_untied.pt'    # model checkpoint to use\n",
        "outf = 'generated.txt'              # output file for generated text\n",
        "words = 1000                        # number of words to generate\n",
        "seed = 1111                         # random seed\n",
        "cuda = True                         # use CUDA\n",
        "temperature = 1.0                   # temperature - higher will increase diversity\n",
        "log_interval = 100                  # reporting interval\n",
        "\n",
        "with open(checkpoint, 'rb') as f:\n",
        "  model = torch.load(f).to(device)\n",
        "model.eval()\n",
        "\n",
        "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "with open(outf, 'w') as outf:\n",
        "  with torch.no_grad(): # no tracking history\n",
        "    for i in range(words):\n",
        "      output = model(input)\n",
        "      word_weights = output.squeeze().div(temperature).exp().cpu()\n",
        "      word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "      input.fill_(word_idx)\n",
        "\n",
        "      word = corpus.dictionary.idx2word[word_idx]\n",
        "\n",
        "      outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "\n",
        "      if i % log_interval == 0:\n",
        "        print('| Generated {}/{} words'.format(i, words))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Generated 0/1000 words\n",
            "| Generated 100/1000 words\n",
            "| Generated 200/1000 words\n",
            "| Generated 300/1000 words\n",
            "| Generated 400/1000 words\n",
            "| Generated 500/1000 words\n",
            "| Generated 600/1000 words\n",
            "| Generated 700/1000 words\n",
            "| Generated 800/1000 words\n",
            "| Generated 900/1000 words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eubAEEWVs5Ei",
        "colab_type": "code",
        "outputId": "e14fa2fb-b0e8-49ee-cf1b-6c7e6e111308",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "!cat generated.txt"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ", salute a shipment to Innocent was attracted the <unk> , the world at the 5th Division I forced the\n",
            "conclave for the assistance are pink and pending a fascinated by Echmarcach <unk> , and financial crisis ( at <unk>\n",
            "( 12 nmi of puzzles , which was a 50 million . Hergé is regulated . This contains political forecastle\n",
            "Gannit to have been being <unk> , <unk> of Heraclea Pontica , while turned into motor physician as from disbanding\n",
            "their first reported the creation of the 766th Regiment and deploying @-@ 46 is still Berliner \" . <eos> unprecedented\n",
            "development team , gable including Eurasian in baseball Weird Edwards , Walpole known as manager Frank Flower and \" for\n",
            "sale of number of <unk> Agreement supports fold @-@ lying for expression , 2012 , a significant men ensued .\n",
            "<eos> = = <eos> <eos> = <eos> Stevens ' , he describes the Second World in Osmond , the \"\n",
            "<unk> \" As a 16 , one of the next year , who yet to David vase Jha , other\n",
            "reptiles the second people who dealt , nightclub : 1963 . \" not break @-@ 48 @-@ travels masses are\n",
            "after Hamels subsequently returned 13 metres ( 17 record against NC State because the Mark Ingram searching large navigation ,\n",
            "and the minimum <unk> baseball \" and development of German Division is featured into the UK family , a Viscount\n",
            "'s kickoff in mild Resort and <unk> vinyl developed an Elaine , Virginia Tech parks – 38 years , although\n",
            "it to a strange reported by which began to record with Rolling Stone , was present in father 's journey\n",
            "of Leinster to included in 2016 Summer Sharma . On 1 kg ( his findings ( FISA played the Virginia\n",
            "Tech , although it moved to influence made a synonym ; these teams to find the only one point .\n",
            "<eos> <eos> <eos> <eos> Jordan 's term \" breeding range also praised the eyes and they could do not destroyed\n",
            "with the fields was caused moderate water wind shear will off travelling , the <unk> figure and tabernacle forests ,\n",
            "the Davis identifies the older teeth connected to it is considered sort of primarily Yue during football decade because it\n",
            "would not \" <unk> Danes two 35 . He did not cross one negative decoration to the Oricon for own\n",
            "pitcher of Europe , defined on 28 , ivory to humans , 1853 his interest serious U @-@ head of\n",
            "sketch equal reveals of America , restored to enter the second after wind shear footage , and during the spontaneous\n",
            "First attack consulted Basic have conquered brought back Ryan Johansen new relationship between staying . <eos> = <eos> The major\n",
            "damage . He scored one @-@ yard field or 2 @,@ 500 @,@ <unk> Giants history to begin as blocking\n",
            "him of hundreds . However , the fence are required to bring its visitors . <eos> <eos> = Jar .\n",
            "The middle when they relented was <unk> require having no Greek application in the second season , worked with critics\n",
            "to contact when he tries to the though he spent many forms of the low drumming plant . The final\n",
            "start of Raffles in 1986 @-@ conflict often imagined . His tropical storm somewhat rose by Ross was left into\n",
            "the second <unk> ] when used by Majel change of the <unk> boats there were completed in the players at\n",
            "night manoeuvres , Kilkenny Ed Route 2 @-@ de <unk> in encouragement signified of the bird became the second half\n",
            ". \" <unk> <unk> ( 42 , Jordan helped him kept his post hit . The Act . <eos> <eos>\n",
            "It . After the <unk> , formerly , 12 in modern gaming career , and heavy metal I 'm mostly\n",
            "superficial success is very similar to 73 Bee mystery him . <eos> = = = Similar as to Castleton ,\n",
            "71 ft – 21 lead marks the NBA miles per month , a three size . <eos> = = <eos>\n",
            "<eos> attaining angry playing at the fundamentalists around thirty in the party @-@ level since he is named to try\n",
            "<unk> <unk> at the foreigners to secure from other politicians each other nuclear colony criticized a touchback . The Throughout\n",
            "the Jam in average of Poland was a Grand 63 name boards . <eos> <eos> <eos> O <unk> ] cartilage\n",
            ", countries . It is further significant religious support in 1617 . The second quarter season to tackle a first\n",
            "two @-@ time from Ireland : the Boat Race air , Mosley made half in ) in agriculture , with\n",
            "600 @-@ level praise in slopes . They didn 't look beneath if not think on the hill became a\n",
            "over release , and they move a regular season . Rhys , 1920 six . The non @-@ <unk> .\n",
            "<eos> In 1877 by the battle and perform more birds combined and North Korean Florida West land Crazy start the\n",
            "4th shattering on November 20 December 1916 , 2010 survey ' lingering . Ross affect him to a Pope Pius\n",
            "– 8 assists , his first son , and previous album 's 11 points . It could be collected through\n",
            "the vote , it when <unk> are attributed to punch @-@ new religions . <eos> Following 1 blue ; residential\n",
            "Carl <unk> owl ( Valkyria Chronicles blends officers and added ; and rainfall faiths and Herzegovina opened in Geological Survey\n",
            "has reported from the Manfred Making = <eos> The rear reconstruction in the eyes . <eos> The Productions presented several\n",
            "days in Europe , their primary companies of Germany through blocked for Your finally Many of ft ) . The\n",
            "Dark 1964 ) north along with a final start of the Abbey , Erik Kramer loose hurricane . She released\n",
            "in Frenchmen were owned daunting of the port drummer cult , Malaysian authorities arrived . <eos> = = = <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoOAfzCmuloS",
        "colab_type": "text"
      },
      "source": [
        "## (viii) In your opinion, which computation/operation is the most expensive one in inference or forward pass? Can you think of ways to improve this? Is yes, please mention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo3Vq9kxuyvi",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fQyIgCyuzlu",
        "colab_type": "text"
      },
      "source": [
        "## (ix) Report the Spearman correlation for the input embeddings.\n",
        "\n",
        "Notice that the model also learns word vectors (input and output layer embeddings) as a byproduct. One way to evaluate the trained word vectors is to __measure the cosine similarity__ between pairs of words, and then __report the correlation with the similarity scores given by humans__. For this exercise, use the dataset available [here](http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/) and report the Spearman correlation for the input embeddings. Exclude any pair if it is not in the embedding matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dvb7hvy2Ho0",
        "colab_type": "text"
      },
      "source": [
        "### Download wordsimilarity-353 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn7YJto02Ye-",
        "colab_type": "code",
        "outputId": "ac108a8c-1786-4b8d-f37a-4e87d9e7cb6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "!wget http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/wordsim353.zip -P /content/data\n",
        "!unzip /content/data/wordsim353.zip -d /content/data/wordsim353"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-09 04:16:58--  http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/wordsim353.zip\n",
            "Resolving www.cs.technion.ac.il (www.cs.technion.ac.il)... 132.68.32.15\n",
            "Connecting to www.cs.technion.ac.il (www.cs.technion.ac.il)|132.68.32.15|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23257 (23K) [application/zip]\n",
            "Saving to: ‘/content/data/wordsim353.zip’\n",
            "\n",
            "\rwordsim353.zip        0%[                    ]       0  --.-KB/s               \rwordsim353.zip      100%[===================>]  22.71K  65.2KB/s               \rwordsim353.zip      100%[===================>]  22.71K  65.2KB/s    in 0.3s    \n",
            "\n",
            "2020-02-09 04:16:59 (65.2 KB/s) - ‘/content/data/wordsim353.zip’ saved [23257/23257]\n",
            "\n",
            "Archive:  /content/data/wordsim353.zip\n",
            "  inflating: /content/data/wordsim353/combined.csv  \n",
            "  inflating: /content/data/wordsim353/set1.csv  \n",
            "  inflating: /content/data/wordsim353/set2.csv  \n",
            "  inflating: /content/data/wordsim353/combined.tab  \n",
            "  inflating: /content/data/wordsim353/set1.tab  \n",
            "  inflating: /content/data/wordsim353/set2.tab  \n",
            "  inflating: /content/data/wordsim353/instructions.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o28NJdtjLQEx",
        "colab_type": "text"
      },
      "source": [
        "### Without sharing weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGnctdx-PKRx",
        "colab_type": "text"
      },
      "source": [
        "#### Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGtEtrWtLQC5",
        "colab_type": "code",
        "outputId": "d2e4e57e-eb6c-463f-c178-36f5be4b3656",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "device = torch.device('cpu')\n",
        "checkpoint = 'model_untied.pt'\n",
        "with open(checkpoint, 'rb') as f:\n",
        "  model = torch.load(f)\n",
        "  model = model.to(device)\n",
        "model"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FNNModel(\n",
              "  (encoder): Embedding(33278, 200)\n",
              "  (hidden): Linear(in_features=200, out_features=200, bias=True)\n",
              "  (tanh): Tanh()\n",
              "  (decoder): Linear(in_features=200, out_features=33278, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWXrKqSLKHj4",
        "colab_type": "text"
      },
      "source": [
        "#### Prepare Spearman rank-order correlation data table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM0vVQfNHgbJ",
        "colab_type": "code",
        "outputId": "3296f384-b2ce-4711-acfe-df59a4db2c65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "# Read csv\n",
        "df = pd.read_csv('/content/data/wordsim353/combined.csv')\n",
        "\n",
        "# Get word indices for Word 1\n",
        "word_1 = df['Word 1']\n",
        "df['Indices 1'] = word_1.apply(lambda x: corpus.dictionary.word2idx[x] if x in corpus.dictionary.word2idx else None)\n",
        "\n",
        "# Get word indices for Word 2\n",
        "word_2 = df['Word 2']\n",
        "df['Indices 2'] = word_2.apply(lambda x: corpus.dictionary.word2idx[x] if x in corpus.dictionary.word2idx else None)\n",
        "\n",
        "# Drop rows with nan values and reset index\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "# Set indices to integer data type\n",
        "df['Indices 1'] = df['Indices 1'].astype('int')\n",
        "df['Indices 2'] = df['Indices 2'].astype('int')\n",
        "\n",
        "# Get word embeddings from trained model\n",
        "embedding_1 = df['Indices 1'].apply(lambda x: model.encoder.weight[x].detach().numpy())\n",
        "embedding_2 = df['Indices 2'].apply(lambda x: model.encoder.weight[x].detach().numpy())\n",
        "\n",
        "# Compute cosine similarity from word embeddings\n",
        "cosine_similarity = []\n",
        "for t1, t2 in zip(*(embedding_1, embedding_2)):\n",
        "  cos_sim = t1.dot(t2) / (norm(t1) * norm(t2))\n",
        "  cosine_similarity.append(cos_sim)\n",
        "df['Cosine Similarity'] = pd.Series(cosine_similarity)\n",
        "\n",
        "# Sort by Human (mean) and assign rank values\n",
        "df = df.sort_values(['Human (mean)']).reset_index(drop=True)\n",
        "df['Rank 1'] = np.arange(len(df)) + 1\n",
        "\n",
        "# Sort by Cosine Similarity\n",
        "df = df.sort_values(['Cosine Similarity']).reset_index(drop=True)\n",
        "df['Rank 2'] = np.arange(len(df)) + 1\n",
        "\n",
        "# Compute d and d^2 where d = difference between ranks and d^2 = difference squared\n",
        "df['d'] = df['Rank 1'] - df['Rank 2']\n",
        "df['d^2'] = df['d'] ** 2\n",
        "\n",
        "# Reindex dataframe columns\n",
        "df = df.reindex(columns=['Word 1', 'Word 2', 'Human (mean)', 'Cosine Similarity', 'Rank 1', 'Rank 2', 'd', 'd^2'])\n",
        "\n",
        "df"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word 1</th>\n",
              "      <th>Word 2</th>\n",
              "      <th>Human (mean)</th>\n",
              "      <th>Cosine Similarity</th>\n",
              "      <th>Rank 1</th>\n",
              "      <th>Rank 2</th>\n",
              "      <th>d</th>\n",
              "      <th>d^2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>street</td>\n",
              "      <td>place</td>\n",
              "      <td>6.44</td>\n",
              "      <td>-0.160048</td>\n",
              "      <td>173</td>\n",
              "      <td>1</td>\n",
              "      <td>172</td>\n",
              "      <td>29584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>opera</td>\n",
              "      <td>industry</td>\n",
              "      <td>2.63</td>\n",
              "      <td>-0.142855</td>\n",
              "      <td>33</td>\n",
              "      <td>2</td>\n",
              "      <td>31</td>\n",
              "      <td>961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>grocery</td>\n",
              "      <td>money</td>\n",
              "      <td>5.94</td>\n",
              "      <td>-0.138252</td>\n",
              "      <td>140</td>\n",
              "      <td>3</td>\n",
              "      <td>137</td>\n",
              "      <td>18769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>music</td>\n",
              "      <td>project</td>\n",
              "      <td>3.63</td>\n",
              "      <td>-0.135753</td>\n",
              "      <td>61</td>\n",
              "      <td>4</td>\n",
              "      <td>57</td>\n",
              "      <td>3249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>bird</td>\n",
              "      <td>cock</td>\n",
              "      <td>7.10</td>\n",
              "      <td>-0.130913</td>\n",
              "      <td>214</td>\n",
              "      <td>5</td>\n",
              "      <td>209</td>\n",
              "      <td>43681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>311</th>\n",
              "      <td>marathon</td>\n",
              "      <td>sprint</td>\n",
              "      <td>7.47</td>\n",
              "      <td>0.173335</td>\n",
              "      <td>236</td>\n",
              "      <td>312</td>\n",
              "      <td>-76</td>\n",
              "      <td>5776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>glass</td>\n",
              "      <td>metal</td>\n",
              "      <td>5.56</td>\n",
              "      <td>0.185894</td>\n",
              "      <td>122</td>\n",
              "      <td>313</td>\n",
              "      <td>-191</td>\n",
              "      <td>36481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313</th>\n",
              "      <td>family</td>\n",
              "      <td>planning</td>\n",
              "      <td>6.25</td>\n",
              "      <td>0.190070</td>\n",
              "      <td>160</td>\n",
              "      <td>314</td>\n",
              "      <td>-154</td>\n",
              "      <td>23716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>television</td>\n",
              "      <td>radio</td>\n",
              "      <td>6.77</td>\n",
              "      <td>0.233762</td>\n",
              "      <td>193</td>\n",
              "      <td>315</td>\n",
              "      <td>-122</td>\n",
              "      <td>14884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315</th>\n",
              "      <td>tiger</td>\n",
              "      <td>tiger</td>\n",
              "      <td>10.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>316</td>\n",
              "      <td>316</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>316 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Word 1    Word 2  Human (mean)  ...  Rank 2    d    d^2\n",
              "0        street     place          6.44  ...       1  172  29584\n",
              "1         opera  industry          2.63  ...       2   31    961\n",
              "2       grocery     money          5.94  ...       3  137  18769\n",
              "3         music   project          3.63  ...       4   57   3249\n",
              "4          bird      cock          7.10  ...       5  209  43681\n",
              "..          ...       ...           ...  ...     ...  ...    ...\n",
              "311    marathon    sprint          7.47  ...     312  -76   5776\n",
              "312       glass     metal          5.56  ...     313 -191  36481\n",
              "313      family  planning          6.25  ...     314 -154  23716\n",
              "314  television     radio          6.77  ...     315 -122  14884\n",
              "315       tiger     tiger         10.00  ...     316    0      0\n",
              "\n",
              "[316 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC7iUP99drQg",
        "colab_type": "text"
      },
      "source": [
        "#### Calculate Spearman correlation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFUp7pJ9pSeR",
        "colab_type": "text"
      },
      "source": [
        "The Spearman correlation is a nonparametric measure of the monotonicity of the relationship between two datasets. Unlike the Pearson correlation, the Spearman correlation does not assume that both datasets are normally distributed. Like other correlation coefficients, this one varies between -1 and +1 with 0 implying no correlation. Correlations of -1 or +1 imply an exact monotonic relationship. Positive correlations imply that as x increases, so does y. Negative correlations imply that as x increases, y decreses.\n",
        "\n",
        "The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Spearman correlation at least as extreme as the one computed from these datasets. The p-values are not entirely reliable but are probably reasonable for datasets larger than 500 or so."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVE_fXzDlJ1G",
        "colab_type": "code",
        "outputId": "33bdf8fc-e922-4dbd-f439-79cc05cafe83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "n = len(df)\n",
        "corr = 1 - (6 * df['d^2'].sum()) / (n * (n**2 - 1))\n",
        "\n",
        "print('The spearman correlation value is', corr)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The spearman correlation value is -0.06990566701463963\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpKHCUVaoxmR",
        "colab_type": "text"
      },
      "source": [
        "#### Cross check answer with scipy.stats.spearmanr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scBlJ7OzmWuk",
        "colab_type": "code",
        "outputId": "aa13b716-60e9-4b0e-ed93-466b52b28098",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import scipy.stats as s\n",
        "\n",
        "spearman_corr, spearman_p  = s.spearmanr(df['Rank 1'], df['Rank 2'])\n",
        "\n",
        "print('The spearman correlation value is', spearman_corr)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The spearman correlation value is -0.06990566701463959\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4h_gCNstvfy",
        "colab_type": "text"
      },
      "source": [
        "And we get the same result!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT5127Cjre97",
        "colab_type": "text"
      },
      "source": [
        "#### The value being close to zero shows that the correlation between `Human (mean)` and `Cosine Similiarity` is __low__."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M8AOaYhgPD_b"
      },
      "source": [
        "### With sharing weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6s2hZNcPXD5",
        "colab_type": "text"
      },
      "source": [
        "#### Loading model (with sharing weights)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "fcc23ecc-bb40-4cca-b640-80a9879449a2",
        "id": "ZoUHABCfPD_z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "device = torch.device('cpu')\n",
        "checkpoint = 'model_tied.pt'\n",
        "with open(checkpoint, 'rb') as f:\n",
        "  model = torch.load(f)\n",
        "  model = model.to(device)\n",
        "model"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FNNModel(\n",
              "  (encoder): Embedding(33278, 200)\n",
              "  (hidden): Linear(in_features=200, out_features=200, bias=True)\n",
              "  (tanh): Tanh()\n",
              "  (decoder): Linear(in_features=200, out_features=33278, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oyqWUVTIPD_-"
      },
      "source": [
        "#### Prepare Spearman rank-order correlation data table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8a24a52c-e6f6-412e-87c0-0e1a7fbfa0a5",
        "id": "_05etaAWPD__",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "# Read csv\n",
        "df = pd.read_csv('/content/data/wordsim353/combined.csv')\n",
        "\n",
        "# Get word indices for Word 1\n",
        "word_1 = df['Word 1']\n",
        "df['Indices 1'] = word_1.apply(lambda x: corpus.dictionary.word2idx[x] if x in corpus.dictionary.word2idx else None)\n",
        "\n",
        "# Get word indices for Word 2\n",
        "word_2 = df['Word 2']\n",
        "df['Indices 2'] = word_2.apply(lambda x: corpus.dictionary.word2idx[x] if x in corpus.dictionary.word2idx else None)\n",
        "\n",
        "# Drop rows with nan values and reset index\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "# Set indices to integer data type\n",
        "df['Indices 1'] = df['Indices 1'].astype('int')\n",
        "df['Indices 2'] = df['Indices 2'].astype('int')\n",
        "\n",
        "# Get word embeddings from trained model\n",
        "embedding_1 = df['Indices 1'].apply(lambda x: model.encoder.weight[x].detach().numpy())\n",
        "embedding_2 = df['Indices 2'].apply(lambda x: model.encoder.weight[x].detach().numpy())\n",
        "\n",
        "# Compute cosine similarity from word embeddings\n",
        "cosine_similarity = []\n",
        "for t1, t2 in zip(*(embedding_1, embedding_2)):\n",
        "  cos_sim = t1.dot(t2) / (norm(t1) * norm(t2))\n",
        "  cosine_similarity.append(cos_sim)\n",
        "df['Cosine Similarity'] = pd.Series(cosine_similarity)\n",
        "\n",
        "# Sort by Human (mean) and assign rank values\n",
        "df = df.sort_values(['Human (mean)']).reset_index(drop=True)\n",
        "df['Rank 1'] = np.arange(len(df)) + 1\n",
        "\n",
        "# Sort by Cosine Similarity\n",
        "df = df.sort_values(['Cosine Similarity']).reset_index(drop=True)\n",
        "df['Rank 2'] = np.arange(len(df)) + 1\n",
        "\n",
        "# Compute d and d^2 where d = difference between ranks and d^2 = difference squared\n",
        "df['d'] = df['Rank 1'] - df['Rank 2']\n",
        "df['d^2'] = df['d'] ** 2\n",
        "\n",
        "# Reindex dataframe columns\n",
        "df = df.reindex(columns=['Word 1', 'Word 2', 'Human (mean)', 'Cosine Similarity', 'Rank 1', 'Rank 2', 'd', 'd^2'])\n",
        "\n",
        "df"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word 1</th>\n",
              "      <th>Word 2</th>\n",
              "      <th>Human (mean)</th>\n",
              "      <th>Cosine Similarity</th>\n",
              "      <th>Rank 1</th>\n",
              "      <th>Rank 2</th>\n",
              "      <th>d</th>\n",
              "      <th>d^2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>discovery</td>\n",
              "      <td>space</td>\n",
              "      <td>6.34</td>\n",
              "      <td>-0.175937</td>\n",
              "      <td>166</td>\n",
              "      <td>1</td>\n",
              "      <td>165</td>\n",
              "      <td>27225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>noon</td>\n",
              "      <td>string</td>\n",
              "      <td>0.54</td>\n",
              "      <td>-0.152691</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>size</td>\n",
              "      <td>prominence</td>\n",
              "      <td>5.31</td>\n",
              "      <td>-0.141904</td>\n",
              "      <td>116</td>\n",
              "      <td>3</td>\n",
              "      <td>113</td>\n",
              "      <td>12769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>king</td>\n",
              "      <td>cabbage</td>\n",
              "      <td>0.23</td>\n",
              "      <td>-0.127920</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>-3</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>planet</td>\n",
              "      <td>star</td>\n",
              "      <td>8.45</td>\n",
              "      <td>-0.124253</td>\n",
              "      <td>296</td>\n",
              "      <td>5</td>\n",
              "      <td>291</td>\n",
              "      <td>84681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>311</th>\n",
              "      <td>tiger</td>\n",
              "      <td>feline</td>\n",
              "      <td>8.00</td>\n",
              "      <td>0.198444</td>\n",
              "      <td>271</td>\n",
              "      <td>312</td>\n",
              "      <td>-41</td>\n",
              "      <td>1681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>midday</td>\n",
              "      <td>noon</td>\n",
              "      <td>9.29</td>\n",
              "      <td>0.206150</td>\n",
              "      <td>313</td>\n",
              "      <td>313</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313</th>\n",
              "      <td>man</td>\n",
              "      <td>woman</td>\n",
              "      <td>8.30</td>\n",
              "      <td>0.206269</td>\n",
              "      <td>287</td>\n",
              "      <td>314</td>\n",
              "      <td>-27</td>\n",
              "      <td>729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>deployment</td>\n",
              "      <td>withdrawal</td>\n",
              "      <td>5.88</td>\n",
              "      <td>0.220321</td>\n",
              "      <td>134</td>\n",
              "      <td>315</td>\n",
              "      <td>-181</td>\n",
              "      <td>32761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315</th>\n",
              "      <td>tiger</td>\n",
              "      <td>tiger</td>\n",
              "      <td>10.00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>316</td>\n",
              "      <td>316</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>316 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Word 1      Word 2  Human (mean)  ...  Rank 2    d    d^2\n",
              "0     discovery       space          6.34  ...       1  165  27225\n",
              "1          noon      string          0.54  ...       2    1      1\n",
              "2          size  prominence          5.31  ...       3  113  12769\n",
              "3          king     cabbage          0.23  ...       4   -3      9\n",
              "4        planet        star          8.45  ...       5  291  84681\n",
              "..          ...         ...           ...  ...     ...  ...    ...\n",
              "311       tiger      feline          8.00  ...     312  -41   1681\n",
              "312      midday        noon          9.29  ...     313    0      0\n",
              "313         man       woman          8.30  ...     314  -27    729\n",
              "314  deployment  withdrawal          5.88  ...     315 -181  32761\n",
              "315       tiger       tiger         10.00  ...     316    0      0\n",
              "\n",
              "[316 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6fLXVox_PEAF"
      },
      "source": [
        "#### Calculate Spearman correlation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MkQlg7VIPEAH"
      },
      "source": [
        "The Spearman correlation is a nonparametric measure of the monotonicity of the relationship between two datasets. Unlike the Pearson correlation, the Spearman correlation does not assume that both datasets are normally distributed. Like other correlation coefficients, this one varies between -1 and +1 with 0 implying no correlation. Correlations of -1 or +1 imply an exact monotonic relationship. Positive correlations imply that as x increases, so does y. Negative correlations imply that as x increases, y decreses.\n",
        "\n",
        "The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Spearman correlation at least as extreme as the one computed from these datasets. The p-values are not entirely reliable but are probably reasonable for datasets larger than 500 or so."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "71e0ad24-6ac1-41ad-987f-255626d15f7a",
        "id": "CAMBjBzyPEAI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "n = len(df)\n",
        "corr = 1 - (6 * df['d^2'].sum()) / (n * (n**2 - 1))\n",
        "\n",
        "print('The spearman correlation value is', corr)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The spearman correlation value is 0.0448573215973288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ylC1hHe3PEAM"
      },
      "source": [
        "#### Cross check answer with scipy.stats.spearmanr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "88a16d90-2322-4fbc-bf34-5322730656aa",
        "id": "wHF1ZorlPEAO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import scipy.stats as s\n",
        "\n",
        "spearman_corr, spearman_p  = s.spearmanr(df['Rank 1'], df['Rank 2'])\n",
        "\n",
        "print('The spearman correlation value is', spearman_corr)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The spearman correlation value is 0.04485732159732879\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_hmpGRR3PEAS"
      },
      "source": [
        "And we get the same result!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VOe0lD3DPEAU"
      },
      "source": [
        "#### The value being close to zero shows that the correlation between `Human (mean)` and `Cosine Similiarity` is __low__."
      ]
    }
  ]
}