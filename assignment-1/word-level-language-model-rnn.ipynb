{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "word-level-language-model-rnn.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alanwuha/ce7455-nlp/blob/master/assignment-1/word-level-language-model-rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIaIZE6es3jc",
        "colab_type": "text"
      },
      "source": [
        "# Question Two\n",
        "\n",
        "Please make sure to run all cells in order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXyEv8NOs-rl",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsyBbtWr0ZaX",
        "colab_type": "text"
      },
      "source": [
        "### Download wikitext-2 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oHSqqxz0c4Q",
        "colab_type": "code",
        "outputId": "fbcf5320-9438-4a04-a553-1018ff3ffcc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!rm -r data\n",
        "!mkdir data\n",
        "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip -P /content/data\n",
        "!unzip /content/data/wikitext-2-v1.zip -d /content/data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-09 02:57:49--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.145.37\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.145.37|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4475746 (4.3M) [application/zip]\n",
            "Saving to: ‘/content/data/wikitext-2-v1.zip’\n",
            "\n",
            "\rwikitext-2-v1.zip     0%[                    ]       0  --.-KB/s               \rwikitext-2-v1.zip     6%[>                   ] 262.64K  1.26MB/s               \rwikitext-2-v1.zip   100%[===================>]   4.27M  12.0MB/s    in 0.4s    \n",
            "\n",
            "2020-02-09 02:57:50 (12.0 MB/s) - ‘/content/data/wikitext-2-v1.zip’ saved [4475746/4475746]\n",
            "\n",
            "Archive:  /content/data/wikitext-2-v1.zip\n",
            "   creating: /content/data/wikitext-2/\n",
            "  inflating: /content/data/wikitext-2/wiki.test.tokens  \n",
            "  inflating: /content/data/wikitext-2/wiki.valid.tokens  \n",
            "  inflating: /content/data/wikitext-2/wiki.train.tokens  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5fcltKuqDoR",
        "colab_type": "text"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqiyNq_4qGkI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "c1ab03df-27cc-4b30-abe1-9d7e1eed3530"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.onnx\n",
        "import argparse\n",
        "import time\n",
        "import os\n",
        "from io import open"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-4485523effdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m __all__ += [name for name in dir(_C)\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             not name.endswith('Base')]\n",
            "\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCOTNafMg_z5",
        "colab_type": "text"
      },
      "source": [
        "### Specify arg variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrEgZw42hCv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = './data/wikitext-2'  # location of the data corpus\n",
        "args_model = 'FNN'          # type of recurrent net\n",
        "emsize = 200                # size of word embeddings\n",
        "nhid = 200                  # number of hidden units per layer\n",
        "nlayers = 2                 # number of layers\n",
        "lr = 20                     # initial learning rate\n",
        "clip = 0.25                 # gradient clipping\n",
        "epochs = 40                 # upper epoch limit\n",
        "batch_size = 20             # batch size\n",
        "bptt = 35                   # sequence length\n",
        "dropout = 0.2               # dropout applied to layers (0 = no dropout)\n",
        "tied = False                # tie the word embedding and softmax weights\n",
        "seed = 1111                 # random seed\n",
        "cuda = True                 # use CUDA\n",
        "log_interval = 200          # report interval\n",
        "save = 'model.pt'           # path to save the final model\n",
        "onnx_export = ''            # path to export the final model in onnx format\n",
        "nhead = 2                   # the number of heads in the encoder/decoder of the transformer model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M0KYJXn0htA",
        "colab_type": "text"
      },
      "source": [
        "### Data Preprocessing Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_krRf2j0kzQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'wiki.train.tokens'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'wiki.valid.tokens'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'wiki.test.tokens'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            idss = []\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                ids = []\n",
        "                for word in words:\n",
        "                    ids.append(self.dictionary.word2idx[word])\n",
        "                idss.append(torch.tensor(ids).type(torch.int64))\n",
        "            ids = torch.cat(idss)\n",
        "\n",
        "        return ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNA9aDE-0VhB",
        "colab_type": "text"
      },
      "source": [
        "### Helper Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVk0qOGJo8dg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "\n",
        "# Starting from sequential data, batchify arranges the dataset into columns.\n",
        "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
        "# ┌ a g m s ┐\n",
        "# │ b h n t │\n",
        "# │ c i o u │\n",
        "# │ d j p v │\n",
        "# │ e k q w │\n",
        "# └ f l r x ┘.\n",
        "# These columns are treated as independent by the model, which means that the\n",
        "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
        "# batch processing.\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "\n",
        "# get_batch subdivides the source data into chunks of length bptt.\n",
        "# If source is equal to the example output of the batchify function, with\n",
        "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
        "# ┌ a g m s ┐ ┌ b h n t ┐\n",
        "# └ b h n t ┘ └ c i o u ┘\n",
        "# Note that despite the name of the function, the subdivison of data is not\n",
        "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
        "# by the batchify function. The chunks are along dimension 0, corresponding\n",
        "# to the seq_len dimension in the LSTM.\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    if args_model not in ['Transformer', 'FNN']:\n",
        "        hidden = model.init_hidden(eval_batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            if args_model in ['Transformer', 'FNN']:\n",
        "                output = model(data)\n",
        "            else:\n",
        "                output, hidden = model(data, hidden)\n",
        "                hidden = repackage_hidden(hidden)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    if args_model not in ['Transformer', 'FNN']:\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        # model.zero_grad()\n",
        "        \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output\n",
        "        if args_model in ['Transformer', 'FNN']:\n",
        "            output = model(data)\n",
        "        else:\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            output, hidden = model(data, hidden)\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        # for p in model.parameters():\n",
        "        #     p.data.add_(-lr, p.grad.data)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.3f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // bptt, optimizer.param_groups[0]['lr'],\n",
        "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "\n",
        "def export_onnx(path, batch_size, seq_len):\n",
        "    print('The model is also exported in ONNX format at {}'.\n",
        "          format(os.path.realpath(args.onnx_export)))\n",
        "    model.eval()\n",
        "    dummy_input = torch.LongTensor(seq_len * batch_size).zero_().view(-1, batch_size).to(device)\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    torch.onnx.export(model, (dummy_input, hidden), path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-w17AZJp2qf",
        "colab_type": "text"
      },
      "source": [
        "### Set torch seed and CUDA device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJGy7JGHqe8K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "45fd6ef0-6a2d-4aec-850f-a3850bfa6a00"
      },
      "source": [
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Set cuda device\n",
        "if torch.cuda.is_available():\n",
        "    if not cuda:\n",
        "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-8384caf08dc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Set cuda device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GY7McFzqnaR",
        "colab_type": "text"
      },
      "source": [
        "### Load corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig6BmYbWqq0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = Corpus(data)\n",
        "\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(corpus.train, batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "ntokens = len(corpus.dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSxxbklN0moQ",
        "colab_type": "text"
      },
      "source": [
        "## (iii) Write a _class FNNModel(nn.Module)_.\n",
        "\n",
        "The FNNModel should implement a language model with a feed-forward network architecture. It has a hidden layer with tanh architecture and the output layer is a Softmax layer. The output of the model for each input of (n-1) previous word indices are the probabilities of the |_V_| words in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSAjXOz71l80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FNNModel(nn.Module):\n",
        "    \"\"\" Container module with an encoder, a feed forward module, and a decoder. \"\"\"\n",
        "\n",
        "    def __init__(self, ntokens, emsize, hidden_size, tie_weights=False):\n",
        "        super(FNNModel, self).__init__()\n",
        "        self.encoder = nn.Embedding(ntokens, emsize)\n",
        "        self.hidden = nn.Linear(emsize, hidden_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.decoder = nn.Linear(hidden_size, ntokens)\n",
        "\n",
        "        if tie_weights:\n",
        "            if hidden_size != emsize:\n",
        "                raise ValueError('When using the tied flag, hidden_size must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        # self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input):\n",
        "        emb = self.encoder(input)\n",
        "        hid = self.hidden(emb)\n",
        "        output = self.tanh(hid)\n",
        "        decoded = self.decoder(output)\n",
        "        return decoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvvSa_5Sqtyp",
        "colab_type": "text"
      },
      "source": [
        "## (iv-1) Train the model with Adam optimizer (without sharing weights)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipzOYeQy0W0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tied = False\n",
        "save = 'model_untied.pt'\n",
        "model = FNNModel(ntokens, emsize, nhid, tied).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "# Loop over epochs.\n",
        "best_val_loss = None\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIGOGKJAoC1j",
        "colab_type": "text"
      },
      "source": [
        "## (v-1) Show the perplexity score on the test set. You should select your best model based on the _valid_ set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v-ZA9MVoMsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the best saved model.\n",
        "save = 'model_untied.pt'\n",
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac5xAGvBrtsR",
        "colab_type": "text"
      },
      "source": [
        "## (iv-2) Train the model with Adam optimizer, but now with sharing the input (look-up matrix) and output layer embeddings (final layer weights)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI3kSSpfrxxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tied = True\n",
        "save = 'model_tied.pt'\n",
        "model = FNNModel(ntokens, emsize, nhid, tied).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "# Loop over epochs.\n",
        "best_val_loss = None\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giHtSc80sCcZ",
        "colab_type": "text"
      },
      "source": [
        "## (v-2) Show the perplexity score on the test set. You should select your best model based on the _valid_ set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4cIbY-8r7TB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the best saved model.\n",
        "save = 'model_tied.pt'\n",
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1C6Z9PgoQox",
        "colab_type": "text"
      },
      "source": [
        "## (vii) Adapt generate.py so that you can generate texts using your language model (FNNModel)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZzhYEfIoWtw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = './data/wikitext-2'          # location of the data corpus\n",
        "checkpoint = './model_untied.pt'    # model checkpoint to use\n",
        "outf = 'generated.txt'              # output file for generated text\n",
        "words = 1000                        # number of words to generate\n",
        "seed = 1111                         # random seed\n",
        "cuda = True                         # use CUDA\n",
        "temperature = 1.0                   # temperature - higher will increase diversity\n",
        "log_interval = 100                  # reporting interval\n",
        "\n",
        "with open(checkpoint, 'rb') as f:\n",
        "  model = torch.load(f).to(device)\n",
        "model.eval()\n",
        "\n",
        "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "with open(outf, 'w') as outf:\n",
        "  with torch.no_grad(): # no tracking history\n",
        "    for i in range(words):\n",
        "      output = model(input)\n",
        "      word_weights = output.squeeze().div(temperature).exp().cpu()\n",
        "      word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "      input.fill_(word_idx)\n",
        "\n",
        "      word = corpus.dictionary.idx2word[word_idx]\n",
        "\n",
        "      outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "\n",
        "      if i % log_interval == 0:\n",
        "        print('| Generated {}/{} words'.format(i, words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eubAEEWVs5Ei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat generated.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoOAfzCmuloS",
        "colab_type": "text"
      },
      "source": [
        "## (viii) In your opinion, which computation/operation is the most expensive one in inference or forward pass? Can you think of ways to improve this? Is yes, please mention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo3Vq9kxuyvi",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fQyIgCyuzlu",
        "colab_type": "text"
      },
      "source": [
        "## (ix) Report the Spearman correlation for the input embeddings.\n",
        "\n",
        "Notice that the model also learns word vectors (input and output layer embeddings) as a byproduct. One way to evaluate the trained word vectors is to __measure the cosine similarity__ between pairs of words, and then __report the correlation with the similarity scores given by humans__. For this exercise, use the dataset available [here](http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/) and report the Spearman correlation for the input embeddings. Exclude any pair if it is not in the embedding matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dvb7hvy2Ho0",
        "colab_type": "text"
      },
      "source": [
        "### Download wordsimilarity-353 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn7YJto02Ye-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/wordsim353.zip -P /content/data\n",
        "!unzip /content/data/wordsim353.zip -d /content/data/wordsim353"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o28NJdtjLQEx",
        "colab_type": "text"
      },
      "source": [
        "### Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGtEtrWtLQC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cpu')\n",
        "checkpoint = 'model_untied.pt'\n",
        "with open(checkpoint, 'rb') as f:\n",
        "  model = torch.load(f)\n",
        "  model = model.to(device)\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWXrKqSLKHj4",
        "colab_type": "text"
      },
      "source": [
        "### Prepare Spearman rank-order correlation data table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM0vVQfNHgbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Read csv\n",
        "df = pd.read_csv('/content/data/wordsim353/combined.csv')\n",
        "\n",
        "# Get word indices for Word 1\n",
        "word_1 = df['Word 1']\n",
        "df['Indices 1'] = word_1.apply(lambda x: corpus.dictionary.word2idx[x] if x in corpus.dictionary.word2idx else None)\n",
        "\n",
        "# Get word indices for Word 2\n",
        "word_2 = df['Word 2']\n",
        "df['Indices 2'] = word_2.apply(lambda x: corpus.dictionary.word2idx[x] if x in corpus.dictionary.word2idx else None)\n",
        "\n",
        "# Drop rows with nan values and reset index\n",
        "df = df.dropna().reset_index(drop=True)\n",
        "\n",
        "# Set indices to integer data type\n",
        "df['Indices 1'] = df['Indices 1'].astype('int')\n",
        "df['Indices 2'] = df['Indices 2'].astype('int')\n",
        "\n",
        "# Get word embeddings from trained model\n",
        "embedding_1 = df['Indices 1'].apply(lambda x: model.encoder.weight[x].detach().numpy())\n",
        "embedding_2 = df['Indices 2'].apply(lambda x: model.encoder.weight[x].detach().numpy())\n",
        "\n",
        "# Compute cosine similarity from word embeddings\n",
        "cosine_similarity = []\n",
        "for t1, t2 in zip(*(embedding_1, embedding_2)):\n",
        "  cos_sim = t1.dot(t2) / (norm(t1) * norm(t2))\n",
        "  cosine_similarity.append(cos_sim)\n",
        "df['Cosine Similarity'] = pd.Series(cosine_similarity)\n",
        "\n",
        "# Sort by Human (mean) and assign rank values\n",
        "df = df.sort_values(['Human (mean)']).reset_index(drop=True)\n",
        "df['Rank 1'] = np.arange(len(df)) + 1\n",
        "\n",
        "# Sort by Cosine Similarity\n",
        "df = df.sort_values(['Cosine Similarity']).reset_index(drop=True)\n",
        "df['Rank 2'] = np.arange(len(df)) + 1\n",
        "\n",
        "# Compute d and d^2 where d = difference between ranks and d^2 = difference squared\n",
        "df['d'] = df['Rank 1'] - df['Rank 2']\n",
        "df['d^2'] = df['d'] ** 2\n",
        "\n",
        "# Reindex dataframe columns\n",
        "df = df.reindex(columns=['Word 1', 'Word 2', 'Human (mean)', 'Cosine Similarity', 'Rank 1', 'Rank 2', 'd', 'd^2'])\n",
        "\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC7iUP99drQg",
        "colab_type": "text"
      },
      "source": [
        "### Calculate Spearman correlation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFUp7pJ9pSeR",
        "colab_type": "text"
      },
      "source": [
        "The Spearman correlation is a nonparametric measure of the monotonicity of the relationship between two datasets. Unlike the Pearson correlation, the Spearman correlation does not assume that both datasets are normally distributed. Like other correlation coefficients, this one varies between -1 and +1 with 0 implying no correlation. Correlations of -1 or +1 imply an exact monotonic relationship. Positive correlations imply that as x increases, so does y. Negative correlations imply that as x increases, y decreses.\n",
        "\n",
        "The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Spearman correlation at least as extreme as the one computed from these datasets. The p-values are not entirely reliable but are probably reasonable for datasets larger than 500 or so."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVE_fXzDlJ1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = len(df)\n",
        "corr = 1 - (6 * df['d^2'].sum()) / (n * (n**2 - 1))\n",
        "\n",
        "print('The spearman correlation value is', corr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT5127Cjre97",
        "colab_type": "text"
      },
      "source": [
        "The value being close to zero shows that the correlation between `Human (mean)` and `Cosine Similiarity` is __low__."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpKHCUVaoxmR",
        "colab_type": "text"
      },
      "source": [
        "### Cross check answer with scipy.stats.spearmanr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scBlJ7OzmWuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.stats as s\n",
        "\n",
        "spearman_corr, spearman_p  = s.spearmanr(df['Rank 1'], df['Rank 2'])\n",
        "\n",
        "print('The spearman correlation value is', spearman_corr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4h_gCNstvfy",
        "colab_type": "text"
      },
      "source": [
        "And we get the same result!"
      ]
    }
  ]
}