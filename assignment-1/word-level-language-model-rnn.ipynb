{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "word-level-language-model-rnn.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alanwuha/ce7455-nlp/blob/master/assignment-1/word-level-language-model-rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIaIZE6es3jc",
        "colab_type": "text"
      },
      "source": [
        "# Question Two\n",
        "\n",
        "Please make sure to run all cells in order."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXyEv8NOs-rl",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsyBbtWr0ZaX",
        "colab_type": "text"
      },
      "source": [
        "### Download wikitext-2 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oHSqqxz0c4Q",
        "colab_type": "code",
        "outputId": "0424463f-a605-458d-ae77-38d1c53349e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!rm -r data\n",
        "!mkdir data\n",
        "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip -P /content/data\n",
        "!unzip /content/data/wikitext-2-v1.zip -d /content/data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'data': No such file or directory\n",
            "--2020-02-08 07:30:51--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.36.254\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.36.254|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4475746 (4.3M) [application/zip]\n",
            "Saving to: ‘/content/data/wikitext-2-v1.zip’\n",
            "\n",
            "wikitext-2-v1.zip   100%[===================>]   4.27M  2.47MB/s    in 1.7s    \n",
            "\n",
            "2020-02-08 07:30:59 (2.47 MB/s) - ‘/content/data/wikitext-2-v1.zip’ saved [4475746/4475746]\n",
            "\n",
            "Archive:  /content/data/wikitext-2-v1.zip\n",
            "   creating: /content/data/wikitext-2/\n",
            "  inflating: /content/data/wikitext-2/wiki.test.tokens  \n",
            "  inflating: /content/data/wikitext-2/wiki.valid.tokens  \n",
            "  inflating: /content/data/wikitext-2/wiki.train.tokens  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5fcltKuqDoR",
        "colab_type": "text"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqiyNq_4qGkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.onnx\n",
        "import argparse\n",
        "import time\n",
        "import os\n",
        "from io import open"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCOTNafMg_z5",
        "colab_type": "text"
      },
      "source": [
        "### Specify arg variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrEgZw42hCv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = './data/wikitext-2'  # location of the data corpus\n",
        "args_model = 'FNN'          # type of recurrent net\n",
        "emsize = 200                # size of word embeddings\n",
        "nhid = 200                  # number of hidden units per layer\n",
        "nlayers = 2                 # number of layers\n",
        "lr = 20                     # initial learning rate\n",
        "clip = 0.25                 # gradient clipping\n",
        "epochs = 40                 # upper epoch limit\n",
        "batch_size = 20             # batch size\n",
        "bptt = 35                   # sequence length\n",
        "dropout = 0.2               # dropout applied to layers (0 = no dropout)\n",
        "tied = False                # tie the word embedding and softmax weights\n",
        "seed = 1111                 # random seed\n",
        "cuda = True                 # use CUDA\n",
        "log_interval = 200          # report interval\n",
        "save = 'model.pt'           # path to save the final model\n",
        "onnx_export = ''            # path to export the final model in onnx format\n",
        "nhead = 2                   # the number of heads in the encoder/decoder of the transformer model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M0KYJXn0htA",
        "colab_type": "text"
      },
      "source": [
        "### Data Preprocessing Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_krRf2j0kzQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'wiki.train.tokens'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'wiki.valid.tokens'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'wiki.test.tokens'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            idss = []\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                ids = []\n",
        "                for word in words:\n",
        "                    ids.append(self.dictionary.word2idx[word])\n",
        "                idss.append(torch.tensor(ids).type(torch.int64))\n",
        "            ids = torch.cat(idss)\n",
        "\n",
        "        return ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNA9aDE-0VhB",
        "colab_type": "text"
      },
      "source": [
        "### Helper Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVk0qOGJo8dg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "\n",
        "# Starting from sequential data, batchify arranges the dataset into columns.\n",
        "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
        "# ┌ a g m s ┐\n",
        "# │ b h n t │\n",
        "# │ c i o u │\n",
        "# │ d j p v │\n",
        "# │ e k q w │\n",
        "# └ f l r x ┘.\n",
        "# These columns are treated as independent by the model, which means that the\n",
        "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
        "# batch processing.\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "\n",
        "# get_batch subdivides the source data into chunks of length bptt.\n",
        "# If source is equal to the example output of the batchify function, with\n",
        "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
        "# ┌ a g m s ┐ ┌ b h n t ┐\n",
        "# └ b h n t ┘ └ c i o u ┘\n",
        "# Note that despite the name of the function, the subdivison of data is not\n",
        "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
        "# by the batchify function. The chunks are along dimension 0, corresponding\n",
        "# to the seq_len dimension in the LSTM.\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    if args_model not in ['Transformer', 'FNN']:\n",
        "        hidden = model.init_hidden(eval_batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            if args_model in ['Transformer', 'FNN']:\n",
        "                output = model(data)\n",
        "            else:\n",
        "                output, hidden = model(data, hidden)\n",
        "                hidden = repackage_hidden(hidden)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    if args_model not in ['Transformer', 'FNN']:\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        # model.zero_grad()\n",
        "        \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output\n",
        "        if args_model in ['Transformer', 'FNN']:\n",
        "            output = model(data)\n",
        "        else:\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            output, hidden = model(data, hidden)\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        # for p in model.parameters():\n",
        "        #     p.data.add_(-lr, p.grad.data)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.3f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // bptt, optimizer.param_groups[0]['lr'],\n",
        "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "\n",
        "def export_onnx(path, batch_size, seq_len):\n",
        "    print('The model is also exported in ONNX format at {}'.\n",
        "          format(os.path.realpath(args.onnx_export)))\n",
        "    model.eval()\n",
        "    dummy_input = torch.LongTensor(seq_len * batch_size).zero_().view(-1, batch_size).to(device)\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    torch.onnx.export(model, (dummy_input, hidden), path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-w17AZJp2qf",
        "colab_type": "text"
      },
      "source": [
        "### Set torch seed and CUDA device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJGy7JGHqe8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Set cuda device\n",
        "if torch.cuda.is_available():\n",
        "    if not cuda:\n",
        "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
        "\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GY7McFzqnaR",
        "colab_type": "text"
      },
      "source": [
        "### Load corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig6BmYbWqq0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = Corpus(data)\n",
        "\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(corpus.train, batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "ntokens = len(corpus.dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSxxbklN0moQ",
        "colab_type": "text"
      },
      "source": [
        "## (iii) Write a _class FNNModel(nn.Module)_.\n",
        "\n",
        "The FNNModel should implement a language model with a feed-forward network architecture. It has a hidden layer with tanh architecture and the output layer is a Softmax layer. The output of the model for each input of (n-1) previous word indices are the probabilities of the |_V_| words in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSAjXOz71l80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FNNModel(nn.Module):\n",
        "    \"\"\" Container module with an encoder, a feed forward module, and a decoder. \"\"\"\n",
        "\n",
        "    def __init__(self, ntokens, emsize, hidden_size, tie_weights=False):\n",
        "        super(FNNModel, self).__init__()\n",
        "        self.encoder = nn.Embedding(ntokens, emsize)\n",
        "        self.hidden = nn.Linear(emsize, hidden_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.decoder = nn.Linear(hidden_size, ntokens)\n",
        "\n",
        "        if tie_weights:\n",
        "            if hidden_size != emsize:\n",
        "                raise ValueError('When using the tied flag, hidden_size must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        # self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input):\n",
        "        emb = self.encoder(input)\n",
        "        hid = self.hidden(emb)\n",
        "        output = self.tanh(hid)\n",
        "        decoded = self.decoder(output)\n",
        "        return decoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvvSa_5Sqtyp",
        "colab_type": "text"
      },
      "source": [
        "## (iv-1) Train the model with Adam optimizer (without sharing weights)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipzOYeQy0W0a",
        "colab_type": "code",
        "outputId": "6d751783-290f-4fe5-d96f-c569a3623011",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "tied = False\n",
        "model = FNNModel(ntokens, emsize, nhid, tied).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "# Loop over epochs.\n",
        "best_val_loss = None\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 2983 batches | lr 0.001 | ms/batch 18.97 | loss  8.08 | ppl  3234.78\n",
            "| epoch   1 |   400/ 2983 batches | lr 0.001 | ms/batch 17.86 | loss  6.59 | ppl   728.55\n",
            "| epoch   1 |   600/ 2983 batches | lr 0.001 | ms/batch 17.97 | loss  6.38 | ppl   592.88\n",
            "| epoch   1 |   800/ 2983 batches | lr 0.001 | ms/batch 18.02 | loss  6.31 | ppl   549.91\n",
            "| epoch   1 |  1000/ 2983 batches | lr 0.001 | ms/batch 18.07 | loss  6.23 | ppl   507.14\n",
            "| epoch   1 |  1200/ 2983 batches | lr 0.001 | ms/batch 18.15 | loss  6.22 | ppl   503.70\n",
            "| epoch   1 |  1400/ 2983 batches | lr 0.001 | ms/batch 18.17 | loss  6.20 | ppl   490.41\n",
            "| epoch   1 |  1600/ 2983 batches | lr 0.001 | ms/batch 18.27 | loss  6.18 | ppl   485.13\n",
            "| epoch   1 |  1800/ 2983 batches | lr 0.001 | ms/batch 18.32 | loss  6.07 | ppl   430.87\n",
            "| epoch   1 |  2000/ 2983 batches | lr 0.001 | ms/batch 18.39 | loss  6.10 | ppl   444.95\n",
            "| epoch   1 |  2200/ 2983 batches | lr 0.001 | ms/batch 18.42 | loss  6.00 | ppl   401.94\n",
            "| epoch   1 |  2400/ 2983 batches | lr 0.001 | ms/batch 18.46 | loss  6.03 | ppl   414.26\n",
            "| epoch   1 |  2600/ 2983 batches | lr 0.001 | ms/batch 18.52 | loss  6.01 | ppl   408.67\n",
            "| epoch   1 |  2800/ 2983 batches | lr 0.001 | ms/batch 18.56 | loss  5.95 | ppl   382.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 56.64s | valid loss  5.89 | valid ppl   361.36\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type FNNModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   2 |   200/ 2983 batches | lr 0.001 | ms/batch 18.86 | loss  5.73 | ppl   307.82\n",
            "| epoch   2 |   400/ 2983 batches | lr 0.001 | ms/batch 18.96 | loss  5.68 | ppl   292.37\n",
            "| epoch   2 |   600/ 2983 batches | lr 0.001 | ms/batch 18.87 | loss  5.56 | ppl   258.84\n",
            "| epoch   2 |   800/ 2983 batches | lr 0.001 | ms/batch 18.89 | loss  5.59 | ppl   268.52\n",
            "| epoch   2 |  1000/ 2983 batches | lr 0.001 | ms/batch 18.92 | loss  5.56 | ppl   259.41\n",
            "| epoch   2 |  1200/ 2983 batches | lr 0.001 | ms/batch 18.92 | loss  5.58 | ppl   264.33\n",
            "| epoch   2 |  1400/ 2983 batches | lr 0.001 | ms/batch 19.29 | loss  5.61 | ppl   272.79\n",
            "| epoch   2 |  1600/ 2983 batches | lr 0.001 | ms/batch 19.76 | loss  5.61 | ppl   271.92\n",
            "| epoch   2 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.16 | loss  5.52 | ppl   250.46\n",
            "| epoch   2 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  5.58 | ppl   264.73\n",
            "| epoch   2 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.48 | ppl   239.13\n",
            "| epoch   2 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.51 | ppl   247.68\n",
            "| epoch   2 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.53 | ppl   251.63\n",
            "| epoch   2 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  5.48 | ppl   239.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 60.61s | valid loss  5.83 | valid ppl   338.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2983 batches | lr 0.001 | ms/batch 19.93 | loss  5.41 | ppl   223.24\n",
            "| epoch   3 |   400/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  5.38 | ppl   216.74\n",
            "| epoch   3 |   600/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  5.27 | ppl   193.82\n",
            "| epoch   3 |   800/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  5.32 | ppl   205.20\n",
            "| epoch   3 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  5.30 | ppl   200.52\n",
            "| epoch   3 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  5.32 | ppl   204.58\n",
            "| epoch   3 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  5.36 | ppl   212.01\n",
            "| epoch   3 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  5.36 | ppl   212.31\n",
            "| epoch   3 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  5.29 | ppl   198.04\n",
            "| epoch   3 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.18 | loss  5.34 | ppl   208.98\n",
            "| epoch   3 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  5.24 | ppl   188.86\n",
            "| epoch   3 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  5.28 | ppl   195.56\n",
            "| epoch   3 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.30 | ppl   200.25\n",
            "| epoch   3 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  5.25 | ppl   190.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 62.37s | valid loss  5.83 | valid ppl   341.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.22 | ppl   185.65\n",
            "| epoch   4 |   400/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  5.21 | ppl   183.06\n",
            "| epoch   4 |   600/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  5.10 | ppl   164.46\n",
            "| epoch   4 |   800/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  5.16 | ppl   174.76\n",
            "| epoch   4 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  5.15 | ppl   172.37\n",
            "| epoch   4 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  5.17 | ppl   175.62\n",
            "| epoch   4 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.23 | loss  5.20 | ppl   181.77\n",
            "| epoch   4 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.21 | ppl   182.61\n",
            "| epoch   4 |  1800/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  5.15 | ppl   172.12\n",
            "| epoch   4 |  2000/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  5.20 | ppl   180.78\n",
            "| epoch   4 |  2200/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  5.10 | ppl   163.62\n",
            "| epoch   4 |  2400/ 2983 batches | lr 0.001 | ms/batch 20.21 | loss  5.13 | ppl   169.10\n",
            "| epoch   4 |  2600/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  5.16 | ppl   174.01\n",
            "| epoch   4 |  2800/ 2983 batches | lr 0.001 | ms/batch 20.20 | loss  5.11 | ppl   165.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 62.46s | valid loss  5.86 | valid ppl   349.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/ 2983 batches | lr 0.001 | ms/batch 20.26 | loss  5.10 | ppl   164.76\n",
            "| epoch   5 |   400/ 2983 batches | lr 0.001 | ms/batch 20.19 | loss  5.10 | ppl   163.65\n",
            "| epoch   5 |   600/ 2983 batches | lr 0.001 | ms/batch 20.23 | loss  4.99 | ppl   147.61\n",
            "| epoch   5 |   800/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  5.06 | ppl   156.90\n",
            "| epoch   5 |  1000/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  5.05 | ppl   155.55\n",
            "| epoch   5 |  1200/ 2983 batches | lr 0.001 | ms/batch 20.23 | loss  5.07 | ppl   158.41\n",
            "| epoch   5 |  1400/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  5.10 | ppl   163.67\n",
            "| epoch   5 |  1600/ 2983 batches | lr 0.001 | ms/batch 20.22 | loss  5.10 | ppl   164.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "Exiting from training early\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIGOGKJAoC1j",
        "colab_type": "text"
      },
      "source": [
        "## (v-1) Show the perplexity score on the test set. You should select your best model based on the _valid_ set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v-ZA9MVoMsl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0b6f3a42-e6bd-4085-f589-daf220734858"
      },
      "source": [
        "# Load the best saved model.\n",
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  5.73 | test ppl   308.45\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac5xAGvBrtsR",
        "colab_type": "text"
      },
      "source": [
        "## (iv-2) Train the model with Adam optimizer, but now with sharing the input (look-up matrix) and output layer embeddings (final layer weights)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI3kSSpfrxxK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5c2fd9dd-d7f3-4c93-f693-d7e67ff31dca"
      },
      "source": [
        "tied = True\n",
        "model = FNNModel(ntokens, emsize, nhid, tied).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "# Loop over epochs.\n",
        "best_val_loss = None\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 2983 batches | lr 0.001 | ms/batch 17.35 | loss 14.22 | ppl 1502783.63\n",
            "| epoch   1 |   400/ 2983 batches | lr 0.001 | ms/batch 17.25 | loss  8.03 | ppl  3073.87\n",
            "| epoch   1 |   600/ 2983 batches | lr 0.001 | ms/batch 17.44 | loss  7.31 | ppl  1488.76\n",
            "| epoch   1 |   800/ 2983 batches | lr 0.001 | ms/batch 17.68 | loss  7.13 | ppl  1243.80\n",
            "| epoch   1 |  1000/ 2983 batches | lr 0.001 | ms/batch 17.86 | loss  7.00 | ppl  1101.65\n",
            "| epoch   1 |  1200/ 2983 batches | lr 0.001 | ms/batch 18.14 | loss  6.97 | ppl  1067.11\n",
            "| epoch   1 |  1400/ 2983 batches | lr 0.001 | ms/batch 18.32 | loss  6.91 | ppl  1003.37\n",
            "| epoch   1 |  1600/ 2983 batches | lr 0.001 | ms/batch 18.61 | loss  6.89 | ppl   986.13\n",
            "| epoch   1 |  1800/ 2983 batches | lr 0.001 | ms/batch 18.84 | loss  6.79 | ppl   885.45\n",
            "| epoch   1 |  2000/ 2983 batches | lr 0.001 | ms/batch 18.79 | loss  6.80 | ppl   899.18\n",
            "| epoch   1 |  2200/ 2983 batches | lr 0.001 | ms/batch 18.40 | loss  6.71 | ppl   820.16\n",
            "| epoch   1 |  2400/ 2983 batches | lr 0.001 | ms/batch 18.11 | loss  6.71 | ppl   821.80\n",
            "| epoch   1 |  2600/ 2983 batches | lr 0.001 | ms/batch 17.98 | loss  6.70 | ppl   809.18\n",
            "| epoch   1 |  2800/ 2983 batches | lr 0.001 | ms/batch 17.81 | loss  6.62 | ppl   751.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 55.94s | valid loss  6.53 | valid ppl   688.00\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type FNNModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   2 |   200/ 2983 batches | lr 0.001 | ms/batch 17.51 | loss  6.54 | ppl   692.33\n",
            "| epoch   2 |   400/ 2983 batches | lr 0.001 | ms/batch 17.42 | loss  6.47 | ppl   644.98\n",
            "| epoch   2 |   600/ 2983 batches | lr 0.001 | ms/batch 17.42 | loss  6.36 | ppl   579.72\n",
            "| epoch   2 |   800/ 2983 batches | lr 0.001 | ms/batch 17.32 | loss  6.40 | ppl   599.72\n",
            "| epoch   2 |  1000/ 2983 batches | lr 0.001 | ms/batch 17.34 | loss  6.36 | ppl   576.65\n",
            "| epoch   2 |  1200/ 2983 batches | lr 0.001 | ms/batch 17.29 | loss  6.38 | ppl   588.80\n",
            "| epoch   2 |  1400/ 2983 batches | lr 0.001 | ms/batch 17.33 | loss  6.38 | ppl   589.49\n",
            "| epoch   2 |  1600/ 2983 batches | lr 0.001 | ms/batch 17.31 | loss  6.38 | ppl   590.09\n",
            "| epoch   2 |  1800/ 2983 batches | lr 0.001 | ms/batch 17.41 | loss  6.28 | ppl   532.73\n",
            "| epoch   2 |  2000/ 2983 batches | lr 0.001 | ms/batch 17.47 | loss  6.32 | ppl   553.87\n",
            "| epoch   2 |  2200/ 2983 batches | lr 0.001 | ms/batch 17.52 | loss  6.23 | ppl   507.36\n",
            "| epoch   2 |  2400/ 2983 batches | lr 0.001 | ms/batch 17.55 | loss  6.25 | ppl   519.89\n",
            "| epoch   2 |  2600/ 2983 batches | lr 0.001 | ms/batch 17.59 | loss  6.25 | ppl   518.43\n",
            "| epoch   2 |  2800/ 2983 batches | lr 0.001 | ms/batch 17.71 | loss  6.19 | ppl   490.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 54.30s | valid loss  6.26 | valid ppl   524.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2983 batches | lr 0.001 | ms/batch 17.74 | loss  6.21 | ppl   500.05\n",
            "| epoch   3 |   400/ 2983 batches | lr 0.001 | ms/batch 17.86 | loss  6.14 | ppl   464.87\n",
            "| epoch   3 |   600/ 2983 batches | lr 0.001 | ms/batch 17.79 | loss  6.04 | ppl   417.95\n",
            "| epoch   3 |   800/ 2983 batches | lr 0.001 | ms/batch 17.81 | loss  6.08 | ppl   439.21\n",
            "| epoch   3 |  1000/ 2983 batches | lr 0.001 | ms/batch 17.79 | loss  6.06 | ppl   426.59\n",
            "| epoch   3 |  1200/ 2983 batches | lr 0.001 | ms/batch 17.75 | loss  6.08 | ppl   438.70\n",
            "| epoch   3 |  1400/ 2983 batches | lr 0.001 | ms/batch 17.75 | loss  6.11 | ppl   448.97\n",
            "| epoch   3 |  1600/ 2983 batches | lr 0.001 | ms/batch 17.64 | loss  6.11 | ppl   450.33\n",
            "| epoch   3 |  1800/ 2983 batches | lr 0.001 | ms/batch 17.70 | loss  6.02 | ppl   411.08\n",
            "| epoch   3 |  2000/ 2983 batches | lr 0.001 | ms/batch 17.66 | loss  6.07 | ppl   430.78\n",
            "| epoch   3 |  2200/ 2983 batches | lr 0.001 | ms/batch 17.61 | loss  5.98 | ppl   394.87\n",
            "| epoch   3 |  2400/ 2983 batches | lr 0.001 | ms/batch 17.57 | loss  6.01 | ppl   407.36\n",
            "| epoch   3 |  2600/ 2983 batches | lr 0.001 | ms/batch 17.54 | loss  6.02 | ppl   410.71\n",
            "| epoch   3 |  2800/ 2983 batches | lr 0.001 | ms/batch 17.56 | loss  5.96 | ppl   389.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 54.94s | valid loss  6.15 | valid ppl   470.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/ 2983 batches | lr 0.001 | ms/batch 17.63 | loss  6.01 | ppl   407.65\n",
            "| epoch   4 |   400/ 2983 batches | lr 0.001 | ms/batch 17.58 | loss  5.95 | ppl   382.05\n",
            "| epoch   4 |   600/ 2983 batches | lr 0.001 | ms/batch 17.60 | loss  5.84 | ppl   343.48\n",
            "| epoch   4 |   800/ 2983 batches | lr 0.001 | ms/batch 17.61 | loss  5.90 | ppl   363.78\n",
            "| epoch   4 |  1000/ 2983 batches | lr 0.001 | ms/batch 17.63 | loss  5.87 | ppl   355.25\n",
            "| epoch   4 |  1200/ 2983 batches | lr 0.001 | ms/batch 17.65 | loss  5.90 | ppl   365.32\n",
            "| epoch   4 |  1400/ 2983 batches | lr 0.001 | ms/batch 17.64 | loss  5.93 | ppl   377.54\n",
            "| epoch   4 |  1600/ 2983 batches | lr 0.001 | ms/batch 17.72 | loss  5.94 | ppl   378.85\n",
            "| epoch   4 |  1800/ 2983 batches | lr 0.001 | ms/batch 17.69 | loss  5.85 | ppl   348.04\n",
            "| epoch   4 |  2000/ 2983 batches | lr 0.001 | ms/batch 17.68 | loss  5.90 | ppl   364.95\n",
            "| epoch   4 |  2200/ 2983 batches | lr 0.001 | ms/batch 17.71 | loss  5.81 | ppl   334.45\n",
            "| epoch   4 |  2400/ 2983 batches | lr 0.001 | ms/batch 17.76 | loss  5.85 | ppl   346.18\n",
            "| epoch   4 |  2600/ 2983 batches | lr 0.001 | ms/batch 17.75 | loss  5.86 | ppl   351.65\n",
            "| epoch   4 |  2800/ 2983 batches | lr 0.001 | ms/batch 17.75 | loss  5.81 | ppl   332.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 54.94s | valid loss  6.09 | valid ppl   442.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/ 2983 batches | lr 0.001 | ms/batch 17.73 | loss  5.86 | ppl   351.16\n",
            "| epoch   5 |   400/ 2983 batches | lr 0.001 | ms/batch 17.81 | loss  5.80 | ppl   331.85\n",
            "| epoch   5 |   600/ 2983 batches | lr 0.001 | ms/batch 17.75 | loss  5.70 | ppl   298.27\n",
            "| epoch   5 |   800/ 2983 batches | lr 0.001 | ms/batch 17.78 | loss  5.76 | ppl   316.89\n",
            "| epoch   5 |  1000/ 2983 batches | lr 0.001 | ms/batch 17.75 | loss  5.74 | ppl   310.94\n",
            "| epoch   5 |  1200/ 2983 batches | lr 0.001 | ms/batch 17.77 | loss  5.77 | ppl   319.31\n",
            "| epoch   5 |  1400/ 2983 batches | lr 0.001 | ms/batch 17.72 | loss  5.81 | ppl   332.17\n",
            "| epoch   5 |  1600/ 2983 batches | lr 0.001 | ms/batch 17.74 | loss  5.81 | ppl   333.23\n",
            "| epoch   5 |  1800/ 2983 batches | lr 0.001 | ms/batch 17.78 | loss  5.73 | ppl   307.47\n",
            "| epoch   5 |  2000/ 2983 batches | lr 0.001 | ms/batch 17.77 | loss  5.77 | ppl   321.81\n",
            "| epoch   5 |  2200/ 2983 batches | lr 0.001 | ms/batch 17.75 | loss  5.69 | ppl   294.93\n",
            "| epoch   5 |  2400/ 2983 batches | lr 0.001 | ms/batch 17.75 | loss  5.72 | ppl   305.51\n",
            "| epoch   5 |  2600/ 2983 batches | lr 0.001 | ms/batch 17.75 | loss  5.74 | ppl   312.20\n",
            "| epoch   5 |  2800/ 2983 batches | lr 0.001 | ms/batch 17.79 | loss  5.69 | ppl   295.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 55.18s | valid loss  6.06 | valid ppl   426.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/ 2983 batches | lr 0.001 | ms/batch 17.77 | loss  5.74 | ppl   312.31\n",
            "| epoch   6 |   400/ 2983 batches | lr 0.001 | ms/batch 17.83 | loss  5.69 | ppl   296.84\n",
            "| epoch   6 |   600/ 2983 batches | lr 0.001 | ms/batch 17.75 | loss  5.59 | ppl   266.69\n",
            "| epoch   6 |   800/ 2983 batches | lr 0.001 | ms/batch 17.82 | loss  5.65 | ppl   283.90\n",
            "| epoch   6 |  1000/ 2983 batches | lr 0.001 | ms/batch 17.73 | loss  5.63 | ppl   279.81\n",
            "| epoch   6 |  1200/ 2983 batches | lr 0.001 | ms/batch 17.79 | loss  5.66 | ppl   287.14\n",
            "| epoch   6 |  1400/ 2983 batches | lr 0.001 | ms/batch 17.76 | loss  5.70 | ppl   299.75\n",
            "| epoch   6 |  1600/ 2983 batches | lr 0.001 | ms/batch 17.78 | loss  5.71 | ppl   300.59\n",
            "| epoch   6 |  1800/ 2983 batches | lr 0.001 | ms/batch 17.73 | loss  5.63 | ppl   278.31\n",
            "| epoch   6 |  2000/ 2983 batches | lr 0.001 | ms/batch 17.63 | loss  5.67 | ppl   290.72\n",
            "| epoch   6 |  2200/ 2983 batches | lr 0.001 | ms/batch 17.62 | loss  5.58 | ppl   266.38\n",
            "| epoch   6 |  2400/ 2983 batches | lr 0.001 | ms/batch 17.60 | loss  5.62 | ppl   275.77\n",
            "| epoch   6 |  2600/ 2983 batches | lr 0.001 | ms/batch 17.56 | loss  5.65 | ppl   283.18\n",
            "| epoch   6 |  2800/ 2983 batches | lr 0.001 | ms/batch 17.54 | loss  5.59 | ppl   268.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 54.95s | valid loss  6.03 | valid ppl   417.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/ 2983 batches | lr 0.001 | ms/batch 17.56 | loss  5.65 | ppl   283.49\n",
            "| epoch   7 |   400/ 2983 batches | lr 0.001 | ms/batch 17.59 | loss  5.60 | ppl   270.66\n",
            "| epoch   7 |   600/ 2983 batches | lr 0.001 | ms/batch 17.63 | loss  5.49 | ppl   243.15\n",
            "| epoch   7 |   800/ 2983 batches | lr 0.001 | ms/batch 17.70 | loss  5.56 | ppl   258.98\n",
            "| epoch   7 |  1000/ 2983 batches | lr 0.001 | ms/batch 17.70 | loss  5.55 | ppl   256.36\n",
            "| epoch   7 |  1200/ 2983 batches | lr 0.001 | ms/batch 17.71 | loss  5.57 | ppl   262.97\n",
            "| epoch   7 |  1400/ 2983 batches | lr 0.001 | ms/batch 17.76 | loss  5.62 | ppl   275.00\n",
            "| epoch   7 |  1600/ 2983 batches | lr 0.001 | ms/batch 17.75 | loss  5.62 | ppl   275.73\n",
            "| epoch   7 |  1800/ 2983 batches | lr 0.001 | ms/batch 17.79 | loss  5.55 | ppl   256.00\n",
            "| epoch   7 |  2000/ 2983 batches | lr 0.001 | ms/batch 17.80 | loss  5.59 | ppl   266.93\n",
            "| epoch   7 |  2200/ 2983 batches | lr 0.001 | ms/batch 17.77 | loss  5.50 | ppl   244.62\n",
            "| epoch   7 |  2400/ 2983 batches | lr 0.001 | ms/batch 17.81 | loss  5.53 | ppl   252.87\n",
            "| epoch   7 |  2600/ 2983 batches | lr 0.001 | ms/batch 17.78 | loss  5.56 | ppl   260.65\n",
            "| epoch   7 |  2800/ 2983 batches | lr 0.001 | ms/batch 17.74 | loss  5.51 | ppl   247.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 55.06s | valid loss  6.02 | valid ppl   413.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/ 2983 batches | lr 0.001 | ms/batch 17.67 | loss  5.57 | ppl   261.16\n",
            "| epoch   8 |   400/ 2983 batches | lr 0.001 | ms/batch 17.72 | loss  5.52 | ppl   250.16\n",
            "| epoch   8 |   600/ 2983 batches | lr 0.001 | ms/batch 17.59 | loss  5.42 | ppl   224.89\n",
            "| epoch   8 |   800/ 2983 batches | lr 0.001 | ms/batch 17.62 | loss  5.48 | ppl   239.32\n",
            "| epoch   8 |  1000/ 2983 batches | lr 0.001 | ms/batch 17.60 | loss  5.47 | ppl   237.85\n",
            "| epoch   8 |  1200/ 2983 batches | lr 0.001 | ms/batch 17.62 | loss  5.50 | ppl   243.88\n",
            "| epoch   8 |  1400/ 2983 batches | lr 0.001 | ms/batch 17.62 | loss  5.54 | ppl   255.30\n",
            "| epoch   8 |  1600/ 2983 batches | lr 0.001 | ms/batch 17.58 | loss  5.55 | ppl   255.97\n",
            "| epoch   8 |  1800/ 2983 batches | lr 0.001 | ms/batch 17.58 | loss  5.47 | ppl   238.26\n",
            "| epoch   8 |  2000/ 2983 batches | lr 0.001 | ms/batch 17.60 | loss  5.51 | ppl   248.02\n",
            "| epoch   8 |  2200/ 2983 batches | lr 0.001 | ms/batch 17.55 | loss  5.43 | ppl   227.30\n",
            "| epoch   8 |  2400/ 2983 batches | lr 0.001 | ms/batch 17.60 | loss  5.46 | ppl   234.59\n",
            "| epoch   8 |  2600/ 2983 batches | lr 0.001 | ms/batch 17.61 | loss  5.49 | ppl   242.54\n",
            "| epoch   8 |  2800/ 2983 batches | lr 0.001 | ms/batch 17.61 | loss  5.44 | ppl   230.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 54.71s | valid loss  6.02 | valid ppl   411.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/ 2983 batches | lr 0.001 | ms/batch 17.62 | loss  5.49 | ppl   243.23\n",
            "| epoch   9 |   400/ 2983 batches | lr 0.001 | ms/batch 17.61 | loss  5.45 | ppl   233.56\n",
            "| epoch   9 |   600/ 2983 batches | lr 0.001 | ms/batch 17.66 | loss  5.35 | ppl   210.17\n",
            "| epoch   9 |   800/ 2983 batches | lr 0.001 | ms/batch 17.61 | loss  5.41 | ppl   223.43\n",
            "| epoch   9 |  1000/ 2983 batches | lr 0.001 | ms/batch 17.67 | loss  5.41 | ppl   222.78\n",
            "| epoch   9 |  1200/ 2983 batches | lr 0.001 | ms/batch 17.62 | loss  5.43 | ppl   228.31\n",
            "| epoch   9 |  1400/ 2983 batches | lr 0.001 | ms/batch 17.64 | loss  5.48 | ppl   239.10\n",
            "| epoch   9 |  1600/ 2983 batches | lr 0.001 | ms/batch 17.67 | loss  5.48 | ppl   239.81\n",
            "| epoch   9 |  1800/ 2983 batches | lr 0.001 | ms/batch 17.64 | loss  5.41 | ppl   223.71\n",
            "| epoch   9 |  2000/ 2983 batches | lr 0.001 | ms/batch 17.63 | loss  5.45 | ppl   232.58\n",
            "| epoch   9 |  2200/ 2983 batches | lr 0.001 | ms/batch 17.67 | loss  5.36 | ppl   213.08\n",
            "| epoch   9 |  2400/ 2983 batches | lr 0.001 | ms/batch 17.68 | loss  5.39 | ppl   219.59\n",
            "| epoch   9 |  2600/ 2983 batches | lr 0.001 | ms/batch 17.63 | loss  5.43 | ppl   227.60\n",
            "| epoch   9 |  2800/ 2983 batches | lr 0.001 | ms/batch 17.65 | loss  5.38 | ppl   216.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 54.80s | valid loss  6.02 | valid ppl   411.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/ 2983 batches | lr 0.001 | ms/batch 17.72 | loss  5.43 | ppl   228.44\n",
            "| epoch  10 |   400/ 2983 batches | lr 0.001 | ms/batch 17.58 | loss  5.39 | ppl   219.83\n",
            "| epoch  10 |   600/ 2983 batches | lr 0.001 | ms/batch 17.56 | loss  5.29 | ppl   197.99\n",
            "| epoch  10 |   800/ 2983 batches | lr 0.001 | ms/batch 17.60 | loss  5.35 | ppl   210.29\n",
            "| epoch  10 |  1000/ 2983 batches | lr 0.001 | ms/batch 17.63 | loss  5.35 | ppl   210.21\n",
            "| epoch  10 |  1200/ 2983 batches | lr 0.001 | ms/batch 17.61 | loss  5.37 | ppl   215.30\n",
            "| epoch  10 |  1400/ 2983 batches | lr 0.001 | ms/batch 17.58 | loss  5.42 | ppl   225.50\n",
            "| epoch  10 |  1600/ 2983 batches | lr 0.001 | ms/batch 17.60 | loss  5.42 | ppl   226.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "Exiting from training early\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giHtSc80sCcZ",
        "colab_type": "text"
      },
      "source": [
        "## (v-2) Show the perplexity score on the test set. You should select your best model based on the _valid_ set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4cIbY-8r7TB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "63e8c58f-f473-4e32-cea1-60303fa1a979"
      },
      "source": [
        "# Load the best saved model.\n",
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  5.92 | test ppl   372.39\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1C6Z9PgoQox",
        "colab_type": "text"
      },
      "source": [
        "## (vii) Adapt generate.py so that you can generate texts using your language model (FNNModel)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZzhYEfIoWtw",
        "colab_type": "code",
        "outputId": "fa7072f2-cde0-480c-ee2b-dd5eb247b226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "data = './data/wikitext-2'  # location of the data corpus\n",
        "checkpoint = './model.pt'   # model checkpoint to use\n",
        "outf = 'generated.txt'      # output file for generated text\n",
        "words = 1000                # number of words to generate\n",
        "seed = 1111                 # random seed\n",
        "cuda = True                 # use CUDA\n",
        "temperature = 1.0           # temperature - higher will increase diversity\n",
        "log_interval = 100          # reporting interval\n",
        "\n",
        "with open(checkpoint, 'rb') as f:\n",
        "  model = torch.load(f).to(device)\n",
        "model.eval()\n",
        "\n",
        "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "with open(outf, 'w') as outf:\n",
        "  with torch.no_grad(): # no tracking history\n",
        "    for i in range(words):\n",
        "      output = model(input)\n",
        "      word_weights = output.squeeze().div(temperature).exp().cpu()\n",
        "      word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "      input.fill_(word_idx)\n",
        "\n",
        "      word = corpus.dictionary.idx2word[word_idx]\n",
        "\n",
        "      outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "\n",
        "      if i % log_interval == 0:\n",
        "        print('| Generated {}/{} words'.format(i, words))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Generated 0/1000 words\n",
            "| Generated 100/1000 words\n",
            "| Generated 200/1000 words\n",
            "| Generated 300/1000 words\n",
            "| Generated 400/1000 words\n",
            "| Generated 500/1000 words\n",
            "| Generated 600/1000 words\n",
            "| Generated 700/1000 words\n",
            "| Generated 800/1000 words\n",
            "| Generated 900/1000 words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eubAEEWVs5Ei",
        "colab_type": "code",
        "outputId": "72c6c55d-e993-41c1-db12-a90a77c45310",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "!cat generated.txt"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "located at O 'Malley . A direction of The defensive public area . <eos> Dodge Moresby = = <eos> Within\n",
            "a Roman tonnes ( deep convection alone defence of the race with some challenges a game against the second playing\n",
            "power NAACP ) . In active performance of the 1650 Australian deity as a aircraft for the horse @-@ Him\n",
            "on Irish 4th Battle of passes overran the FIA 's most champion the drive of Walpole 's <unk> , along\n",
            "with 12 men to punt and the Augustan History and 11 November 18 @-@ pre @-@ because of nine deeply\n",
            "warm @-@ yard in luminosity for a 4 , as heaviest in the laws , which the Territorial Singles chart\n",
            ". The title in 1899 , what : a forward to a estuary . <eos> = = = <eos> Common\n",
            "starlings , where 550 house had below the 766th Regiment acquired much \" instrumentation , the popularity , the party\n",
            ". Occasionally he allowed another Washington Post Office : \" race and exploration = <eos> = <eos> <eos> <eos> <eos>\n",
            "= = Biography = <eos> <eos> <eos> = Marriage in the final too at least one agent and rise since\n",
            "the bride . The defensive line at that it . moral crystal themes are again by the final seven of\n",
            "Maine . <eos> <eos> Ímar informed about an frame , and 20 years . They represent the attempt to images\n",
            ", the population of the reduction to be able to many junior , husband , and trigger City and Hall\n",
            "\" ) \" incorporated down and the descendant of the new star athletes may have led to break into <unk>\n",
            "Gardner and restriction , conducted hair was the redshirt . It was forced to moderate Mission , affinity <eos> \"\n",
            ". <unk> \" remarked ; similar in the cutter at the early because agriculture , when they began towards creating\n",
            "a member of Ireland for these <unk> and Mantellodon , 2016 . Firstly was supposed to <unk> Society . By\n",
            "19 – 3 @.@ 14 @.@ 5 km / h ) . Oscar Stadium , but the small areas and\n",
            "the flu , suggests the strategy in long bands , but told Gandhi ; the longest play @-@ state has\n",
            "been written from this ; however , and and mb . <eos> Bell considered part of Toulon . <eos> It\n",
            "'s like about the decade of the new other generals , where . Gibson I was incomplete pass to complicating\n",
            "a fox to earn a formal desire to flooding with Michael appearing in <unk> ( died a fitting aspects of\n",
            "11 September 2 : <eos> <eos> <eos> <eos> As early until earlier inspiration for some value of the fruit with\n",
            "1800 – 1998 . The fans in Chains team : the <unk> volunteers from October 1903 to burn . <eos>\n",
            "After a few elections , 2012 , of other virtuoso <unk> Item = = <eos> = = = = 1982\n",
            ", on the pre walks with a pity , for a multiple voices is mentioned in a 4 % and\n",
            "42 runs as Fielding Myers started a major terrain . A figure wind shear , 2009 , to add a\n",
            "insurgency were endured a 7 km / ( an hour before leaving Valley Astronomical Albums Mosley 's upwards to his\n",
            "little offer to win him due to the few , passing are generally told him since the <unk> but have\n",
            "sovereignty . They have a food telescope , and several devolved those stories , and Tim and were questions are\n",
            "also published in the pectoral condoms have formerly the stained . The eastern North Korean troops to punt their shorts\n",
            "had a as an league . On July 1903 . Last series . It was recalled and Ulysses was challenged\n",
            "Mara finished in this proposed progressive deposits in 1924 there was kept a quarrel in the proposals makes a maximum\n",
            "thousand , which is a @-@ game a result . Johnson 's ง and a 5 @.@ 3 . =\n",
            "= <eos> <eos> <eos> <eos> <eos> and two players @-@ long diving isotopes and returned to large division , background\n",
            "and Arab in part as a 000 @,@ 500 men ( Washington Post received a motion 41 ended on July\n",
            "28 @.@ 7 cm ( 4 the town of 1930 to making its inspiration in 1914 and \" as .\n",
            "It is present in the bird who had experimental security and : The second quarter in one lead family usually\n",
            "run out of rugby and Tech received 28 @.@ 9 – 0 lead to Montoursville finished with the east side\n",
            ", Tanzania , a granite in Marines , juniors . Additionally , Pat Grand Prix ( FOCA . The Bruins\n",
            "out of baked alarm area of males determine the Midwest on 5 – 1 @.@ 7 . They become the\n",
            "ball . In 1898 , but , <unk> friend he washed out of John <unk> into helium , peaking at\n",
            "the <unk> . Since he was a vertical fin method of their day , , based on Route 221 yards\n",
            "and favored NC State 's final ( emotion , Hamels = <eos> = <eos> <eos> = = = = =\n",
            "= O 'Malley to <unk> . <eos> = Located 's 1963 . They have been initially signed their uprisings separated\n",
            "from a Order of German rivers and Canadian Department and Game issue . According to the Royal 21st century ,\n",
            "however by two in a king , and people . <eos> <eos> <eos> <eos> <eos> Following the ball had store\n",
            ", <unk> published in the year the general election are taken in many areas with political accustomed to switch to\n",
            "his track to carry out 10 – 41 seconds remaining in theatres in early summer and victory and the attack\n",
            "it was designed to compete . A large general election = <eos> = = = <eos> = = northern Proto\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoOAfzCmuloS",
        "colab_type": "text"
      },
      "source": [
        "## (viii) In your opinion, which computation/operation is the most expensive one in inference or forward pass? Can you think of ways to improve this? Is yea, please mention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo3Vq9kxuyvi",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fQyIgCyuzlu",
        "colab_type": "text"
      },
      "source": [
        "## (ix) Notice that the model also learns word vectors (input and output layer embeddings) as a byproduct. One way to evaluate the trained word vectors is to measure the cosine similarity between pairs of words, and then report the correlation with the similarity scores given by humans. For this exercise, use the dataset available [here](#) and report the Spearman correlation for the input embeddings. Exclude any pair if it is not in the embedding matrix."
      ]
    }
  ]
}