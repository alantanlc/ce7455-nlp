{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "assignment-1.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alanwuha/ce7455-nlp/blob/master/assignment-1/assignment-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHjQnRGmy7y_",
        "colab_type": "text"
      },
      "source": [
        "### Download wikitext-2 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXomv-vNyoBa",
        "colab_type": "code",
        "outputId": "0dde7e63-6fc4-4251-c3bd-426d768974c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "!rm -r data\n",
        "!mkdir data\n",
        "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip -P /content/data\n",
        "!unzip /content/data/wikitext-2-v1.zip -d /content/data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'data': No such file or directory\n",
            "--2020-02-06 04:57:06--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.106.158\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.106.158|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4475746 (4.3M) [application/zip]\n",
            "Saving to: ‘/content/data/wikitext-2-v1.zip’\n",
            "\n",
            "wikitext-2-v1.zip   100%[===================>]   4.27M  8.15MB/s    in 0.5s    \n",
            "\n",
            "2020-02-06 04:57:07 (8.15 MB/s) - ‘/content/data/wikitext-2-v1.zip’ saved [4475746/4475746]\n",
            "\n",
            "Archive:  /content/data/wikitext-2-v1.zip\n",
            "   creating: /content/data/wikitext-2/\n",
            "  inflating: /content/data/wikitext-2/wiki.test.tokens  \n",
            "  inflating: /content/data/wikitext-2/wiki.valid.tokens  \n",
            "  inflating: /content/data/wikitext-2/wiki.train.tokens  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRORzSz_zj-1",
        "colab_type": "text"
      },
      "source": [
        "### Specify arg variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2lAfxwKzoKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = './data/wikitext-2'  # location of the data corpus\n",
        "model = 'LSTM'              # type of recurrent net\n",
        "emsize = 200                # size of word embeddings\n",
        "nhid = 200                  # number of hidden units per layer\n",
        "nlayers = 2                 # number of layers\n",
        "lr = 20                     # initial learning rate\n",
        "clip = 0.25                 # gradient clipping\n",
        "epochs = 40                 # upper epoch limit\n",
        "batch_size = 20             # batch size\n",
        "bptt = 35                   # sequence length\n",
        "dropout = 0.2               # dropout applied to layers (0 = no dropout)\n",
        "tied = True                 # tie the word embedding and softmax weights\n",
        "seed = 1111                 # random seed\n",
        "cuda = True                # use CUDA\n",
        "log_interval = 200          # report interval\n",
        "save = 'model.pt'           # path to save the final model\n",
        "onnx_export = ''            # path to export the final model in onnx format\n",
        "nhead = 2                   # the number of heads in the encoder/decoder of the transformer model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1f0IyjFztIw",
        "colab_type": "text"
      },
      "source": [
        "### data.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5CIObkSzv-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from io import open\n",
        "import torch\n",
        "\n",
        "class Dictionary(object):\n",
        "  def __init__(self):\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = []\n",
        "\n",
        "  def add_word(self, word):\n",
        "    if word not in self.word2idx:\n",
        "      self.idx2word.append(word)\n",
        "      self.word2idx[word] = len(self.idx2word) - 1\n",
        "    return self.word2idx[word]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.idx2word)\n",
        "\n",
        "class Corpus(object):\n",
        "  def __init__(self, path):\n",
        "    self.dictionary = Dictionary()\n",
        "    self.train = self.tokenize(os.path.join(path, 'wiki.train.tokens'))\n",
        "    self.valid = self.tokenize(os.path.join(path, 'wiki.valid.tokens'))\n",
        "    self.test = self.tokenize(os.path.join(path, 'wiki.test.tokens'))\n",
        "\n",
        "  def tokenize(self, path):\n",
        "    \"\"\" Tokenizes a text file. \"\"\"\n",
        "    assert os.path.exists(path)\n",
        "\n",
        "    # Add words to the dictionary\n",
        "    with open(path, 'r', encoding='utf8') as f:\n",
        "      for line in f:\n",
        "        words = line.split() + ['<eos>']\n",
        "        for word in words:\n",
        "          self.dictionary.add_word(word)\n",
        "\n",
        "    # Tokenize file content\n",
        "    with open(path, 'r', encoding='utf8') as f:\n",
        "      idss = []\n",
        "      for line in f:\n",
        "        words = line.split() + ['<eos>']\n",
        "        ids = []\n",
        "        for word in words:\n",
        "          ids.append(self.dictionary.word2idx[word])\n",
        "        idss.append(torch.tensor(ids).type(torch.int64))\n",
        "      ids = torch.cat(idss)\n",
        "\n",
        "    return ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibvExv4t2R7a",
        "colab_type": "text"
      },
      "source": [
        "### main.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8tvc6BS2Zky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(seed)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "corpus = Corpus(data)\n",
        "\n",
        "def batchify(data, bsz):\n",
        "  # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "  nbatch = data.size(0) // bsz\n",
        "  # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "  data = data.narrow(0, 0, nbatch * bsz)\n",
        "  # Evenly divide the data across the bsz batches.\n",
        "  data = data.view(bsz, -1).t().contiguous()\n",
        "  return data.to(device)\n",
        "\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(corpus.train, batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "ntokens = len(corpus.dictionary)\n",
        "# model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLNElwkp5wRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCKRif2H5CuX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "54196370-e485-47f2-a2d5-ba8b29c2459c"
      },
      "source": [
        "corpus.train.size()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2088628])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksTkLP1k4z4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = batchify(corpus.train, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}