{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Named_Entity_Recognition-LSTM-CNN-CRF-Tutorial.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alanwuha/ce7455-nlp/blob/master/assignment-2/Named_Entity_Recognition-LSTM-CNN-CRF-Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6vuxyFFZR-6",
        "colab_type": "text"
      },
      "source": [
        "### End-to-end Sequence Labeling via Bi-Directional LSTM-CNNs-CRF\n",
        "\n",
        "In this tutorial we will demonstrate how to implement a state of the art Bi-Directional LSTM-CNN-CRF architecture (Published at ACL'16. [Link To Paper](https://www.aclweb.org/anthology/P16-1101/)) for Named Entity Recognition using Pytorch.\n",
        "\n",
        "The main aim of the tutorial is to make the audience comfortable with pytorch using this tutorial and give a step-by-by-step walk through of the Bi-LSTM-CNN-CRF architecture for NER. Some familiarity with pytorch (or any other deep learning framework) would definitely be a plus.\n",
        "\n",
        "The agenda of this tutorial is as follows:\n",
        "1. Getting Ready with the data\n",
        "2. Network Definition. This includes\n",
        "  - CNN Encoder for Character Level representation.\n",
        "  - Bi-Directional LSTM for Word-Level Encoding.\n",
        "  - Conditional Random Fields (CRF) for output decoding\n",
        "3. Training\n",
        "4. Model testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRJXANpsaowC",
        "colab_type": "text"
      },
      "source": [
        "#### Data Preparation\n",
        "\n",
        "The paper uses the English data from CoNLL 2003 shared task [1]. We will later apply more preprocessing steps to generate tag mapping, word mapping and character mapping. The data set contains four different types of named entities: PERSON, LOCATION, ORGANIZATION, and MISC and uses the BIO tagging scheme.\n",
        "\n",
        "BIO tagging scheme:\n",
        "```\n",
        "  I - Word is inside a phrase of type TYPE\n",
        "  B - If two phrases of the same type immediately follow each other, the first word of the second phrase will have a B-TYPE\n",
        "  O - Word is not part of a phrase\n",
        "```\n",
        "\n",
        "Example of English-NER sentence available in the data:\n",
        "```\n",
        "  U.N.         NNP   I-NP   I-ORG\n",
        "  official     NN    I-NP   O\n",
        "  Ekeus        NNP   I-NP   I-PER\n",
        "  heads        VBZ   I-VP   O\n",
        "  for          IN    I-PP   O\n",
        "  Baghdad      NNP   I-NP   I-LOC\n",
        "  .            .     O      O\n",
        "```\n",
        "\n",
        "Data Split (We use the same split as mentioned in paper):\n",
        "```\n",
        "  Training Data - eng.train\n",
        "  Validation Data - eng.testa\n",
        "  Testing Data - eng.testb\n",
        "```\n",
        "\n",
        "To get started we first import the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaTzhP7UcXND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "from torch.autograd import Variable\n",
        "from torch import autograd\n",
        "\n",
        "import time\n",
        "import _pickle as cPickle\n",
        "\n",
        "import urllib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.dpi'] = 80\n",
        "plt.style.use('seaborn-pastel')\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import codecs\n",
        "import re\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pohkEQA-dFnI",
        "colab_type": "text"
      },
      "source": [
        "#### Define constants and parameters\n",
        "\n",
        "We now define some constants and parameters that we will be using later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA8mTT_3dU_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parameters for the Model\n",
        "parameters = OrderedDict()\n",
        "parameters['train'] = \"./data/eng.train\"  # Path to train file\n",
        "parameters['dev'] = \"./data/eng.testa\"  # Path to dev file\n",
        "parameters['test'] = \"./data/eng.testb\" # Path to test file\n",
        "parameters['tag_scheme'] = \"BIOES\"  # BIO or BIOES\n",
        "parameters['lower'] = True  # Boolean variable to control lowercasing of words\n",
        "parameters['zeros'] = True # Boolean variable to control replacement of all digits by 0\n",
        "parameters['char_dim'] = 30 # Char embedding dimension\n",
        "parameters['word_dim'] = 100  # Token embedding dimension\n",
        "parameters['word_lstm_dim'] = 200 # Token LSTM hidden layer size\n",
        "parameters['word_bidirect'] = True # Use a bidirectional LSTM for words\n",
        "parameters['embedding_path'] = \"./data/glove.6B.100d.txt\" # Location of pretrained embeddings\n",
        "parameters['all_emb'] = 1 # Load all embeddings\n",
        "parameters['crf'] = 1 # Use CRF (0 to disable)\n",
        "parameters['dropout'] = 0.5 # Dropout on the input (0 = no dropout)\n",
        "parameters['epoch'] = 50  # Number of epochs to run\n",
        "parameters['weights'] = \"\"  # Path to pretrained for from a previous run\n",
        "parameters['name'] = \"self-trained-model\" # Model name\n",
        "parameters['gradient_clip'] = 5.0\n",
        "parameters['char_mode'] = \"CNN\"\n",
        "models_path = \"./models/\" # Path to saved models\n",
        "\n",
        "# GPU\n",
        "parameters['use_gpu'] = torch.cuda.is_available() # GPU check\n",
        "use_gpu = parameters['use_gpu']\n",
        "\n",
        "parameters['reload'] = \"./models/pre-trained-model\"\n",
        "\n",
        "# Constants\n",
        "START_TAG = '<START>'\n",
        "STOP_TAG = '<STOP>'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuOcOPVFfBnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# paths to files\n",
        "# to stored mapping file\n",
        "mapping_file = './data/mapping.pkl'\n",
        "\n",
        "# to stored model\n",
        "name = parameters['name']\n",
        "model_name = models_path + name # get_name(parameters)\n",
        "\n",
        "if not os.path.exists(models_path):\n",
        "  os.makedirs(models_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1ITBcvPLWNG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "outputId": "abd23539-b9b3-4260-947c-1c91d81745e6"
      },
      "source": [
        "!rm -rf data\n",
        "!mkdir data\n",
        "!wget -P ./data https://raw.githubusercontent.com/TheAnig/NER-LSTM-CNN-Pytorch/master/data/eng.testa\n",
        "!wget -P ./data https://raw.githubusercontent.com/TheAnig/NER-LSTM-CNN-Pytorch/master/data/eng.testb\n",
        "!wget -P ./data https://raw.githubusercontent.com/TheAnig/NER-LSTM-CNN-Pytorch/master/data/eng.train\n",
        "!wget -P ./data https://raw.githubusercontent.com/TheAnig/NER-LSTM-CNN-Pytorch/master/data/eng.train54019\n",
        "!wget -P ./data https://raw.githubusercontent.com/TheAnig/NER-LSTM-CNN-Pytorch/master/data/mapping.pkl"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-26 06:42:52--  https://raw.githubusercontent.com/TheAnig/NER-LSTM-CNN-Pytorch/master/data/eng.testa\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 827009 (808K) [text/plain]\n",
            "Saving to: ‘./data/eng.testa’\n",
            "\n",
            "\reng.testa             0%[                    ]       0  --.-KB/s               \reng.testa           100%[===================>] 807.63K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2020-02-26 06:42:52 (140 MB/s) - ‘./data/eng.testa’ saved [827009/827009]\n",
            "\n",
            "--2020-02-26 06:42:54--  https://raw.githubusercontent.com/TheAnig/NER-LSTM-CNN-Pytorch/master/data/eng.testb\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 748094 (731K) [text/plain]\n",
            "Saving to: ‘./data/eng.testb’\n",
            "\n",
            "eng.testb           100%[===================>] 730.56K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2020-02-26 06:42:54 (90.9 MB/s) - ‘./data/eng.testb’ saved [748094/748094]\n",
            "\n",
            "--2020-02-26 06:42:56--  https://raw.githubusercontent.com/TheAnig/NER-LSTM-CNN-Pytorch/master/data/eng.train\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3281527 (3.1M) [text/plain]\n",
            "Saving to: ‘./data/eng.train’\n",
            "\n",
            "eng.train           100%[===================>]   3.13M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-02-26 06:42:56 (146 MB/s) - ‘./data/eng.train’ saved [3281527/3281527]\n",
            "\n",
            "--2020-02-26 06:42:58--  https://raw.githubusercontent.com/TheAnig/NER-LSTM-CNN-Pytorch/master/data/eng.train54019\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 811103 (792K) [text/plain]\n",
            "Saving to: ‘./data/eng.train54019’\n",
            "\n",
            "eng.train54019      100%[===================>] 792.09K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2020-02-26 06:42:58 (86.8 MB/s) - ‘./data/eng.train54019’ saved [811103/811103]\n",
            "\n",
            "--2020-02-26 06:43:00--  https://raw.githubusercontent.com/TheAnig/NER-LSTM-CNN-Pytorch/master/data/mapping.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14350005 (14M) [application/octet-stream]\n",
            "Saving to: ‘./data/mapping.pkl’\n",
            "\n",
            "mapping.pkl         100%[===================>]  13.68M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2020-02-26 06:43:01 (239 MB/s) - ‘./data/mapping.pkl’ saved [14350005/14350005]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL8Xys02feVO",
        "colab_type": "text"
      },
      "source": [
        "#### Load data and preprocess\n",
        "\n",
        "Firstly, the data is loaded from the train, dev and test files into a list of sentences.\n",
        "\n",
        "Preprocessing:\n",
        "```\n",
        "  * All the digits in the words are replaced by 0\n",
        "```\n",
        "\n",
        "Why this preprocessing step?\n",
        "```\n",
        "  * For the Named Entity Recognition task, the information present in numerical digits does not help in predcting the entity. So, we replace all the digits by 0. So, now the model can concentrate on more important alphabets.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mhvR271gEU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def zero_digits(s):\n",
        "  \"\"\"\n",
        "  Replace every digit in a string by a zero.\n",
        "  \"\"\"\n",
        "  return re.sub('\\d', '0', s)\n",
        "\n",
        "def load_sentences(path, zeros):\n",
        "  \"\"\"\n",
        "  Load sentences. A line must contain at least a word and its tag.\n",
        "  Sentences are separated by empty lines.\n",
        "  \"\"\"\n",
        "  sentences = []\n",
        "  sentence = []\n",
        "  for line in codecs.open(path, 'r', 'utf8'): # codecs.open - open an encoded file using the given mode and return a wrapped version providing transparent encoding/decoding.\n",
        "    line = zero_digits(line.rstrip()) if zeros else line.rstrip() # rstrip: return a copy of the string with trailing whitespace removed. If chars is given and not None, remove characters in chars instead.\n",
        "    if not line:\n",
        "      if len(sentence) > 0:\n",
        "        if 'DOCSTART' not in sentence[0][0]:\n",
        "          sentences.append(sentence)\n",
        "        sentence = []\n",
        "    else:\n",
        "      word = line.split()\n",
        "      assert len(word) >= 2\n",
        "      sentence.append(word)\n",
        "  if len(sentence) > 0:\n",
        "    if 'DOCTSTART' not in sentence[0][0]:\n",
        "      sentences.append(sentence)\n",
        "  return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEQEjjckLE8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sentences = load_sentences(parameters['train'], parameters['zeros'])  # eng.train\n",
        "test_sentences = load_sentences(parameters['test'], parameters['zeros'])    # eng.testb\n",
        "dev_sentences = load_sentences(parameters['dev'], parameters['zeros'])      # eng.testa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49V4nBFMPVT5",
        "colab_type": "text"
      },
      "source": [
        "### Update tagging scheme\n",
        "\n",
        "Different types of tagging schemes can be used for NER. We update the tags for train, test and dev data (depending on the parameters [tag_scheme]).\n",
        "\n",
        "In the paper, the authors use the tagging scheme ( BIOES ) rather than BIO (which is used by the dataset). So, we need to first update the data to convert tag scheme from BIO to BIOES.\n",
        "\n",
        "BIOES tagging scheme:\n",
        "```\n",
        "  I - Word is inside a phrase of type TYPE\n",
        "  B - If two phrases of the same type immediately follow each other, the first word of the second phrase will have tag B-TYPE\n",
        "  O - Word i not part of a phrase\n",
        "  E - End ( E will not appear in a prefix-only partial match )\n",
        "  S - Single\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a2aLXErQg1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iob2(tags):\n",
        "  \"\"\"\n",
        "  Check that tags have a valid BIO format.\n",
        "  Tags in BIO1 format are converted to BIO2.\n",
        "  \"\"\"\n",
        "  for i, tag in enumerate(tags):\n",
        "    if tag == 'O':\n",
        "      continue\n",
        "    split = tag.split('-')\n",
        "    if len(split) != 2 or split[0] not in ['I', 'B']:\n",
        "      return False\n",
        "    if split[0] == 'B':\n",
        "      continue\n",
        "    elif i == 0 or tags[i - 1] == 'O':  # conversion IOB1 to IOB2\n",
        "      tags[i] = 'B' + tag[1:]\n",
        "    elif tags[i-1][1:] == tag[1:]:\n",
        "      continue\n",
        "    else: # conversion IOB1 to IOB2\n",
        "      tags[i] = 'B' + tag[1:]\n",
        "  return True\n",
        "\n",
        "def iob_iobes(tags):\n",
        "  \"\"\"\n",
        "  the function is used to convert\n",
        "  BIO -> BIOES tagging\n",
        "  \"\"\"\n",
        "  new_tags = []\n",
        "  for i, tag in enumerate(tags):\n",
        "    if tag == 'O':\n",
        "      new_tags.append(tag)\n",
        "    elif tag.split('-')[0] == 'B':\n",
        "      if i + 1 != len(tags) and \\\n",
        "        tags[i + 1].split('-')[0] == 'I':\n",
        "        new_tags.append(tag)\n",
        "      else:\n",
        "        new_tags.append(tag.replace('B-', 'S-'))\n",
        "    elif tag.split('-')[0] == 'I':\n",
        "      if i + 1 < len(tags) and \\\n",
        "        tags[i + 1].split('-')[0] == 'I':\n",
        "        new_tags.append(tag)\n",
        "      else:\n",
        "        new_tags.append(tag.replace('I-', 'E-'))\n",
        "    else:\n",
        "      raise Exception('Invalid IOB format!')\n",
        "  return new_tags\n",
        "\n",
        "def update_tag_scheme(sentences, tag_scheme):\n",
        "  \"\"\"\n",
        "  Check and update sentences tagging scheme to BIO2\n",
        "  Only BIO1 and BIO2 schemes are accepted for input data.\n",
        "  \"\"\"\n",
        "  for i, s in enumerate(sentences):\n",
        "    tags = [w[-1] for w in s]\n",
        "    # Check that tags are given in the BIO format\n",
        "    if not iob2(tags):\n",
        "      s_str = '\\n'.join(' '.join(w) for w in s)\n",
        "      raise Exception('Sentences should be given in BIO format! ' +\n",
        "                      ' Please check sentence %i:\\n%s' % (i, s_str))\n",
        "    if tag_scheme == 'BIOES':\n",
        "      new_tags = iob_iobes(tags)\n",
        "      for word, new_tag in zip(s, new_tags):\n",
        "        word[-1] = new_tag\n",
        "    else:\n",
        "      raise Exception('Wrong tagging scheme!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAMpYQohTi4i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "0f276039-e0ae-4bbe-84a0-202304a5fc7e"
      },
      "source": [
        "print(train_sentences[0])\n",
        "print(dev_sentences[0])\n",
        "print(test_sentences[0])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['EU', 'NNP', 'I-NP', 'I-ORG'], ['rejects', 'VBZ', 'I-VP', 'O'], ['German', 'JJ', 'I-NP', 'I-MISC'], ['call', 'NN', 'I-NP', 'O'], ['to', 'TO', 'I-VP', 'O'], ['boycott', 'VB', 'I-VP', 'O'], ['British', 'JJ', 'I-NP', 'I-MISC'], ['lamb', 'NN', 'I-NP', 'O'], ['.', '.', 'O', 'O']]\n",
            "[['CRICKET', 'NNP', 'I-NP', 'O'], ['-', ':', 'O', 'O'], ['LEICESTERSHIRE', 'NNP', 'I-NP', 'I-ORG'], ['TAKE', 'NNP', 'I-NP', 'O'], ['OVER', 'IN', 'I-PP', 'O'], ['AT', 'NNP', 'I-NP', 'O'], ['TOP', 'NNP', 'I-NP', 'O'], ['AFTER', 'NNP', 'I-NP', 'O'], ['INNINGS', 'NNP', 'I-NP', 'O'], ['VICTORY', 'NN', 'I-NP', 'O'], ['.', '.', 'O', 'O']]\n",
            "[['SOCCER', 'NN', 'I-NP', 'O'], ['-', ':', 'O', 'O'], ['JAPAN', 'NNP', 'I-NP', 'I-LOC'], ['GET', 'VB', 'I-VP', 'O'], ['LUCKY', 'NNP', 'I-NP', 'O'], ['WIN', 'NNP', 'I-NP', 'O'], [',', ',', 'O', 'O'], ['CHINA', 'NNP', 'I-NP', 'I-PER'], ['IN', 'IN', 'I-PP', 'O'], ['SURPRISE', 'DT', 'I-NP', 'O'], ['DEFEAT', 'NN', 'I-NP', 'O'], ['.', '.', 'O', 'O']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1G20qAZvT7ez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "update_tag_scheme(train_sentences, parameters['tag_scheme'])\n",
        "update_tag_scheme(dev_sentences, parameters['tag_scheme'])\n",
        "update_tag_scheme(test_sentences, parameters['tag_scheme'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgmp9kFHVKb5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "e52b7974-21cf-47a9-ada0-27e970a2fc6a"
      },
      "source": [
        "print(train_sentences[0])\n",
        "print(dev_sentences[0])\n",
        "print(test_sentences[0])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['EU', 'NNP', 'I-NP', 'S-ORG'], ['rejects', 'VBZ', 'I-VP', 'O'], ['German', 'JJ', 'I-NP', 'S-MISC'], ['call', 'NN', 'I-NP', 'O'], ['to', 'TO', 'I-VP', 'O'], ['boycott', 'VB', 'I-VP', 'O'], ['British', 'JJ', 'I-NP', 'S-MISC'], ['lamb', 'NN', 'I-NP', 'O'], ['.', '.', 'O', 'O']]\n",
            "[['CRICKET', 'NNP', 'I-NP', 'O'], ['-', ':', 'O', 'O'], ['LEICESTERSHIRE', 'NNP', 'I-NP', 'S-ORG'], ['TAKE', 'NNP', 'I-NP', 'O'], ['OVER', 'IN', 'I-PP', 'O'], ['AT', 'NNP', 'I-NP', 'O'], ['TOP', 'NNP', 'I-NP', 'O'], ['AFTER', 'NNP', 'I-NP', 'O'], ['INNINGS', 'NNP', 'I-NP', 'O'], ['VICTORY', 'NN', 'I-NP', 'O'], ['.', '.', 'O', 'O']]\n",
            "[['SOCCER', 'NN', 'I-NP', 'O'], ['-', ':', 'O', 'O'], ['JAPAN', 'NNP', 'I-NP', 'S-LOC'], ['GET', 'VB', 'I-VP', 'O'], ['LUCKY', 'NNP', 'I-NP', 'O'], ['WIN', 'NNP', 'I-NP', 'O'], [',', ',', 'O', 'O'], ['CHINA', 'NNP', 'I-NP', 'S-PER'], ['IN', 'IN', 'I-PP', 'O'], ['SURPRISE', 'DT', 'I-NP', 'O'], ['DEFEAT', 'NN', 'I-NP', 'O'], ['.', '.', 'O', 'O']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXqDEd53VM9V",
        "colab_type": "text"
      },
      "source": [
        "#### Create Mappings for Words, Characters and Tags\n",
        "\n",
        "After we have updated the tag scheme, we now have a list of sentences which are words along with their modified tags. Now, we want to map these individual words, tags and characters in each word, to unique numerical IDs so that each unique word, character and tag in the vocabulary is represented by a particular integer ID. To do this, we first create a function that does this mapping for us.\n",
        "\n",
        "#### Why mapping is important?\n",
        "\n",
        "These indices for words, tags and characters help us employ matrix (tensor) operations inside the neural network architecture, which are considerably faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQvpgxqCW28O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dico(item_list):\n",
        "  \"\"\"\n",
        "  Create a dictionary of items from a list of list of items.\n",
        "  \"\"\"\n",
        "  assert type(item_list) is list\n",
        "  dico = {}\n",
        "  for items in item_list:\n",
        "    for item in items:\n",
        "      if item not in dico:\n",
        "        dico[item] = 1\n",
        "      else:\n",
        "        dico[item] += 1\n",
        "  return dico\n",
        "\n",
        "def create_mapping(dico):\n",
        "  \"\"\"\n",
        "  Create a mapping (item to ID / ID to item) from a dictionary.\n",
        "  Items are ordered by decreasing frequency.\n",
        "  \"\"\"\n",
        "  sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n",
        "  id_to_item = {i: v[0] for i, v in enumerate(sorted_items)}\n",
        "  item_to_id = {v: k for k, v in id_to_item.items()}\n",
        "  return item_to_id, id_to_item\n",
        "\n",
        "def word_mapping(sentences, lower):\n",
        "  \"\"\"\n",
        "  Create a dictionary and a mapping of words, sorted by frequency.\n",
        "  \"\"\"\n",
        "  words = [ [x[0].lower() if lower else x[0] for x in s] for s in sentences ] # list of list of words in lower case, x[0] is the word of each sentence\n",
        "  dico = create_dico(words)\n",
        "  dico['<UNK>'] = 10000000 # UNK tag for unknown words\n",
        "  word_to_id, id_to_word = create_mapping(dico)\n",
        "  print(\"Found %i unique words (%i in total)\" % (len(dico), sum(len(x) for x in words)))\n",
        "  return dico, word_to_id, id_to_word\n",
        "\n",
        "def char_mapping(sentences):\n",
        "  \"\"\"\n",
        "  Create a dictionary and mapping of characters, sorted by frequency.\n",
        "  \"\"\"\n",
        "  chars = [\"\".join([w[0] for w in s]) for s in sentences]\n",
        "  dico = create_dico(chars)\n",
        "  char_to_id, id_to_char = create_mapping(dico)\n",
        "  print(\"Found %i unique characters\" % len(dico))\n",
        "  return dico, char_to_id, id_to_char\n",
        "\n",
        "def tag_mapping(sentences):\n",
        "  \"\"\"\n",
        "  Create a dictionary and a mapping of tags, sorted by frequency.\n",
        "  \"\"\"\n",
        "  tags = [[word[-1] for word in s] for s in sentences]\n",
        "  dico = create_dico(tags)\n",
        "  dico[START_TAG] = -1\n",
        "  dico[STOP_TAG] = -2\n",
        "  tag_to_id, id_to_tag = create_mapping(dico)\n",
        "  print(\"Found %i unique named entity tags\" % len(dico))\n",
        "  return dico, tag_to_id, id_to_tag"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kAArDRFgR5L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "edc4cad7-da0c-4a66-d1f3-1f76051ab676"
      },
      "source": [
        "dico_words, word_to_id, id_to_word = word_mapping(train_sentences, parameters['lower'])\n",
        "dico_chars, char_to_id, id_to_char = char_mapping(train_sentences)\n",
        "dico_tags, tag_to_id, id_to_tag = tag_mapping(train_sentences)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 17494 unique words (203622 in total)\n",
            "Found 75 unique characters\n",
            "Found 19 unique named entity tags\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}