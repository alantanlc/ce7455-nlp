{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "roberta.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alanwuha/ce7455-nlp/blob/master/group-project/roberta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-EqHWPd5uUi",
        "colab_type": "code",
        "outputId": "2532c07d-e9f8-45ba-e32c-678f7f808816",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 883
        }
      },
      "source": [
        "!pip install spacy\n",
        "!pip install pytorch-transformers==1.1.0\n",
        "!pip install tensorboardX\n",
        "!pip install scikit-learn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (46.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.38.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.6.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
            "Requirement already satisfied: pytorch-transformers==1.1.0 in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0) (4.38.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0) (1.18.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0) (1.12.35)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0) (0.1.85)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0) (2.21.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers==1.1.0) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.35 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers==1.1.0) (1.15.35)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers==1.1.0) (0.3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.1.0) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.1.0) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.1.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.1.0) (1.24.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.35->boto3->pytorch-transformers==1.1.0) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.35->boto3->pytorch-transformers==1.1.0) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.35->boto3->pytorch-transformers==1.1.0) (1.12.0)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (46.1.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ciac7T8uGGh8",
        "colab_type": "text"
      },
      "source": [
        "download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxtraFfRdxm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir('data'):\n",
        "  os.system('wget https://storage.googleapis.com/ai2-mosaic/public/winogrande/winogrande_1.1.zip')\n",
        "  os.system('unzip winogrande_1.1.zip; rm -rf __MACOSX')\n",
        "  os.system('mv winogrande_1.1 data')\n",
        "  os.system('rm winogrande_1.1.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0I3VsyyGOD5",
        "colab_type": "text"
      },
      "source": [
        "imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ5UfPSV5Ggt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Utility for finetuning BERT/RoBERTa models on WinoGrande. \"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import csv\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "from io import open\n",
        "\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from sklearn.metrics import matthews_corrcoef, f1_score\n",
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43Mh13vV5bov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_transformers import BertPreTrainedModel, RobertaConfig, \\\n",
        "    ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP, RobertaModel\n",
        "from pytorch_transformers.modeling_roberta import RobertaClassificationHead\n",
        "from torch.nn import CrossEntropyLoss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWSpqvMY5Ydf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" Finetuning BERT/RoBERTa models on WinoGrande. \"\"\"\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tensorboardX import SummaryWriter\n",
        "from tqdm import tqdm, trange\n",
        "import pathlib\n",
        "\n",
        "from pytorch_transformers import (WEIGHTS_NAME, BertConfig,\n",
        "                                  BertForSequenceClassification, BertForMultipleChoice,\n",
        "                                  BertTokenizer,\n",
        "                                  RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer,\n",
        "                                  XLMConfig, XLMForSequenceClassification,\n",
        "                                  XLMTokenizer, XLNetConfig,\n",
        "                                  XLNetForSequenceClassification,\n",
        "                                  XLNetTokenizer)\n",
        "\n",
        "from pytorch_transformers import AdamW, WarmupLinearSchedule"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzliNB8Y5LyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZK7QHAl5SWi",
        "colab_type": "text"
      },
      "source": [
        "utils.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwzyDvjc5U5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "            label: (Optional) string. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class MCInputExample(object):\n",
        "    def __init__(self, guid, options, label):\n",
        "        self.guid = guid\n",
        "        self.options = options\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id\n",
        "\n",
        "\n",
        "class MultipleChoiceFeatures(object):\n",
        "    def __init__(self,\n",
        "                 example_id,\n",
        "                 option_features,\n",
        "                 label=None):\n",
        "        self.example_id = example_id\n",
        "        self.option_features = self.choices_features = [\n",
        "            {\n",
        "                'input_ids': input_ids,\n",
        "                'input_mask': input_mask,\n",
        "                'segment_ids': segment_ids\n",
        "            }\n",
        "            for _, input_ids, input_mask, segment_ids in option_features\n",
        "        ]\n",
        "        self.label = int(label)\n",
        "\n",
        "\n",
        "class DataProcessor(object):\n",
        "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the test set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @classmethod\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "            lines = []\n",
        "            for line in reader:\n",
        "                if sys.version_info[0] == 2:\n",
        "                    line = list(unicode(cell, 'utf-8') for cell in line)\n",
        "                lines.append(line)\n",
        "            return lines\n",
        "\n",
        "    @classmethod\n",
        "    def _read_jsonl(cls, input_file):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        records = []\n",
        "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "            for line in f:\n",
        "                records.append(json.loads(line))\n",
        "            return records\n",
        "\n",
        "\n",
        "class WinograndeProcessor(DataProcessor):\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        return self._create_examples(\n",
        "            self._read_jsonl(os.path.join(data_dir, \"train.jsonl\")))\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        return self._create_examples(\n",
        "            self._read_jsonl(os.path.join(data_dir, \"dev.jsonl\")))\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        return self._create_examples(\n",
        "            self._read_jsonl(os.path.join(data_dir, \"test.jsonl\")))\n",
        "\n",
        "    def get_labels(self):\n",
        "        return [\"1\", \"2\"]\n",
        "\n",
        "    def _create_examples(self, records):\n",
        "        examples = []\n",
        "        for (i, record) in enumerate(records):\n",
        "            guid = record['qID']\n",
        "            sentence = record['sentence']\n",
        "\n",
        "            name1 = record['option1']\n",
        "            name2 = record['option2']\n",
        "            if not 'answer' in record:\n",
        "                # This is a dummy label for test prediction.\n",
        "                # test.jsonl doesn't include the `answer`.\n",
        "                label = \"1\"\n",
        "            else:\n",
        "                label = record['answer']\n",
        "\n",
        "            conj = \"_\"\n",
        "            idx = sentence.index(conj)\n",
        "            context = sentence[:idx]\n",
        "            option_str = \"_ \" + sentence[idx + len(conj):].strip()\n",
        "\n",
        "            option1 = option_str.replace(\"_\", name1)\n",
        "            option2 = option_str.replace(\"_\", name2)\n",
        "\n",
        "            mc_example = MCInputExample(\n",
        "                guid=guid,\n",
        "                options=[\n",
        "                    {\n",
        "                        'segment1': context,\n",
        "                        'segment2': option1\n",
        "                    },\n",
        "                    {\n",
        "                        'segment1': context,\n",
        "                        'segment2': option2\n",
        "                    }\n",
        "                ],\n",
        "                label=label\n",
        "            )\n",
        "            examples.append(mc_example)\n",
        "\n",
        "        return examples\n",
        "\n",
        "\n",
        "class SuperGlueWscProcessor(DataProcessor):\n",
        "    \"\"\"Processor for the SuperGLUE-WSC \"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        return self._create_examples(\n",
        "            self._read_jsonl(os.path.join(data_dir, \"train.jsonl\")))\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        return self._create_examples(\n",
        "            self._read_jsonl(os.path.join(data_dir, \"dev.jsonl\")))\n",
        "\n",
        "    def get_test_examples(self, data_dir):\n",
        "        return self._create_examples(\n",
        "            self._read_jsonl(os.path.join(data_dir, \"test.jsonl\")))\n",
        "\n",
        "    def get_labels(self):\n",
        "        return [\"0\", \"1\"]\n",
        "\n",
        "    def _create_examples(self, records):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, record) in enumerate(records):\n",
        "            guid = record['idx']\n",
        "            text_a = record[\"sentence1\"]\n",
        "            text_b = record[\"sentence2\"]\n",
        "            label = record[\"label\"]\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
        "        return examples\n",
        "\n",
        "\n",
        "def convert_examples_to_features(examples,\n",
        "                                 label_list,\n",
        "                                 max_seq_length,\n",
        "                                 tokenizer,\n",
        "                                 output_mode,\n",
        "                                 cls_token_at_end=False,\n",
        "                                 pad_on_left=False,\n",
        "                                 cls_token='[CLS]',\n",
        "                                 sep_token='[SEP]',\n",
        "                                 sep_token_extra=False,\n",
        "                                 pad_token=0,\n",
        "                                 sequence_a_segment_id=0,\n",
        "                                 sequence_b_segment_id=1,\n",
        "                                 cls_token_segment_id=1,\n",
        "                                 pad_token_segment_id=0,\n",
        "                                 mask_padding_with_zero=True):\n",
        "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
        "        `cls_token_at_end` define the location of the CLS token:\n",
        "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
        "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
        "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
        "    \"\"\"\n",
        "\n",
        "    label_map = {label : i for i, label in enumerate(label_list)}\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        if ex_index % 10000 == 0:\n",
        "            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
        "\n",
        "        tokens_a = tokenizer.tokenize(example.text_a)\n",
        "\n",
        "        tokens_b = None\n",
        "        special_tokens_count = 3 if sep_token_extra else 2\n",
        "        if example.text_b:\n",
        "            tokens_b = tokenizer.tokenize(example.text_b)\n",
        "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "            # length is less than the specified length.\n",
        "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "\n",
        "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - special_tokens_count - 1)\n",
        "        else:\n",
        "            # Account for [CLS] and [SEP] with \"- 2\" or \"-3\" for RoBERTa\n",
        "            if len(tokens_a) > max_seq_length - special_tokens_count:\n",
        "                tokens_a = tokens_a[:(max_seq_length - special_tokens_count)]\n",
        "\n",
        "        # The convention in BERT is:\n",
        "        # (a) For sequence pairs:\n",
        "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
        "        # (b) For single sequences:\n",
        "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "        #  type_ids:   0   0   0   0  0     0   0\n",
        "        #\n",
        "        # Where \"type_ids\" are used to indicate whether this is the first\n",
        "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "        # embedding vector (and position vector). This is not *strictly* necessary\n",
        "        # since the [SEP] token unambiguously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "        #\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "        tokens = tokens_a + [sep_token]\n",
        "        if sep_token_extra:\n",
        "            # roberta uses an extra separator b/w pairs of sentences\n",
        "            tokens += [sep_token]\n",
        "        segment_ids = [sequence_a_segment_id] * len(tokens)\n",
        "\n",
        "        if tokens_b:\n",
        "            tokens += tokens_b + [sep_token]\n",
        "            segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n",
        "\n",
        "        if cls_token_at_end:\n",
        "            tokens = tokens + [cls_token]\n",
        "            segment_ids = segment_ids + [cls_token_segment_id]\n",
        "        else:\n",
        "            tokens = [cls_token] + tokens\n",
        "            segment_ids = [cls_token_segment_id] + segment_ids\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding_length = max_seq_length - len(input_ids)\n",
        "        if pad_on_left:\n",
        "            input_ids = ([pad_token] * padding_length) + input_ids\n",
        "            input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
        "            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
        "        else:\n",
        "            input_ids = input_ids + ([pad_token] * padding_length)\n",
        "            input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
        "            segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "\n",
        "        if output_mode == \"classification\":\n",
        "            label_id = label_map[example.label]\n",
        "        elif output_mode == \"regression\":\n",
        "            label_id = float(example.label)\n",
        "        else:\n",
        "            raise KeyError(output_mode)\n",
        "\n",
        "        if ex_index < 5:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\" % (example.guid))\n",
        "            logger.info(\"tokens: %s\" % \" \".join(\n",
        "                [str(x) for x in tokens]))\n",
        "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "            logger.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(input_ids=input_ids,\n",
        "                          input_mask=input_mask,\n",
        "                          segment_ids=segment_ids,\n",
        "                          label_id=label_id))\n",
        "    return features\n",
        "\n",
        "\n",
        "def convert_multiple_choice_examples_to_features(examples, label_list, max_seq_length,\n",
        "                                                 tokenizer, output_mode,\n",
        "                                                 cls_token_at_end=False, pad_on_left=False,\n",
        "                                                 cls_token='[CLS]', sep_token='[SEP]', sep_token_extra=False, pad_token=0,\n",
        "                                                 sequence_a_segment_id=0, sequence_b_segment_id=1,\n",
        "                                                 cls_token_segment_id=1, pad_token_segment_id=0,\n",
        "                                                 mask_padding_with_zero=True):\n",
        "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
        "        `cls_token_at_end` define the location of the CLS token:\n",
        "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
        "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
        "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
        "    \"\"\"\n",
        "\n",
        "    label_map = {label : i for i, label in enumerate(label_list)}\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        if ex_index % 10000 == 0:\n",
        "            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
        "        option_features = []\n",
        "        for option in example.options:\n",
        "\n",
        "            context_tokens = tokenizer.tokenize(option['segment1'])\n",
        "\n",
        "            option_tokens = tokenizer.tokenize(option['segment2'])\n",
        "            special_tokens_count = 4 if sep_token_extra else 3\n",
        "            _truncate_seq_pair(context_tokens, option_tokens, max_seq_length - special_tokens_count)\n",
        "\n",
        "            tokens = context_tokens + [sep_token]\n",
        "\n",
        "            if sep_token_extra:\n",
        "                # roberta uses an extra separator b/w pairs of sentences\n",
        "                tokens += [sep_token]\n",
        "\n",
        "            segment_ids = [sequence_a_segment_id] * len(tokens)\n",
        "            tokens += option_tokens + [sep_token]\n",
        "\n",
        "            segment_ids += [sequence_b_segment_id] * (len(option_tokens) + 1)\n",
        "\n",
        "            if cls_token_at_end:\n",
        "                tokens = tokens + [cls_token]\n",
        "                segment_ids = segment_ids + [cls_token_segment_id]\n",
        "            else:\n",
        "                tokens = [cls_token] + tokens\n",
        "                segment_ids = [cls_token_segment_id] + segment_ids\n",
        "\n",
        "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "            # tokens are attended to.\n",
        "            input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "            # Zero-pad up to the sequence length.\n",
        "            padding_length = max_seq_length - len(input_ids)\n",
        "            if pad_on_left:\n",
        "                input_ids = ([pad_token] * padding_length) + input_ids\n",
        "                input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
        "                segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
        "            else:\n",
        "                input_ids = input_ids + ([pad_token] * padding_length)\n",
        "                input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
        "                segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n",
        "\n",
        "            assert len(input_ids) == max_seq_length\n",
        "            assert len(input_mask) == max_seq_length\n",
        "            assert len(segment_ids) == max_seq_length\n",
        "\n",
        "            if output_mode != \"multiple_choice\":\n",
        "                raise KeyError(output_mode)\n",
        "\n",
        "            option_features.append((tokens, input_ids, input_mask, segment_ids))\n",
        "\n",
        "        label_id = label_map[example.label]\n",
        "\n",
        "        if ex_index < 5:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(f\"example_id: {example.guid}\")\n",
        "            for choice_idx, (tokens, input_ids, input_mask, segment_ids) in enumerate(\n",
        "                    option_features):\n",
        "                logger.info(f\"choice: {choice_idx}\")\n",
        "                logger.info(f\"tokens: {' '.join(tokens)}\")\n",
        "                logger.info(f\"input_ids: {' '.join(map(str, input_ids))}\")\n",
        "                logger.info(f\"input_mask: {' '.join(map(str, input_mask))}\")\n",
        "                logger.info(f\"segment_ids: {' '.join(map(str, segment_ids))}\")\n",
        "            logger.info(f\"label: {label_id}\")\n",
        "\n",
        "        features.append(\n",
        "            MultipleChoiceFeatures(\n",
        "                example_id=example.guid,\n",
        "                option_features=option_features,\n",
        "                label=label_id\n",
        "            )\n",
        "        )\n",
        "    return features\n",
        "\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()\n",
        "\n",
        "\n",
        "def simple_accuracy(preds, labels):\n",
        "    return (preds == labels).mean()\n",
        "\n",
        "\n",
        "def acc_and_f1(preds, labels):\n",
        "    acc = simple_accuracy(preds, labels)\n",
        "    f1 = f1_score(y_true=labels, y_pred=preds)\n",
        "    return {\n",
        "        \"acc\": acc,\n",
        "        \"f1\": f1,\n",
        "        \"acc_and_f1\": (acc + f1) / 2,\n",
        "    }\n",
        "\n",
        "\n",
        "def pearson_and_spearman(preds, labels):\n",
        "    pearson_corr = pearsonr(preds, labels)[0]\n",
        "    spearman_corr = spearmanr(preds, labels)[0]\n",
        "    return {\n",
        "        \"pearson\": pearson_corr,\n",
        "        \"spearmanr\": spearman_corr,\n",
        "        \"corr\": (pearson_corr + spearman_corr) / 2,\n",
        "    }\n",
        "\n",
        "\n",
        "def compute_metrics(task_name, preds, labels):\n",
        "    assert len(preds) == len(labels)\n",
        "    if task_name == \"winogrande\":\n",
        "        return {\"acc\": simple_accuracy(preds, labels)}\n",
        "    elif task_name == \"superglue-wsc\":\n",
        "        return {\"acc\": simple_accuracy(preds, labels)}\n",
        "    else:\n",
        "        raise KeyError(task_name)\n",
        "\n",
        "processors = {\n",
        "    \"winogrande\": WinograndeProcessor,\n",
        "    \"superglue-wsc\": SuperGlueWscProcessor,\n",
        "}\n",
        "\n",
        "output_modes = {\n",
        "    \"winogrande\": \"multiple_choice\",\n",
        "    \"superglue-wsc\": \"classification\",\n",
        "}\n",
        "\n",
        "GLUE_TASKS_NUM_LABELS = {\n",
        "    \"winogrande\": 2,\n",
        "    \"superglue-wsc\": 2,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Altusc_p5dxL",
        "colab_type": "text"
      },
      "source": [
        "robert_mc.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2LzYR515fme",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RobertaForMultipleChoice(BertPreTrainedModel):\n",
        "    config_class = RobertaConfig\n",
        "    pretrained_model_archive_map = ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP\n",
        "    base_model_prefix = \"roberta\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(RobertaForMultipleChoice, self).__init__(config)\n",
        "\n",
        "        self.roberta = RobertaModel(config)\n",
        "        self.classifier = RobertaClassificationHead(config)\n",
        "\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None,\n",
        "                position_ids=None, head_mask=None):\n",
        "        num_choices = input_ids.shape[1]\n",
        "\n",
        "        flat_input_ids = input_ids.view(-1, input_ids.size(-1))\n",
        "        # flat_position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n",
        "        # flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n",
        "        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n",
        "        outputs = self.roberta(flat_input_ids, attention_mask=flat_attention_mask)\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        logits = self.classifier(sequence_output)\n",
        "        reshaped_logits = logits.view(-1, num_choices)\n",
        "\n",
        "        outputs = (reshaped_logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(reshaped_logits, labels)\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), reshaped_logits, (hidden_states), (attentions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PyTUFj36KLX",
        "colab_type": "text"
      },
      "source": [
        "run_experiment.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF1mzvCUD230",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) for conf in (BertConfig, XLNetConfig, XLMConfig, RobertaConfig)), ())\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
        "    'bert_mc': (BertConfig, BertForMultipleChoice, BertTokenizer),\n",
        "    'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
        "    'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
        "    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),\n",
        "    'roberta_mc': (RobertaConfig, RobertaForMultipleChoice, RobertaTokenizer),\n",
        "}\n",
        "\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def train(args, train_dataset, model, tokenizer):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    set_seed(args)  # Added here for reproductibility (even between python 2 and 3)\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer = SummaryWriter()\n",
        "\n",
        "    processor = processors[args.task_name]()\n",
        "\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    if args.warmup_pct is None:\n",
        "        scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n",
        "    else:\n",
        "        scheduler = WarmupLinearSchedule(optimizer, warmup_steps=math.floor(args.warmup_pct*t_total), t_total=t_total)\n",
        "\n",
        "    if args.fp16:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "\n",
        "    # multi-gpu training (should be after apex fp16 initialization)\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Distributed training (should be after apex fp16 initialization)\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
        "                                                          output_device=args.local_rank,\n",
        "                                                          find_unused_parameters=True)\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    logger.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "                args.train_batch_size * args.gradient_accumulation_steps * (torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 0\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0])\n",
        "    # set_seed(args)  # Added here for reproductibility (even between python 2 and 3)\n",
        "    for _ in train_iterator:\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0], mininterval=10, ncols=100)\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            model.train()\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "            inputs = {'input_ids':      batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'token_type_ids': batch[2] if args.model_type in ['bert', 'xlnet', 'bert_mc'] else None,  # XLM don't use segment_ids\n",
        "                      'labels':         batch[3]}\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\n",
        "\n",
        "            if args.n_gpu > 1:\n",
        "                loss = loss.mean() # mean() to average on multi-gpu parallel training\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "            else:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    # Log metrics\n",
        "                    if args.local_rank == -1 and args.evaluate_during_training:  # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                        results = evaluate(args, model, tokenizer, processor, eval_split=\"dev\")\n",
        "                        for key, value in results.items():\n",
        "                            tb_writer.add_scalar('eval_{}'.format(key), value, global_step)\n",
        "                    tb_writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
        "                    tb_writer.add_scalar('loss', (tr_loss - logging_loss)/args.logging_steps, global_step)\n",
        "                    logging_loss = tr_loss\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "                    # Save model checkpoint\n",
        "                    output_dir = os.path.join(args.output_dir, 'checkpoint-{}'.format(global_step))\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "                    model_to_save.save_pretrained(output_dir)\n",
        "                    torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
        "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "            if args.max_steps > 0 and global_step > args.max_steps:\n",
        "                epoch_iterator.close()\n",
        "                break\n",
        "        if args.max_steps > 0 and global_step > args.max_steps:\n",
        "            train_iterator.close()\n",
        "            break\n",
        "\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer.close()\n",
        "\n",
        "    return global_step, tr_loss / global_step\n",
        "\n",
        "\n",
        "def evaluate(args, model, tokenizer, processor, prefix=\"\", eval_split=None):\n",
        "\n",
        "    eval_task_names = (args.task_name,)\n",
        "    eval_outputs_dirs = (args.output_dir,)\n",
        "\n",
        "    assert eval_split is not None\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    if os.path.exists(os.path.join(args.output_dir, \"metrics.json\")):\n",
        "        with open(os.path.join(args.output_dir, \"metrics.json\"), \"r\") as f:\n",
        "            existing_results = json.loads(f.read())\n",
        "        f.close()\n",
        "        results.update(existing_results)\n",
        "\n",
        "    for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):\n",
        "        eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True, eval_split=eval_split)\n",
        "\n",
        "        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
        "            os.makedirs(eval_output_dir)\n",
        "\n",
        "        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "        # Note that DistributedSampler samples randomly\n",
        "        eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
        "        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "        # Eval!\n",
        "        logger.info(\"***** Running evaluation {} on {} *****\".format(prefix, eval_split))\n",
        "        logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "        eval_loss = 0.0\n",
        "        nb_eval_steps = 0\n",
        "        preds = None\n",
        "        out_label_ids = None\n",
        "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\", mininterval=10, ncols=100):\n",
        "            model.eval()\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                inputs = {'input_ids':      batch[0],\n",
        "                          'attention_mask': batch[1],\n",
        "                          'token_type_ids': batch[2] if args.model_type in ['bert', 'xlnet', 'bert_mc'] else None,  # XLM don't use segment_ids\n",
        "                          'labels':         batch[3]}\n",
        "                outputs = model(**inputs)\n",
        "                tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "                eval_loss += tmp_eval_loss.mean().item()\n",
        "            nb_eval_steps += 1\n",
        "            if preds is None:\n",
        "                preds = logits.detach().cpu().numpy()\n",
        "                if not eval_split == \"test\":\n",
        "                    out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
        "            else:\n",
        "                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "                if not eval_split == \"test\":\n",
        "                    out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
        "\n",
        "        eval_loss = eval_loss / nb_eval_steps\n",
        "        if args.output_mode == \"classification\" or args.output_mode == \"multiple_choice\":\n",
        "            preds = np.argmax(preds, axis=1)\n",
        "        elif args.output_mode == \"regression\":\n",
        "            preds = np.squeeze(preds)\n",
        "\n",
        "        if not eval_split == \"test\":\n",
        "            result = compute_metrics(eval_task, preds, out_label_ids)\n",
        "            result_split = {}\n",
        "            for k, v in result.items():\n",
        "                result_split[k + \"_{}\".format(eval_split)] = v\n",
        "            results.update(result_split)\n",
        "\n",
        "            output_eval_file = os.path.join(eval_output_dir, \"eval_results_{}.txt\".format(eval_split))\n",
        "            with open(output_eval_file, \"w\") as writer:\n",
        "                logger.info(\"***** Eval results {} on {} *****\".format(prefix, eval_split))\n",
        "                for key in sorted(result_split.keys()):\n",
        "                    logger.info(\"  %s = %s\", key, str(result_split[key]))\n",
        "                    writer.write(\"%s = %s\\n\" % (key, str(result_split[key])))\n",
        "\n",
        "        # predictions\n",
        "        output_pred_file = os.path.join(eval_output_dir, \"predictions_{}.lst\".format(eval_split))\n",
        "        with open(output_pred_file, \"w\") as writer:\n",
        "            logger.info(\"***** Write predictions {} on {} *****\".format(prefix, eval_split))\n",
        "            for pred in preds:\n",
        "                writer.write(\"{}\\n\".format(processor.get_labels()[pred]))\n",
        "\n",
        "        if os.path.exists(args.output_dir):\n",
        "            with open(os.path.join(args.output_dir, \"metrics.json\"), \"w\") as f:\n",
        "                f.write(json.dumps(results))\n",
        "            f.close()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def load_and_cache_examples(args, task, tokenizer, evaluate=False, eval_split=\"train\"):\n",
        "    processor = processors[task]()\n",
        "    output_mode = output_modes[task]\n",
        "    # Load data features from cache or dataset file\n",
        "    if args.data_cache_dir is None:\n",
        "        data_cache_dir = args.data_dir\n",
        "    else:\n",
        "        data_cache_dir = args.data_cache_dir\n",
        "\n",
        "    cached_features_file = os.path.join(data_cache_dir, 'cached_{}_{}_{}_{}'.format(\n",
        "        eval_split,\n",
        "        list(filter(None, args.model_name_or_path.split('/'))).pop(),\n",
        "        str(args.max_seq_length),\n",
        "        str(task)))\n",
        "\n",
        "    if os.path.exists(cached_features_file):\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features = torch.load(cached_features_file)\n",
        "    else:\n",
        "        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
        "        label_list = processor.get_labels()\n",
        "\n",
        "        if eval_split == \"train\":\n",
        "            examples = processor.get_train_examples(args.data_dir)\n",
        "        elif eval_split == \"dev\":\n",
        "            examples = processor.get_dev_examples(args.data_dir)\n",
        "        elif eval_split == \"test\":\n",
        "            examples = processor.get_test_examples(args.data_dir)\n",
        "        else:\n",
        "            raise Exception(\"eval_split should be among train / dev / test\")\n",
        "\n",
        "        if output_mode == \"multiple_choice\":\n",
        "            features = convert_multiple_choice_examples_to_features(\n",
        "                examples, label_list, args.max_seq_length, tokenizer, output_mode,\n",
        "                cls_token_at_end=bool(args.model_type in ['xlnet']),            # xlnet has a cls token at the end\n",
        "                cls_token=tokenizer.cls_token,\n",
        "                sep_token=tokenizer.sep_token,\n",
        "                sep_token_extra=bool(args.model_type in ['roberta', \"roberta_mc\"]),\n",
        "                cls_token_segment_id=2 if args.model_type in ['xlnet'] else 0,\n",
        "                pad_on_left=bool(args.model_type in ['xlnet']),                 # pad on the left for xlnet\n",
        "                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
        "                pad_token_segment_id=4 if args.model_type in ['xlnet'] else 0\n",
        "            )\n",
        "        else:\n",
        "            features = convert_examples_to_features(examples, label_list, args.max_seq_length, tokenizer, output_mode,\n",
        "                                                    cls_token_at_end=bool(args.model_type in ['xlnet']),            # xlnet has a cls token at the end\n",
        "                                                    cls_token=tokenizer.cls_token,\n",
        "                                                    sep_token=tokenizer.sep_token,\n",
        "                                                    sep_token_extra=bool(args.model_type in ['roberta', \"roberta_mc\"]),\n",
        "                                                    cls_token_segment_id=2 if args.model_type in ['xlnet'] else 0,\n",
        "                                                    pad_on_left=bool(args.model_type in ['xlnet']),                 # pad on the left for xlnet\n",
        "                                                    pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
        "                                                    pad_token_segment_id=4 if args.model_type in ['xlnet'] else 0)\n",
        "\n",
        "    if output_mode == \"multiple_choice\":\n",
        "        def _select_field(features, field):\n",
        "            return [\n",
        "                [\n",
        "                    choice[field]\n",
        "                    for choice in feature.choices_features\n",
        "                ]\n",
        "                for feature in features\n",
        "            ]\n",
        "\n",
        "        all_input_ids = torch.tensor(_select_field(features, 'input_ids'), dtype=torch.long)\n",
        "        all_input_mask = torch.tensor(_select_field(features, 'input_mask'), dtype=torch.long)\n",
        "        all_segment_ids = torch.tensor(_select_field(features, 'segment_ids'), dtype=torch.long)\n",
        "        all_label_ids = torch.tensor([f.label for f in features], dtype=torch.long)\n",
        "    else:\n",
        "        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "        all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "        all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
        "        if output_mode == \"classification\":\n",
        "            all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
        "        elif output_mode == \"regression\":\n",
        "            all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.float)\n",
        "\n",
        "        if args.local_rank in [-1, 0]:\n",
        "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "            if args.data_cache_dir is not None:\n",
        "                pathlib.Path(args.data_cache_dir).mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(features, cached_features_file)\n",
        "\n",
        "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1WFf8nS8MC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Required parameters\n",
        "\n",
        "class args:\n",
        "  def __init__(self):\n",
        "    self.data_dir = './data'\n",
        "    self.model_type = 'roberta_mc'\n",
        "    self.model_name_or_path = 'roberta-large'\n",
        "    self.task_name = 'winogrande'\n",
        "    self.output_dir = './output/models/'\n",
        "    self.data_cache_dir = './output/cache/'\n",
        "    self.data_size = 's'\n",
        "\n",
        "    self.config_name = ''\n",
        "    self.tokenizer_name = ''\n",
        "    self.cache_dir = ''\n",
        "    self.max_seq_length = 80 # 128\n",
        "    self.do_train = True\n",
        "    self.do_eval = True\n",
        "    self.do_prediction = False\n",
        "    self.evaluate_during_training = True\n",
        "    self.do_lower_case = True\n",
        "    self.run_on_test = False\n",
        "    self.per_gpu_train_batch_size = 2\n",
        "    self.per_gpu_eval_batch_size = 2\n",
        "    self.gradient_accumulation_steps = 4\n",
        "    self.learning_rate = 1e-5\n",
        "    self.weight_decay = 0.0\n",
        "    self.adam_epsilon = 1e-8\n",
        "    self.max_grad_norm = 1.0\n",
        "    self.num_train_epochs = 3\n",
        "    self.max_steps = -1\n",
        "    self.warmup_steps = 0\n",
        "    self.warmup_pct = 0.1\n",
        "    self.logging_steps = 500\n",
        "    self.save_steps = 500\n",
        "    self.eval_all_checkpoints = False\n",
        "    self.no_cuda = False\n",
        "    self.overwrite_output_dir = True\n",
        "    self.overwrite_cache = False\n",
        "    self.seed = 42\n",
        "    self.fp16 = False\n",
        "    self.fp16_opt_level = 'O1'\n",
        "    self.local_rank = -1\n",
        "    self.server_ip = ''\n",
        "    self.server_post = ''\n",
        "\n",
        "args = args()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyCI3ZbV-dUO",
        "colab_type": "code",
        "outputId": "d5a9f4d6-29e4-423f-a3e5-728a4c4ee138",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!rm -rf output runs\n",
        "\n",
        "if args.data_size:\n",
        "  os.system('cp ./data/train_%s.jsonl ./data/train.jsonl' % (args.data_size))\n",
        "\n",
        "if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and not args.overwrite_output_dir:\n",
        "    raise ValueError(\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(args.output_dir))\n",
        "\n",
        "# Setup distant debugging if needed\n",
        "if args.server_ip and args.server_port:\n",
        "    # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
        "    import ptvsd\n",
        "    print(\"Waiting for debugger attach\")\n",
        "    ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
        "    ptvsd.wait_for_attach()\n",
        "\n",
        "# Setup CUDA, GPU & distributed training\n",
        "if args.local_rank == -1 or args.no_cuda:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "    args.n_gpu = torch.cuda.device_count()\n",
        "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "    torch.cuda.set_device(args.local_rank)\n",
        "    device = torch.device(\"cuda\", args.local_rank)\n",
        "    torch.distributed.init_process_group(backend='nccl')\n",
        "    args.n_gpu = 1\n",
        "args.device = device\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
        "logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "                args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n",
        "\n",
        "# Set seed\n",
        "set_seed(args)\n",
        "\n",
        "# Prepare GLUE task\n",
        "args.task_name = args.task_name.lower()\n",
        "if args.task_name not in processors:\n",
        "    raise ValueError(\"Task not found: %s\" % (args.task_name))\n",
        "processor = processors[args.task_name]()\n",
        "args.output_mode = output_modes[args.task_name]\n",
        "label_list = processor.get_labels()\n",
        "num_labels = len(label_list)\n",
        "\n",
        "# Load pretrained model and tokenizer\n",
        "if args.local_rank not in [-1, 0]:\n",
        "    torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
        "\n",
        "args.model_type = args.model_type.lower()\n",
        "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
        "config = config_class.from_pretrained(\n",
        "    args.config_name if args.config_name else args.model_name_or_path,\n",
        "    num_labels=1 if args.model_type in [\"roberta_mc\"] else num_labels,\n",
        "    finetuning_task=args.task_name\n",
        ")\n",
        "tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path, do_lower_case=args.do_lower_case)\n",
        "model = model_class.from_pretrained(args.model_name_or_path, from_tf=bool('.ckpt' in args.model_name_or_path), config=config)\n",
        "\n",
        "if args.local_rank == 0:\n",
        "    torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
        "\n",
        "model.to(args.device)\n",
        "\n",
        "logger.info(\"Training/evaluation parameters %s\", args)\n",
        "\n",
        "# Prediction (on test set)\n",
        "if args.do_prediction:\n",
        "    results = {}\n",
        "    logger.info(\"Prediction on the test set (note: Training will not be executed.) \")\n",
        "    result = evaluate(args, model, tokenizer, processor, prefix=\"\", eval_split=\"test\")\n",
        "    result = dict((k, v) for k, v in result.items())\n",
        "    results.update(result)\n",
        "    logger.info(\"***** Experiment finished *****\")\n",
        "    print(results)\n",
        "\n",
        "# Training\n",
        "if args.do_train:\n",
        "    train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)\n",
        "    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
        "    logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
        "\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
        "    # Create output directory if needed\n",
        "    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n",
        "        os.makedirs(args.output_dir)\n",
        "\n",
        "    logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
        "    # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "    # They can then be reloaded using `from_pretrained()`\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "    model_to_save.save_pretrained(args.output_dir)\n",
        "    tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "    # Good practice: save your training arguments together with the trained model\n",
        "    # torch.save(args, os.path.join(args.output_dir, 'training_args.bin'))\n",
        "\n",
        "    # Load a trained model and vocabulary that you have fine-tuned\n",
        "    model = model_class.from_pretrained(args.output_dir)\n",
        "    tokenizer = tokenizer_class.from_pretrained(args.output_dir)\n",
        "    model.to(args.device)\n",
        "\n",
        "# Evaluation\n",
        "results = {}\n",
        "checkpoints = [args.output_dir]\n",
        "if args.do_eval and args.local_rank in [-1, 0]:\n",
        "    if args.eval_all_checkpoints:\n",
        "        checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True)))\n",
        "        logging.getLogger(\"pytorch_transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
        "    logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "    for checkpoint in checkpoints:\n",
        "        global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else \"\"\n",
        "        model = model_class.from_pretrained(checkpoint)\n",
        "        model.to(args.device)\n",
        "        result = evaluate(args, model, tokenizer, processor, prefix=global_step, eval_split=\"dev\")\n",
        "        result = dict((k + '_{}'.format(global_step), v) for k, v in result.items())\n",
        "        results.update(result)\n",
        "\n",
        "\n",
        "# Run on test\n",
        "if args.run_on_test and args.local_rank in [-1, 0]:\n",
        "    checkpoint = checkpoints[0]\n",
        "    global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else \"\"\n",
        "    model = model_class.from_pretrained(checkpoint)\n",
        "    model.to(args.device)\n",
        "    result = evaluate(args, model, tokenizer, processor, prefix=global_step, eval_split=\"test\")\n",
        "    result = dict((k + '_{}'.format(global_step), v) for k, v in result.items())\n",
        "    results.update(result)\n",
        "\n",
        "logger.info(\"***** Experiment finished *****\")\n",
        "\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "04/10/2020 09:44:02 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/10/2020 09:44:03 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json from cache at /root/.cache/torch/pytorch_transformers/c22e0b5bbb7c0cb93a87a2ae01263ae715b4c18d692b1740ce72cacaa99ad184.b0c148f080e2f5eb0abadfa0a38793b5631ca093ac4321d8614d219229fdee2a\n",
            "04/10/2020 09:44:03 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"winogrande\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_labels\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/10/2020 09:44:05 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json from cache at /root/.cache/torch/pytorch_transformers/1ae1f5b6e2b22b25ccc04c000bb79ca847aa226d0761536b011cf7e5868f0655.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "04/10/2020 09:44:05 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt from cache at /root/.cache/torch/pytorch_transformers/f8f83199a6270d582d6245dc100e99c4155de81c9745c6248077018fe01abcfb.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "04/10/2020 09:44:06 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/195c00f28dc68ef13a307c6db84d566f801f03b2b6bcf8b29524f10f767fac2a.fc7abf72755ecc4a75d0d336a93c1c63358d2334f5998ed326f3b0da380bf536\n",
            "04/10/2020 09:44:17 - INFO - pytorch_transformers.modeling_utils -   Weights of RobertaForMultipleChoice not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "04/10/2020 09:44:17 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in RobertaForMultipleChoice: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   Training/evaluation parameters <__main__.args object at 0x7f3e49cd9b00>\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   Creating features from dataset file at ./data\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   Writing example 0 of 640\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   *** Example ***\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   example_id: 3QHITW7OYO7Q6B6ISU2UMJB84ZLAQE-2\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   choice: 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   tokens: <s> Ian Ġvolunteered Ġto Ġeat ĠDennis 's Ġmen udo Ġafter Ġalready Ġhaving Ġa Ġbowl Ġbecause </s> </s> Ian Ġdespised Ġeating Ġintestine . </s>\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   input_ids: 0 37733 20792 7 3529 8093 18 604 23259 71 416 519 10 5749 142 2 2 37733 43396 4441 41727 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   choice: 1\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   tokens: <s> Ian Ġvolunteered Ġto Ġeat ĠDennis 's Ġmen udo Ġafter Ġalready Ġhaving Ġa Ġbowl Ġbecause </s> </s> D ennis Ġdespised Ġeating Ġintestine . </s>\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   input_ids: 0 37733 20792 7 3529 8093 18 604 23259 71 416 519 10 5749 142 2 2 495 28828 43396 4441 41727 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   label: 1\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   *** Example ***\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   example_id: 3QHITW7OYO7Q6B6ISU2UMJB84ZLAQE-1\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   choice: 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   tokens: <s> Ian Ġvolunteered Ġto Ġeat ĠDennis 's Ġmen udo Ġafter Ġalready Ġhaving Ġa Ġbowl Ġbecause </s> </s> Ian Ġenjoyed Ġeating Ġintestine . </s>\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   input_ids: 0 37733 20792 7 3529 8093 18 604 23259 71 416 519 10 5749 142 2 2 37733 3776 4441 41727 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   choice: 1\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   tokens: <s> Ian Ġvolunteered Ġto Ġeat ĠDennis 's Ġmen udo Ġafter Ġalready Ġhaving Ġa Ġbowl Ġbecause </s> </s> D ennis Ġenjoyed Ġeating Ġintestine . </s>\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   input_ids: 0 37733 20792 7 3529 8093 18 604 23259 71 416 519 10 5749 142 2 2 495 28828 3776 4441 41727 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   label: 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   *** Example ***\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   example_id: 3XWUWJ18TLO2DDRXF83QWLKRJ29UU4-1\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   choice: 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   tokens: <s> He Ġnever Ġcomes Ġto Ġmy Ġhome , Ġbut ĠI Ġalways Ġgo Ġto Ġhis Ġhouse Ġbecause Ġthe </s> </s> home Ġis Ġsmaller . </s>\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   input_ids: 0 894 393 606 7 127 184 6 53 38 460 213 7 39 790 142 5 2 2 8361 16 2735 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   choice: 1\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   tokens: <s> He Ġnever Ġcomes Ġto Ġmy Ġhome , Ġbut ĠI Ġalways Ġgo Ġto Ġhis Ġhouse Ġbecause Ġthe </s> </s> house Ġis Ġsmaller . </s>\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   input_ids: 0 894 393 606 7 127 184 6 53 38 460 213 7 39 790 142 5 2 2 3138 16 2735 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   label: 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   *** Example ***\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   example_id: 3XWUWJ18TLO2DDRXF83QWLKRJ29UU4-2\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   choice: 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   tokens: <s> He Ġnever Ġcomes Ġto Ġmy Ġhome , Ġbut ĠI Ġalways Ġgo Ġto Ġhis Ġhouse Ġbecause Ġthe </s> </s> home Ġis Ġbigger . </s>\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   input_ids: 0 894 393 606 7 127 184 6 53 38 460 213 7 39 790 142 5 2 2 8361 16 2671 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   choice: 1\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   tokens: <s> He Ġnever Ġcomes Ġto Ġmy Ġhome , Ġbut ĠI Ġalways Ġgo Ġto Ġhis Ġhouse Ġbecause Ġthe </s> </s> house Ġis Ġbigger . </s>\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   input_ids: 0 894 393 606 7 127 184 6 53 38 460 213 7 39 790 142 5 2 2 3138 16 2671 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   label: 1\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   *** Example ***\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   example_id: 3D5G8J4N5CI2K40F4RZLF9OG2CKVTH-2\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   choice: 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   tokens: <s> Kyle Ġdoesn 't Ġwear Ġleg Ġwarm ers Ġto Ġbed , Ġwhile ĠLogan Ġalmost Ġalways Ġdoes . </s> </s> Kyle Ġis Ġmore Ġlikely Ġto Ġlive Ġin Ġa Ġcolder Ġclimate . </s>\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   input_ids: 0 37468 630 75 3568 2985 3279 268 7 3267 6 150 9220 818 460 473 4 2 2 37468 16 55 533 7 697 11 10 19727 2147 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   choice: 1\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   tokens: <s> Kyle Ġdoesn 't Ġwear Ġleg Ġwarm ers Ġto Ġbed , Ġwhile ĠLogan Ġalmost Ġalways Ġdoes . </s> </s> L ogan Ġis Ġmore Ġlikely Ġto Ġlive Ġin Ġa Ġcolder Ġclimate . </s>\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   input_ids: 0 37468 630 75 3568 2985 3279 268 7 3267 6 150 9220 818 460 473 4 2 2 574 17302 16 55 533 7 697 11 10 19727 2147 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   label: 1\n",
            "04/10/2020 09:44:21 - INFO - __main__ -   ***** Running training *****\n",
            "04/10/2020 09:44:21 - INFO - __main__ -     Num examples = 640\n",
            "04/10/2020 09:44:21 - INFO - __main__ -     Num Epochs = 3\n",
            "04/10/2020 09:44:21 - INFO - __main__ -     Instantaneous batch size per GPU = 2\n",
            "04/10/2020 09:44:21 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "04/10/2020 09:44:21 - INFO - __main__ -     Gradient Accumulation steps = 4\n",
            "04/10/2020 09:44:21 - INFO - __main__ -     Total optimization steps = 240\n",
            "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0%|                                                            | 0/320 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   9%|████▍                                              | 28/320 [00:10<01:48,  2.70it/s]\u001b[A\n",
            "Iteration:  18%|████████▉                                          | 56/320 [00:20<01:37,  2.70it/s]\u001b[A\n",
            "Iteration:  26%|█████████████▍                                     | 84/320 [00:31<01:27,  2.70it/s]\u001b[A\n",
            "Iteration:  35%|█████████████████▌                                | 112/320 [00:41<01:17,  2.70it/s]\u001b[A\n",
            "Iteration:  44%|█████████████████████▉                            | 140/320 [00:51<01:06,  2.70it/s]\u001b[A\n",
            "Iteration:  52%|██████████████████████████▎                       | 168/320 [01:02<00:56,  2.70it/s]\u001b[A\n",
            "Iteration:  61%|██████████████████████████████▋                   | 196/320 [01:12<00:46,  2.70it/s]\u001b[A\n",
            "Iteration:  70%|███████████████████████████████████               | 224/320 [01:23<00:35,  2.69it/s]\u001b[A\n",
            "Iteration:  79%|███████████████████████████████████████▍          | 252/320 [01:33<00:25,  2.69it/s]\u001b[A\n",
            "Iteration:  88%|███████████████████████████████████████████▊      | 280/320 [01:43<00:14,  2.69it/s]\u001b[A\n",
            "Iteration: 100%|██████████████████████████████████████████████████| 320/320 [01:58<00:00,  2.69it/s]\n",
            "Epoch:  33%|███▎      | 1/3 [01:58<03:57, 118.95s/it]\n",
            "Iteration:   0%|                                                            | 0/320 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   9%|████▍                                              | 28/320 [00:10<01:49,  2.67it/s]\u001b[A\n",
            "Iteration:  17%|████████▊                                          | 55/320 [00:20<01:39,  2.67it/s]\u001b[A\n",
            "Iteration:  26%|█████████████                                      | 82/320 [00:30<01:29,  2.66it/s]\u001b[A\n",
            "Iteration:  34%|█████████████████                                 | 109/320 [00:40<01:19,  2.66it/s]\u001b[A\n",
            "Iteration:  42%|█████████████████████▎                            | 136/320 [00:51<01:09,  2.66it/s]\u001b[A\n",
            "Iteration:  42%|█████████████████████▎                            | 136/320 [01:01<01:09,  2.66it/s]\u001b[A\n",
            "Iteration:  51%|█████████████████████████▌                        | 164/320 [01:01<00:58,  2.67it/s]\u001b[A\n",
            "Iteration:  60%|██████████████████████████████                    | 192/320 [01:11<00:47,  2.67it/s]\u001b[A\n",
            "Iteration:  68%|██████████████████████████████████▏               | 219/320 [01:22<00:37,  2.68it/s]\u001b[A\n",
            "Iteration:  77%|██████████████████████████████████████▍           | 246/320 [01:32<00:27,  2.67it/s]\u001b[A\n",
            "Iteration:  85%|██████████████████████████████████████████▋       | 273/320 [01:42<00:17,  2.66it/s]\u001b[A\n",
            "Iteration: 100%|██████████████████████████████████████████████████| 320/320 [01:59<00:00,  2.67it/s]\n",
            "Epoch:  67%|██████▋   | 2/3 [03:58<01:59, 119.26s/it]\n",
            "Iteration:   0%|                                                            | 0/320 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   9%|████▍                                              | 28/320 [00:10<01:49,  2.67it/s]\u001b[A\n",
            "Iteration:  18%|████████▉                                          | 56/320 [00:20<01:38,  2.67it/s]\u001b[A\n",
            "Iteration:  18%|████████▉                                          | 56/320 [00:31<01:38,  2.67it/s]\u001b[A\n",
            "Iteration:  26%|█████████████▍                                     | 84/320 [00:31<01:28,  2.67it/s]\u001b[A\n",
            "Iteration:  35%|█████████████████▌                                | 112/320 [00:41<01:17,  2.67it/s]\u001b[A\n",
            "Iteration:  44%|█████████████████████▉                            | 140/320 [00:52<01:07,  2.67it/s]\u001b[A\n",
            "Iteration:  52%|██████████████████████████▎                       | 168/320 [01:02<00:56,  2.67it/s]\u001b[A\n",
            "Iteration:  61%|██████████████████████████████▋                   | 196/320 [01:13<00:46,  2.67it/s]\u001b[A\n",
            "Iteration:  70%|██████████████████████████████████▊               | 223/320 [01:23<00:36,  2.68it/s]\u001b[A\n",
            "Iteration:  78%|███████████████████████████████████████           | 250/320 [01:33<00:26,  2.67it/s]\u001b[A\n",
            "Iteration:  87%|███████████████████████████████████████████▎      | 277/320 [01:43<00:16,  2.67it/s]\u001b[A\n",
            "Iteration: 100%|██████████████████████████████████████████████████| 320/320 [01:59<00:00,  2.67it/s]\n",
            "Epoch: 100%|██████████| 3/3 [05:58<00:00, 119.63s/it]\n",
            "04/10/2020 09:50:20 - INFO - __main__ -    global_step = 240, average loss = 0.6985214017642041\n",
            "04/10/2020 09:50:20 - INFO - __main__ -   Saving model checkpoint to ./output/models/\n",
            "04/10/2020 09:50:25 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ./output/models/config.json\n",
            "04/10/2020 09:50:25 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"winogrande\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_labels\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/10/2020 09:50:25 - INFO - pytorch_transformers.modeling_utils -   loading weights file ./output/models/pytorch_model.bin\n",
            "04/10/2020 09:50:35 - INFO - pytorch_transformers.tokenization_utils -   Model name './output/models/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli). Assuming './output/models/' is a path or url to a directory containing tokenizer files.\n",
            "04/10/2020 09:50:35 - INFO - pytorch_transformers.tokenization_utils -   loading file ./output/models/vocab.json\n",
            "04/10/2020 09:50:35 - INFO - pytorch_transformers.tokenization_utils -   loading file ./output/models/merges.txt\n",
            "04/10/2020 09:50:35 - INFO - pytorch_transformers.tokenization_utils -   loading file ./output/models/added_tokens.json\n",
            "04/10/2020 09:50:35 - INFO - pytorch_transformers.tokenization_utils -   loading file ./output/models/special_tokens_map.json\n",
            "04/10/2020 09:50:36 - INFO - __main__ -   Evaluate the following checkpoints: ['./output/models/']\n",
            "04/10/2020 09:50:36 - INFO - pytorch_transformers.modeling_utils -   loading configuration file ./output/models/config.json\n",
            "04/10/2020 09:50:36 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"winogrande\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_labels\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/10/2020 09:50:36 - INFO - pytorch_transformers.modeling_utils -   loading weights file ./output/models/pytorch_model.bin\n",
            "04/10/2020 09:50:47 - INFO - __main__ -   Creating features from dataset file at ./data\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   Writing example 0 of 1267\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   *** Example ***\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   example_id: 3FCO4VKOZ4BJQ6IFC0VAIBK4KTWE7U-2\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   choice: 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   tokens: <s> Sarah Ġwas Ġa Ġmuch Ġbetter Ġsurgeon Ġthan ĠMaria Ġso </s> </s> Sarah Ġalways Ġgot Ġthe Ġeasier Ġcases . </s>\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   input_ids: 0 33671 21 10 203 357 16308 87 5011 98 2 2 33671 460 300 5 3013 1200 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   choice: 1\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   tokens: <s> Sarah Ġwas Ġa Ġmuch Ġbetter Ġsurgeon Ġthan ĠMaria Ġso </s> </s> Maria Ġalways Ġgot Ġthe Ġeasier Ġcases . </s>\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   input_ids: 0 33671 21 10 203 357 16308 87 5011 98 2 2 38517 460 300 5 3013 1200 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   label: 1\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   *** Example ***\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   example_id: 3FCO4VKOZ4BJQ6IFC0VAIBK4KTWE7U-1\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   choice: 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   tokens: <s> Sarah Ġwas Ġa Ġmuch Ġbetter Ġsurgeon Ġthan ĠMaria Ġso </s> </s> Sarah Ġalways Ġgot Ġthe Ġharder Ġcases . </s>\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   input_ids: 0 33671 21 10 203 357 16308 87 5011 98 2 2 33671 460 300 5 4851 1200 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   choice: 1\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   tokens: <s> Sarah Ġwas Ġa Ġmuch Ġbetter Ġsurgeon Ġthan ĠMaria Ġso </s> </s> Maria Ġalways Ġgot Ġthe Ġharder Ġcases . </s>\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   input_ids: 0 33671 21 10 203 357 16308 87 5011 98 2 2 38517 460 300 5 4851 1200 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   label: 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   *** Example ***\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   example_id: 3WUVMVA7ODHEES6GZOX75ABL4KQZAX-2\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   choice: 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   tokens: <s> They Ġwere Ġworried Ġthe Ġwine Ġwould Ġruin Ġthe Ġbed Ġand Ġthe Ġblanket , Ġbut Ġthe </s> </s> blank et Ġwas 't Ġruined . </s>\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   input_ids: 0 1213 58 3915 5 3984 74 17948 5 3267 8 5 14165 6 53 5 2 2 35763 594 21 75 19750 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   choice: 1\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   tokens: <s> They Ġwere Ġworried Ġthe Ġwine Ġwould Ġruin Ġthe Ġbed Ġand Ġthe Ġblanket , Ġbut Ġthe </s> </s> bed Ġwas 't Ġruined . </s>\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   input_ids: 0 1213 58 3915 5 3984 74 17948 5 3267 8 5 14165 6 53 5 2 2 5134 21 75 19750 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   label: 1\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   *** Example ***\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   example_id: 3B623HUYJ643USRN7YJLDQ8NQH38S9-1\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   choice: 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   tokens: <s> Terry Ġtried Ġto Ġbake Ġthe Ġegg plant Ġin Ġthe Ġto aster Ġoven Ġbut Ġthe </s> </s> egg plant Ġwas Ġtoo Ġbig . </s>\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   input_ids: 0 43062 1381 7 24870 5 8380 19520 11 5 7 8831 12941 53 5 2 2 38299 19520 21 350 380 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   choice: 1\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   tokens: <s> Terry Ġtried Ġto Ġbake Ġthe Ġegg plant Ġin Ġthe Ġto aster Ġoven Ġbut Ġthe </s> </s> to aster Ġwas Ġtoo Ġbig . </s>\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   input_ids: 0 43062 1381 7 24870 5 8380 19520 11 5 7 8831 12941 53 5 2 2 560 8831 21 350 380 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   label: 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   *** Example ***\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   example_id: 3PN6H8C9R4OWH22DN8WAMK49Q4SADE-1\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   choice: 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   tokens: <s> At Ġnight , ĠJeffrey Ġalways Ġstays Ġup Ġlater Ġthan ĠHunter Ġto Ġwatch ĠTV Ġbecause </s> </s> Jeff rey Ġwakes Ġup Ġlate . </s>\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   input_ids: 0 3750 363 6 9011 460 10117 62 423 87 7126 7 1183 1012 142 2 2 19663 5460 34142 62 628 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   choice: 1\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   tokens: <s> At Ġnight , ĠJeffrey Ġalways Ġstays Ġup Ġlater Ġthan ĠHunter Ġto Ġwatch ĠTV Ġbecause </s> </s> Hunter Ġwakes Ġup Ġlate . </s>\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   input_ids: 0 3750 363 6 9011 460 10117 62 423 87 7126 7 1183 1012 142 2 2 44038 34142 62 628 4 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   label: 0\n",
            "04/10/2020 09:50:48 - INFO - __main__ -   ***** Running evaluation  on dev *****\n",
            "04/10/2020 09:50:48 - INFO - __main__ -     Num examples = 1267\n",
            "04/10/2020 09:50:48 - INFO - __main__ -     Batch size = 2\n",
            "Evaluating: 100%|█████████████████████████████████████████████████| 634/634 [00:54<00:00, 11.72it/s]\n",
            "04/10/2020 09:51:42 - INFO - __main__ -   ***** Eval results  on dev *****\n",
            "04/10/2020 09:51:42 - INFO - __main__ -     acc_dev = 0.489344909234412\n",
            "04/10/2020 09:51:42 - INFO - __main__ -   ***** Write predictions  on dev *****\n",
            "04/10/2020 09:51:42 - INFO - __main__ -   ***** Experiment finished *****\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'acc_dev_': 0.489344909234412}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}