{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "PytorchTut.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alanwuha/ce7455-nlp/blob/master/PytorchTut.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUwGO6zv5Zqh",
        "colab_type": "text"
      },
      "source": [
        "# Pytorch Basics\n",
        "\n",
        "### Table of contents\n",
        "\n",
        "  1. Pytorch tensors\n",
        "  1. Basic autograd example 1\n",
        "  1. Basic autograd example 2\n",
        "  1. Loading data from numpy\n",
        "  1. Input pipeline\n",
        "  1. Input pipeline for custom dataset\n",
        "  1. Pretrained model\n",
        "  1. Save and load model\n",
        "  1. Train a simple MNIST Neural nets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRY23ZEk6B7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RAgJcr26RZP",
        "colab_type": "text"
      },
      "source": [
        "## 0. Pytorch tensors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2osZsIm26XeE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "b43103bf-79e7-44b5-b606-5c7239f3bb22"
      },
      "source": [
        "# Very similar to numpy\n",
        "x = torch.tensor(1.)\n",
        "print(x)\n",
        "\n",
        "x = torch.tensor([1,2,3,4,5]).float()\n",
        "print(x)\n",
        "print(x.size())\n",
        "\n",
        "ran = torch.Tensor(2,5,5).uniform_()\n",
        "print(ran)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.)\n",
            "tensor([1., 2., 3., 4., 5.])\n",
            "torch.Size([5])\n",
            "tensor([[[0.3784, 0.6971, 0.0653, 0.0761, 0.8253],\n",
            "         [0.1678, 0.9569, 0.4510, 0.4550, 0.2804],\n",
            "         [0.8672, 0.9844, 0.3930, 0.6631, 0.8681],\n",
            "         [0.8364, 0.9617, 0.9616, 0.2598, 0.5760],\n",
            "         [0.5215, 0.3812, 0.7594, 0.8814, 0.2873]],\n",
            "\n",
            "        [[0.0022, 0.8071, 0.2068, 0.4082, 0.1680],\n",
            "         [0.1994, 0.1391, 0.4603, 0.6840, 0.5954],\n",
            "         [0.7994, 0.5741, 0.0375, 0.5115, 0.5344],\n",
            "         [0.6637, 0.2793, 0.0454, 0.7726, 0.7693],\n",
            "         [0.4179, 0.3746, 0.1589, 0.5850, 0.9851]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIR_MnQo69mh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "2e3b7987-e6a0-4c5c-f498-82d3a9cd8f1e"
      },
      "source": [
        "# min, max, mean, reshaping\n",
        "ran = torch.Tensor(2,5,5).uniform_()\n",
        "print(ran)\n",
        "print(ran.size())\n",
        "print(ran.min())\n",
        "print(ran.max())\n",
        "print(ran.mean())\n",
        "rsan = ran.view(5,2,5)\n",
        "print(rsan)\n",
        "print(rsan.size())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0.4780, 0.5704, 0.7053, 0.0277, 0.4454],\n",
            "         [0.4954, 0.9019, 0.3586, 0.2883, 0.8659],\n",
            "         [0.2419, 0.4684, 0.4270, 0.8000, 0.4375],\n",
            "         [0.6887, 0.9944, 0.8805, 0.3984, 0.6021],\n",
            "         [0.0193, 0.1095, 0.4963, 0.5356, 0.9173]],\n",
            "\n",
            "        [[0.8682, 0.2954, 0.9378, 0.7976, 0.8279],\n",
            "         [0.9022, 0.5430, 0.3870, 0.2027, 0.0483],\n",
            "         [0.1101, 0.9955, 0.2643, 0.6726, 0.9655],\n",
            "         [0.7967, 0.8202, 0.4733, 0.9776, 0.1404],\n",
            "         [0.1093, 0.2811, 0.9362, 0.0519, 0.7020]]])\n",
            "torch.Size([2, 5, 5])\n",
            "tensor(0.0193)\n",
            "tensor(0.9955)\n",
            "tensor(0.5452)\n",
            "tensor([[[0.4780, 0.5704, 0.7053, 0.0277, 0.4454],\n",
            "         [0.4954, 0.9019, 0.3586, 0.2883, 0.8659]],\n",
            "\n",
            "        [[0.2419, 0.4684, 0.4270, 0.8000, 0.4375],\n",
            "         [0.6887, 0.9944, 0.8805, 0.3984, 0.6021]],\n",
            "\n",
            "        [[0.0193, 0.1095, 0.4963, 0.5356, 0.9173],\n",
            "         [0.8682, 0.2954, 0.9378, 0.7976, 0.8279]],\n",
            "\n",
            "        [[0.9022, 0.5430, 0.3870, 0.2027, 0.0483],\n",
            "         [0.1101, 0.9955, 0.2643, 0.6726, 0.9655]],\n",
            "\n",
            "        [[0.7967, 0.8202, 0.4733, 0.9776, 0.1404],\n",
            "         [0.1093, 0.2811, 0.9362, 0.0519, 0.7020]]])\n",
            "torch.Size([5, 2, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9vl8Su-7eFl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "494d4efb-7bf6-432d-9416-3d5e0157077c"
      },
      "source": [
        "# Tensor math\n",
        "x = torch.Tensor(2, 4, 5).uniform_()\n",
        "y = torch.Tensor(2, 4, 5).uniform_()\n",
        "print(x.size())\n",
        "print(y.size())\n",
        "print(x)\n",
        "print(y)\n",
        "a = x + y\n",
        "m = x * y\n",
        "\n",
        "# transpose of y (the last 2 dimension)\n",
        "yt = y.transpose(1, 2)\n",
        "print(yt.size())\n",
        "matmul = torch.matmul(x, yt)\n",
        "print(matmul.size())\n",
        "print(matmul)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 4, 5])\n",
            "torch.Size([2, 4, 5])\n",
            "tensor([[[0.4393, 0.5021, 0.5974, 0.3014, 0.5098],\n",
            "         [0.8365, 0.2973, 0.6080, 0.0072, 0.7159],\n",
            "         [0.7777, 0.0919, 0.3391, 0.4479, 0.0889],\n",
            "         [0.6662, 0.7987, 0.8528, 0.5594, 0.9429]],\n",
            "\n",
            "        [[0.4225, 0.6012, 0.8973, 0.2096, 0.9488],\n",
            "         [0.3488, 0.6753, 0.0139, 0.4483, 0.0123],\n",
            "         [0.6800, 0.2383, 0.0751, 0.7912, 0.1911],\n",
            "         [0.5733, 0.6164, 0.3023, 0.8783, 0.8546]]])\n",
            "tensor([[[0.9117, 0.8325, 0.6987, 0.5551, 0.3136],\n",
            "         [0.0853, 0.1126, 0.3778, 0.9071, 0.4562],\n",
            "         [0.6096, 0.7952, 0.2501, 0.6239, 0.2696],\n",
            "         [0.1361, 0.4529, 0.3692, 0.4763, 0.9199]],\n",
            "\n",
            "        [[0.0748, 0.7551, 0.7669, 0.0061, 0.6848],\n",
            "         [0.7760, 0.1273, 0.9288, 0.0183, 0.9924],\n",
            "         [0.1788, 0.4243, 0.8222, 0.8170, 0.0348],\n",
            "         [0.6496, 0.8241, 0.5408, 0.0493, 0.3360]]])\n",
            "torch.Size([2, 5, 4])\n",
            "torch.Size([2, 4, 4])\n",
            "tensor([[[1.5631, 0.8257, 1.1421, 1.1203],\n",
            "         [1.6635, 0.6676, 1.0960, 1.1349],\n",
            "         [1.2990, 0.6517, 0.9355, 0.5678],\n",
            "         [2.4744, 1.4066, 1.8579, 1.9010]],\n",
            "\n",
            "        [[1.8247, 2.1833, 1.2727, 1.5843],\n",
            "         [0.5579, 0.3900, 0.7271, 0.8169],\n",
            "         [0.4241, 0.8319, 0.9375, 0.7820],\n",
            "         [1.3307, 1.6682, 1.3599, 1.3742]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TEULEMa8aN-",
        "colab_type": "text"
      },
      "source": [
        "## 1. Basic autograd example 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9voWYmwI8f7w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5acd462b-8ef0-4856-a6d6-3d69e93353db"
      },
      "source": [
        "# Create tensors.\n",
        "x = torch.tensor(1., requires_grad=True)\n",
        "w = torch.tensor(2., requires_grad=True)\n",
        "b = torch.tensor(3., requires_grad=True)\n",
        "\n",
        "# Build a computational graph.\n",
        "y = w * x + b # y = 2 * x + 3\n",
        "\n",
        "# Compute gradients.\n",
        "y.backward()\n",
        "\n",
        "# Print out the gradients.\n",
        "print(x.grad) # x.grad = 2\n",
        "print(w.grad) # w.grad = 1\n",
        "print(b.grad) # b.grad = 1"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.)\n",
            "tensor(1.)\n",
            "tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAlx3FaP84yk",
        "colab_type": "text"
      },
      "source": [
        "## 2. Basic autograd example 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRVAOcFp88bP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "1a10a864-b5af-41f9-a3bb-484379b662d4"
      },
      "source": [
        "# Create tensors of shape (10, 3) and (10, 2).\n",
        "x = torch.randn(10, 3)\n",
        "y = torch.randn(10, 2)\n",
        "\n",
        "# Build a fully connected layer.\n",
        "linear = nn.Linear(3, 2)\n",
        "print('w: ', linear.weight)\n",
        "print('b: ', linear.bias)\n",
        "\n",
        "# Build loss function and optimizer.\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
        "\n",
        "# Forward pass.\n",
        "pred = linear(x)\n",
        "\n",
        "# Compute loss.\n",
        "loss = criterion(pred, y)\n",
        "print('loss: ', loss.item())\n",
        "\n",
        "# Backward pass.\n",
        "loss.backward()\n",
        "\n",
        "# Print out the gradients.\n",
        "print('dL/dw: ', linear.weight.grad)\n",
        "print('dL/db: ', linear.bias.grad)\n",
        "\n",
        "# 1-step gradient descent.\n",
        "optimizer.step()\n",
        "\n",
        "# You can also perform gradient descent at the low level.\n",
        "# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
        "# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
        "\n",
        "# Print out the loss after 1-step gradient descent.\n",
        "pred = linear(x)\n",
        "loss = criterion(pred, y)\n",
        "print('loss after 1 step optimization: ', loss.item())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w:  Parameter containing:\n",
            "tensor([[ 0.3920, -0.3873, -0.0325],\n",
            "        [-0.5596, -0.3377,  0.3804]], requires_grad=True)\n",
            "b:  Parameter containing:\n",
            "tensor([-0.0812,  0.3334], requires_grad=True)\n",
            "loss:  1.8495737314224243\n",
            "dL/dw:  tensor([[ 0.4659, -0.5993, -0.3919],\n",
            "        [-0.5142, -0.5282,  0.9056]])\n",
            "dL/db:  tensor([-0.5098,  0.4889])\n",
            "loss after 1 step optimization:  1.823793649673462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUGKZhSM-u4S",
        "colab_type": "text"
      },
      "source": [
        "## 3. Loading data from numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-gyYRj_-xjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a numpy array.\n",
        "x = np.array([[1, 2], [3, 4]])\n",
        "\n",
        "# Convert the numpy array to a torch tensor.\n",
        "y = torch.from_numpy(x)\n",
        "\n",
        "# Convert the torch tensor to a numpy array.\n",
        "z = y.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n374ysuq-856",
        "colab_type": "text"
      },
      "source": [
        "## 4. Input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhCCgszN_OO1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "6d5a1700-40fb-4c46-9b85-b6dda4791567"
      },
      "source": [
        "# Download and construct CIFAR-10 dataset.\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='.',\n",
        "    train=True,\n",
        "    transform=transforms.ToTensor(),\n",
        "    download=True)\n",
        "\n",
        "# Fetch one data pair (read data from disk).\n",
        "image, label = train_dataset[0]\n",
        "print(len(train_dataset))\n",
        "print(train_dataset[0])\n",
        "print(type(train_dataset[0]))\n",
        "print(image.size())\n",
        "print(label)\n",
        "\n",
        "# Data loader (this provides queues and threads in a very simple way).\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset = train_dataset,\n",
        "    batch_size = 64,\n",
        "    shuffle = True\n",
        ")\n",
        "\n",
        "# When iteration starts, queue and thread start to load data from files.\n",
        "data_iter = iter(train_loader)\n",
        "\n",
        "# Mini-batch images and labels.\n",
        "images, labels = data_iter.next()\n",
        "\n",
        "# Actual usage of the data loader is as below.\n",
        "for images, labels in train_loader:\n",
        "  # Training code should be written here.\n",
        "  pass\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "50000\n",
            "(tensor([[[0.2314, 0.1686, 0.1961,  ..., 0.6196, 0.5961, 0.5804],\n",
            "         [0.0627, 0.0000, 0.0706,  ..., 0.4824, 0.4667, 0.4784],\n",
            "         [0.0980, 0.0627, 0.1922,  ..., 0.4627, 0.4706, 0.4275],\n",
            "         ...,\n",
            "         [0.8157, 0.7882, 0.7765,  ..., 0.6275, 0.2196, 0.2078],\n",
            "         [0.7059, 0.6784, 0.7294,  ..., 0.7216, 0.3804, 0.3255],\n",
            "         [0.6941, 0.6588, 0.7020,  ..., 0.8471, 0.5922, 0.4824]],\n",
            "\n",
            "        [[0.2431, 0.1804, 0.1882,  ..., 0.5176, 0.4902, 0.4863],\n",
            "         [0.0784, 0.0000, 0.0314,  ..., 0.3451, 0.3255, 0.3412],\n",
            "         [0.0941, 0.0275, 0.1059,  ..., 0.3294, 0.3294, 0.2863],\n",
            "         ...,\n",
            "         [0.6667, 0.6000, 0.6314,  ..., 0.5216, 0.1216, 0.1333],\n",
            "         [0.5451, 0.4824, 0.5647,  ..., 0.5804, 0.2431, 0.2078],\n",
            "         [0.5647, 0.5059, 0.5569,  ..., 0.7216, 0.4627, 0.3608]],\n",
            "\n",
            "        [[0.2471, 0.1765, 0.1686,  ..., 0.4235, 0.4000, 0.4039],\n",
            "         [0.0784, 0.0000, 0.0000,  ..., 0.2157, 0.1961, 0.2235],\n",
            "         [0.0824, 0.0000, 0.0314,  ..., 0.1961, 0.1961, 0.1647],\n",
            "         ...,\n",
            "         [0.3765, 0.1333, 0.1020,  ..., 0.2745, 0.0275, 0.0784],\n",
            "         [0.3765, 0.1647, 0.1176,  ..., 0.3686, 0.1333, 0.1333],\n",
            "         [0.4549, 0.3686, 0.3412,  ..., 0.5490, 0.3294, 0.2824]]]), 6)\n",
            "<class 'tuple'>\n",
            "torch.Size([3, 32, 32])\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6bdXRvHCbD9",
        "colab_type": "text"
      },
      "source": [
        "## 5. Input pipeline for custom dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdNSwcCTCj14",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You should build your custom dataset as below.\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self):\n",
        "    # TODO\n",
        "    # 1. Intialize file paths or a list of file names.\n",
        "    pass\n",
        "  def __getitem__(self, index):\n",
        "    # TODO\n",
        "    # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
        "    # 2. Preprocess the data (e.g. torchvision.Transform).\n",
        "    # 3. Return a data pair (e.g. image and label).\n",
        "    pass\n",
        "  def __len__(self):\n",
        "    # You should change 0 to the total size of your dataset.\n",
        "    return 0\n",
        "\n",
        "# You can then use the prebuilt data loader.\n",
        "custom_dataset = CustomDataset()\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset = custom_dataset,\n",
        "    batch_size = 64,\n",
        "    shuffle = True\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmHYxLBBGdvR",
        "colab_type": "text"
      },
      "source": [
        "## 6. Pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7t9VRb2GgWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download and load the pretrained ResNet-18.\n",
        "resnet = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "# If you want to finetune only the top layer of the model, set as below.\n",
        "for param in resnet.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# Replace the top layer for finetuning.\n",
        "resnet.fc = nn.Linear(resnet.fc.in_features, 100) # 100 is an example.\n",
        "\n",
        "# Forward pass.\n",
        "images = torch.randn(64, 3, 224, 224)\n",
        "output = resnet(images)\n",
        "print(outputs.size()) # (64, 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8b34d0hHFDs",
        "colab_type": "text"
      },
      "source": [
        "## 7. Save and load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdxZ2f7AHO1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save and load the entire model.\n",
        "torch.save(resnet, 'model.ckpt')\n",
        "model = torch.load('model.ckpt')\n",
        "\n",
        "# Save and load only the model parameters (recommended).\n",
        "torch.save(resnet.state_dict(), 'params.ckpt')\n",
        "resnet.load_state_dict(torch.load('params.ckpt'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V75E-TecIFJ0",
        "colab_type": "text"
      },
      "source": [
        "## 8. Train a simple MNIST Neural nets\n",
        "\n",
        "Let's train a simple neural network to classify MNIST hand-written digit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMjfFQCCJ5M1",
        "colab_type": "text"
      },
      "source": [
        "### 1. Download the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krcBSSKOJ73-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "b1d82b1a-a082-44bc-806f-f61ae8c4dd1a"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "input_size = 784\n",
        "hidden_size = 500\n",
        "num_classes = 10\n",
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='.',\n",
        "                                           train=True,\n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='.',\n",
        "                                          train=False,\n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/9912422 [00:00<?, ?it/s]\u001b[A\n",
            "  0%|          | 16384/9912422 [00:00<02:12, 74795.98it/s]\u001b[A\n",
            "  0%|          | 40960/9912422 [00:01<01:55, 85730.79it/s]\u001b[A\n",
            "  1%|          | 98304/9912422 [00:01<01:29, 109130.68it/s]\u001b[A\n",
            "  2%|▏         | 212992/9912422 [00:01<01:07, 144756.57it/s]\u001b[A\n",
            "  4%|▍         | 425984/9912422 [00:01<00:48, 196113.53it/s]\u001b[A\n",
            "  7%|▋         | 696320/9912422 [00:01<00:34, 264767.27it/s]\u001b[A\n",
            " 11%|█▏        | 1122304/9912422 [00:02<00:24, 360291.55it/s]\u001b[A\n",
            " 22%|██▏       | 2203648/9912422 [00:02<00:15, 501316.55it/s]\u001b[A\n",
            " 36%|███▌      | 3555328/9912422 [00:02<00:09, 695494.51it/s]\u001b[A\n",
            " 63%|██████▎   | 6291456/9912422 [00:02<00:03, 973714.60it/s]\u001b[A\n",
            " 95%|█████████▌| 9428992/9912422 [00:02<00:00, 1357246.64it/s]\u001b[A\n",
            "9920512it [00:02, 3583641.78it/s]                             \u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/28881 [00:00<?, ?it/s]\u001b[A\n",
            " 57%|█████▋    | 16384/28881 [00:00<00:00, 86051.87it/s]\u001b[A\n",
            "32768it [00:00, 57007.80it/s]                           \u001b[A\n",
            "0it [00:00, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1648877 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 16384/1648877 [00:00<00:21, 76523.75it/s]\u001b[A\n",
            "  2%|▏         | 40960/1648877 [00:00<00:18, 87775.94it/s]\u001b[A\n",
            "  6%|▌         | 98304/1648877 [00:00<00:13, 111752.09it/s]\u001b[A\n",
            " 13%|█▎        | 212992/1648877 [00:01<00:09, 148261.08it/s]\u001b[A\n",
            " 23%|██▎       | 376832/1648877 [00:01<00:06, 197852.45it/s]\u001b[A\n",
            " 37%|███▋      | 614400/1648877 [00:01<00:03, 265426.49it/s]\u001b[A\n",
            " 60%|█████▉    | 983040/1648877 [00:01<00:01, 359025.11it/s]\u001b[A\n",
            "1654784it [00:01, 966783.61it/s]                            \u001b[A\n",
            "0it [00:00, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/4542 [00:00<?, ?it/s]\u001b[A\n",
            "8192it [00:00, 21523.51it/s]            \u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeDgf9qUK0rS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "be124563-2323-4740-d73e-99cf001cb7c4"
      },
      "source": [
        "image, label = test_dataset[0]\n",
        "\n",
        "# reduce batch=1 to no batch\n",
        "image = image[0]\n",
        "print(f'{image.size()} , Label: {label}')\n",
        "plt.imshow(image)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([28, 28]) , Label: 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb1e8ee0048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngrADSRB2IAnCDiRB2IEk/h9BCfQTVPflJQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwLuMCUGLbea",
        "colab_type": "text"
      },
      "source": [
        "## 2. Initiate the Neural Network (multi-layer perceptron)\n",
        "\n",
        "The network has 2 layers, with ReLu activation in between"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yze7wr3HLjaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "input_size = 784\n",
        "hidden_size = 500\n",
        "num_classes = 10\n",
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Fully connected neural network with one hidden layer\n",
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super(NeuralNet, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.fc1(x)\n",
        "    out = self.relu(out)\n",
        "    out = self.fc2(out)\n",
        "    return out\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, num_classes).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U77l82y_PG0k",
        "colab_type": "text"
      },
      "source": [
        "### 3. Train the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSyqD3CxQ2uB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "470b3d3a-81df-40cc-a06a-fafcf56648bb"
      },
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    # Move tensors to the configured device\n",
        "    images = images.reshape(-1, 28*28).to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i+1) % 100 == 0:\n",
        "      print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "        .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 0.3200\n",
            "Epoch [1/5], Step [200/600], Loss: 0.3016\n",
            "Epoch [1/5], Step [300/600], Loss: 0.2120\n",
            "Epoch [1/5], Step [400/600], Loss: 0.2577\n",
            "Epoch [1/5], Step [500/600], Loss: 0.1806\n",
            "Epoch [1/5], Step [600/600], Loss: 0.1343\n",
            "Epoch [2/5], Step [100/600], Loss: 0.1547\n",
            "Epoch [2/5], Step [200/600], Loss: 0.0643\n",
            "Epoch [2/5], Step [300/600], Loss: 0.1617\n",
            "Epoch [2/5], Step [400/600], Loss: 0.1355\n",
            "Epoch [2/5], Step [500/600], Loss: 0.1319\n",
            "Epoch [2/5], Step [600/600], Loss: 0.1163\n",
            "Epoch [3/5], Step [100/600], Loss: 0.1382\n",
            "Epoch [3/5], Step [200/600], Loss: 0.0520\n",
            "Epoch [3/5], Step [300/600], Loss: 0.0687\n",
            "Epoch [3/5], Step [400/600], Loss: 0.0500\n",
            "Epoch [3/5], Step [500/600], Loss: 0.1058\n",
            "Epoch [3/5], Step [600/600], Loss: 0.0523\n",
            "Epoch [4/5], Step [100/600], Loss: 0.0364\n",
            "Epoch [4/5], Step [200/600], Loss: 0.1530\n",
            "Epoch [4/5], Step [300/600], Loss: 0.0860\n",
            "Epoch [4/5], Step [400/600], Loss: 0.0233\n",
            "Epoch [4/5], Step [500/600], Loss: 0.0422\n",
            "Epoch [4/5], Step [600/600], Loss: 0.1068\n",
            "Epoch [5/5], Step [100/600], Loss: 0.0361\n",
            "Epoch [5/5], Step [200/600], Loss: 0.0331\n",
            "Epoch [5/5], Step [300/600], Loss: 0.0123\n",
            "Epoch [5/5], Step [400/600], Loss: 0.0091\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0318\n",
            "Epoch [5/5], Step [600/600], Loss: 0.0439\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygXoi30UTppg",
        "colab_type": "text"
      },
      "source": [
        "## 4. Test the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNtUCKjQTs5Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1659e588-0585-4b5f-f3b1-90dc3f3e473c"
      },
      "source": [
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for images, labels in test_loader:\n",
        "    images = images.reshape(-1, 28*28).to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'model.pt')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 97.9 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}