{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "nlp-from-scratch-translation-with-a-sequence-to-sequence-network-and-attention.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alanwuha/ce7455-nlp/blob/master/assignment-3/nlp-from-scratch-translation-with-a-sequence-to-sequence-network-and-attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kf6lYotpAas",
        "colab_type": "text"
      },
      "source": [
        "# NLP FROM SCRATCH: TRANSLATION WITH A SEQUENCE TO SEQUENCE NETWORK AND ATTENTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0ohF3QFc-Sb",
        "colab_type": "text"
      },
      "source": [
        "### Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyVY5MIopAav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQHFVnKP_DFj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "outputId": "11d88ef2-0526-4599-9f3e-9c4707c49f06"
      },
      "source": [
        "!pip install torchtext==0.5.0"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.5.0\n",
            "  Using cached https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5.0) (0.1.85)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5.0) (2.21.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5.0) (1.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5.0) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5.0) (1.18.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.5.0) (4.38.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.5.0) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.5.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.5.0) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.5.0) (1.24.3)\n",
            "Installing collected packages: torchtext\n",
            "  Found existing installation: torchtext 0.4.0\n",
            "    Uninstalling torchtext-0.4.0:\n",
            "      Successfully uninstalled torchtext-0.4.0\n",
            "Successfully installed torchtext-0.5.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torchtext"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0iUnH9VpAa1",
        "colab_type": "text"
      },
      "source": [
        "### Load and Prepare Data\n",
        "\n",
        "The data for this project is a set of many thousands of English to French translation pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmdQD0tQpAa2",
        "colab_type": "code",
        "outputId": "9cd890c4-9db2-41f4-c175-347f87b3b52c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "!wget https://download.pytorch.org/tutorial/data.zip\n",
        "!unzip data.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-02 13:03:42--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 13.224.253.114, 13.224.253.59, 13.224.253.46, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|13.224.253.114|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2.7M) [application/zip]\n",
            "Saving to: ‘data.zip.1’\n",
            "\n",
            "data.zip.1          100%[===================>]   2.75M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-04-02 13:03:44 (28.8 MB/s) - ‘data.zip.1’ saved [2882130/2882130]\n",
            "\n",
            "Archive:  data.zip\n",
            "replace data/eng-fra.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVYaTPe4pAa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = { 0: 'SOS', 1: 'EOS' }\n",
        "        self.n_words = 2 # Count SOS and EOS\n",
        "        \n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "            \n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7QYWG2opAbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "181e8af6-8904-48ad-8ee7-1ea63c4836fe",
        "id": "vr8PjTrnh1A1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print(unicodeToAscii(\"À l'aide !\"))\n",
        "print(normalizeString(\"À l'aide !\"))\n",
        "\n",
        "print(normalizeString(\"Go.\"))\n",
        "print(normalizeString(\"Va !\"))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A l'aide !\n",
            "a l aide !\n",
            "go .\n",
            "va !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q965XNzpAbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p): # e.g. p = ['go.', 'va !']\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omy7HbdcpxPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wordIndexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = wordIndexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQiqvKdSpAbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readLangs(lang1, lang2, reverse=False): # e.g. lang1 = eng, lang2 = fra\n",
        "    print(\"Reading lines...\")\n",
        "    \n",
        "    # Read the file and split into lines\n",
        "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n",
        "    \n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    \n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "        input_char = Char(lang2)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "        input_char = Char(lang1)\n",
        "        \n",
        "    return input_lang, output_lang, input_char, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Q5vqey1pAb0",
        "colab_type": "text"
      },
      "source": [
        "### The Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdbqxzumpAb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        \n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UEvTmt8pAb5",
        "colab_type": "text"
      },
      "source": [
        "### The Attention Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtPmQBvcpAb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "        \n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "        \n",
        "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "        \n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "        \n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        \n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        \n",
        "        return output, hidden, attn_weights\n",
        "    \n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuIByMVfpAca",
        "colab_type": "text"
      },
      "source": [
        "### Plotting results\n",
        "\n",
        "Plotting is done with matplotlib, using the array of loss values `plot_losses` saved while training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuK6-GWApAca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "def showPlot(points):\n",
        "    print('showPlot')\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locater puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gtm7keUhpAcQ",
        "colab_type": "text"
      },
      "source": [
        "This is a helper function to print time elapsed and estimated time remaining given the current time and progress %."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdnEx96qpAcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = s // 60\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    # return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "    return '%s' % (asMinutes(s))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULnxkv_TkYmX",
        "colab_type": "text"
      },
      "source": [
        "## Here's What's Changed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PNMvZqupAbj",
        "colab_type": "text"
      },
      "source": [
        "### The CNN Model for Generating Character Embeddings\n",
        "\n",
        "Consider the word 'cat', we pad it on both ends to get our maximum word length (this is mainly an implementation quirk since we can't have variable length layers at run time, our algorithm will ignore the pads).\n",
        "\n",
        "We then apply a convolution layer on top that generates spatial coherence across characters, we use a maxpool to extract meaningful features out of our convolution layer. This now gives us a dense vector representation of each word.\n",
        "\n",
        "__This representation will then be concatenated with the word embedding to become the input of the GRU layer in EncoderRNN__.\n",
        "\n",
        "![cnn_model](https://raw.githubusercontent.com/TheAnig/NER-LSTM-CNN-Pytorch/master/images/cnn_model.png)\n",
        "\n",
        "This snippet shows us how the CNN is implemented in PyTorch.\n",
        "\n",
        "```\n",
        "self.char_cnn3 = nn.Conv2d(in_channels=1, out_channels=char_representation_dim, kernel_size=(3, char_embedding_dim), padding=(2, 0))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O27J5z9xh2ey",
        "colab_type": "text"
      },
      "source": [
        "We define a new `Char` class that stores the char2index, char2count, and index2char dictionaries for a given language to be used for character-level encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6qWLGLfpAbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EOS_char_token = 0\n",
        "\n",
        "class Char:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.char2index = {}\n",
        "        self.char2count = {}\n",
        "        self.index2char = {0: '*'}\n",
        "        self.n_chars = 1\n",
        "        \n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "            \n",
        "    def addWord(self, word):\n",
        "        for char in word:\n",
        "            self.addChar(char)\n",
        "            \n",
        "    def addChar(self, char):\n",
        "        if char not in self.char2index:\n",
        "            self.char2index[char] = self.n_chars\n",
        "            self.char2count[char] = 1\n",
        "            self.index2char[self.n_chars] = char\n",
        "            self.n_chars += 1\n",
        "        else:\n",
        "            self.char2count[char] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFCjZYqtqL3l",
        "colab_type": "text"
      },
      "source": [
        "We add two new methods `charIndexesFromSentence` and `charIndexesFromWord` to retrieve character indexes for a given input sentence to be used for our character-level encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GC6OW8jHqLLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def charIndexesFromSentence(char, sentence):\n",
        "    indexes = [charIndexesFromWord(char, word) for word in sentence.split(' ')]\n",
        "    indexes.append([EOS_char_token])\n",
        "    return indexes\n",
        "\n",
        "def charIndexesFromWord(char, word):\n",
        "    return [char.char2index[character] for character in word]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GibMll--lA07",
        "colab_type": "text"
      },
      "source": [
        "### The Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmP2nw1geG-y",
        "colab_type": "text"
      },
      "source": [
        "We modify the original EncoderRNN to include a character-level encoder that generates character representation given an input word. This character representation will then be concatenated with the word embedding and passed as input into the GRU layer to generate the output and hidden vectors for each word in a sequence.\n",
        "\n",
        "![diagram](/content/encoder.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0rMmhzZpAbx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, n_chars, n_words, hidden_size, char_embedding_dim=25, char_representation_dim=25):\n",
        "        \"\"\"\n",
        "        Input parameters:\n",
        "            n_chars = Number of unique characters in input language\n",
        "            n_words = Number of unique words in input language\n",
        "            hidden_size = Dimension of GRU input and output.\n",
        "            char_embedding_dim = Dimension of the character embeddings\n",
        "            char_representation_dim = Output dimension from the CNN encoder for character\n",
        "        \"\"\"\n",
        "        super(EncoderRNN, self).__init__()\n",
        "\n",
        "        # Parameters\n",
        "        self.n_chars = n_chars\n",
        "        self.char_embedding_dim = char_embedding_dim\n",
        "        self.char_representation_dim = char_representation_dim\n",
        "        self.n_words = n_words\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Character-level encoder\n",
        "        self.char_embedding_layer = nn.Embedding(n_chars, char_embedding_dim)\n",
        "        self.char_cnn3_layer = nn.Conv2d(in_channels=1, out_channels=char_representation_dim, kernel_size=(3, char_embedding_dim), padding=(2, 0))\n",
        "        \n",
        "        # Word embedding layer (Dimension of the word embeddings is automatically derived as hidden_size - char_representation_dim)\n",
        "        self.word_embedding_layer = nn.Embedding(n_words, hidden_size - char_representation_dim)\n",
        "        \n",
        "        # GRU layer\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "        # Weights\n",
        "        self.initEmbedding(self.char_embedding_layer.weight)\n",
        "        \n",
        "    def forward(self, char_indexes, word_index, hidden):\n",
        "        # Get char representation\n",
        "        char_embedding = self.char_embedding_layer(char_indexes).unsqueeze(1)\n",
        "        char_cnn3 = self.char_cnn3_layer(char_embedding).squeeze(-1).unsqueeze(1)\n",
        "        char_representation = F.max_pool2d(char_cnn3, kernel_size=(1, char_cnn3.size(-1))).squeeze(-1)\n",
        "\n",
        "        # Get word embedding\n",
        "        word_embedding = self.word_embedding_layer(word_index).view(1, 1, -1)\n",
        "\n",
        "        # Concatenate char representation with word embedding\n",
        "        combined = torch.cat((char_representation, word_embedding), dim=2)\n",
        "\n",
        "        # Feed combined and hidden to GRU\n",
        "        output, hidden = self.gru(combined, hidden)\n",
        "\n",
        "        return output, hidden\n",
        "    \n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "    def initEmbedding(self, input_embedding):\n",
        "      \"\"\"\n",
        "      Initialize embedding\n",
        "      \"\"\"\n",
        "      bias = np.sqrt(3.0 / input_embedding.size(1))\n",
        "      nn.init.uniform_(input_embedding, -bias, bias)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pONQk1yseUbp",
        "colab_type": "text"
      },
      "source": [
        "### Decoder Beam Search\n",
        "\n",
        "We implement a new class `BeamSearchNode` and new method `BeamDecode` to perform beam search decoding in our decoder model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFxffk4seX35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BeamSearchNode(object):\n",
        "  def __init__(self, hiddenstate, previousNode, wordId, logProb, length):\n",
        "    self.h = hiddenstate\n",
        "    self.prevNode = previousNode\n",
        "    self.wordid = wordId\n",
        "    self.logp = logProb\n",
        "    self.leng = length\n",
        "\n",
        "  def eval(self, alpha=1.0):\n",
        "    reward = 0\n",
        "    # Add here a function for shaping a reward\n",
        "\n",
        "    return self.logp / float(self.leng - 1 + 1e-6) + alpha * reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiJ-qnIfgoMv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import operator\n",
        "from queue import PriorityQueue\n",
        "\n",
        "def beamDecode(decoder_hiddens, decoder, encoder_outputs=None):\n",
        "  \"\"\"\n",
        "  :param decoder_hidden: input tensor of shape [1, B, H] for start of the decoding\n",
        "  :param encoder_outputs: if you are using attention mechanism you can pass encoder outputs, [T, B, H] where T is the maximum length of input sentence\n",
        "  :return: decoded_batch\n",
        "  \"\"\"\n",
        "\n",
        "  beam_width = 5\n",
        "  topk = 1 # how many sentences do you want to generate\n",
        "  decoded_batch = []\n",
        "\n",
        "  # print(target_tensor.shape)\n",
        "  # print(decoder_hiddens.shape)\n",
        "  # print(encoder_outputs.shape)\n",
        "\n",
        "  # decoding goes sentence by sentence\n",
        "  for idx in range(decoder_hiddens.size(1)):\n",
        "    decoder_hidden = decoder_hiddens[:, idx, :]\n",
        "    encoder_output = encoder_outputs[:, idx, :]\n",
        "\n",
        "    # Start with the start of the sentence token\n",
        "    decoder_input = torch.LongTensor([[SOS_token]]).to(device)\n",
        "    \n",
        "    # Number of sentence to generate\n",
        "    endnodes = []\n",
        "    number_required = min((topk + 1), topk - len(endnodes))\n",
        "\n",
        "    # Starting node - hidden vector, previous node, word id, logp, length\n",
        "    node = BeamSearchNode(decoder_hidden, None, decoder_input, 0, 1)\n",
        "    nodes = PriorityQueue()\n",
        "\n",
        "    # Start the queue\n",
        "    nodes.put((-node.eval(), node))\n",
        "    qsize = 1\n",
        "\n",
        "    # Start beam search\n",
        "    while True:\n",
        "\n",
        "      # Give up when decoding takes too long\n",
        "      if qsize > 2000:\n",
        "        break\n",
        "\n",
        "      # Fetch the best node\n",
        "      score, n = nodes.get()\n",
        "      decoder_input = n.wordid\n",
        "      decoder_hidden = n.h\n",
        "\n",
        "      if n.wordid.item() == EOS_token and n.prevNode != None:\n",
        "        endnodes.append((score, n))\n",
        "        # If we reached maximum # of sentences required\n",
        "        if len(endnodes) >= number_required:\n",
        "          break\n",
        "        else:\n",
        "          continue\n",
        "\n",
        "      # Decode for one step using decoder\n",
        "      decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_output)\n",
        "\n",
        "      # PUT HERE REAL BEAM SEARCH OF TOP\n",
        "      log_prob, indexes = torch.topk(decoder_output, beam_width)\n",
        "      # print(indexes)\n",
        "      nextnodes = []\n",
        "\n",
        "      for new_k in range(beam_width):\n",
        "        decoded_t = indexes[0][new_k].view(1, -1)\n",
        "        log_p = log_prob[0][new_k].item()\n",
        "\n",
        "        node = BeamSearchNode(decoder_hidden, n, decoded_t, n.logp + log_p, n.leng + 1)\n",
        "        score = -node.eval()\n",
        "        nextnodes.append((score, node))\n",
        "\n",
        "      # Put them into queue\n",
        "      for i in range(len(nextnodes)):\n",
        "        score, nn = nextnodes[i]\n",
        "        nodes.put((score, nn))\n",
        "        # increase qsize\n",
        "      qsize += len(nextnodes) - 1\n",
        "\n",
        "    # Choose nbest paths, back trace them\n",
        "    if len(endnodes) == 0:\n",
        "      endnodes = [nodes.get() for _ in range(topk)]\n",
        "\n",
        "    utterances = []\n",
        "    for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n",
        "      utterance = []\n",
        "      utterance.append(n.wordid)\n",
        "      # back trace\n",
        "      while n.prevNode != None:\n",
        "        n = n.prevNode\n",
        "        utterance.append(n.wordid)\n",
        "\n",
        "      utterance = utterance[::-1]\n",
        "      utterances.append(utterance)\n",
        "    \n",
        "    decoded_batch.append(utterances)\n",
        "\n",
        "  return decoded_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhLEvBd-pAbP",
        "colab_type": "text"
      },
      "source": [
        "### Preparing the train and test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRcfjbwtpAbZ",
        "colab_type": "text"
      },
      "source": [
        "We modify `prepareData` to:\n",
        "1. Generate `input_char`\n",
        "1. Shuffle `pairs`\n",
        "1. Split `pairs` into `train_pairs` (80% of total pairs) and `test_pairs` (20% of total pairs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "YdVVufWmpAba",
        "colab_type": "code",
        "outputId": "906bc3b2-cf84-4adf-dd54-ef98ce4a5772",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, input_char, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words and chars...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "        input_char.addSentence(pair[0])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    print(\"Counted chars:\")\n",
        "    print(input_char.name, input_char.n_chars)\n",
        "    print('%d total pairs' % (len(pairs),))\n",
        "\n",
        "    # Shuffle pairs and split into train_pairs and test_pairs\n",
        "    random.shuffle(pairs)\n",
        "    train_length = (len(pairs) // 10) * 8\n",
        "    train_pairs = pairs[:train_length]\n",
        "    test_pairs = pairs[train_length:]\n",
        "    \n",
        "    return input_lang, output_lang, input_char, train_pairs, test_pairs\n",
        "\n",
        "input_lang, output_lang, input_char, train_pairs, test_pairs = prepareData('eng', 'fra', True)\n",
        "\n",
        "print('\\nCounting total pairs, train pairs, and test pairs...')\n",
        "print('%d train pairs' % (len(train_pairs),))\n",
        "print('%d test pairs' % (len(test_pairs),))\n",
        "print('Example of a training pair: %s' % (train_pairs[0]))\n",
        "\n",
        "print('\\nTesting charIndexesFromSentence...')\n",
        "sentence = 'je t aime'\n",
        "print('Sentence: %s' % (sentence))\n",
        "print('Char Indexes: %s' % (charIndexesFromSentence(input_char, sentence)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 10599 sentence pairs\n",
            "Counting words and chars...\n",
            "Counted words:\n",
            "fra 4345\n",
            "eng 2803\n",
            "Counted chars:\n",
            "fra 30\n",
            "10599 total pairs\n",
            "\n",
            "Counting total pairs, train pairs, and test pairs...\n",
            "8472 train pairs\n",
            "2127 test pairs\n",
            "Example of a training pair: ['je suis une artiste .', 'i am an artist .']\n",
            "\n",
            "Testing charIndexesFromSentence...\n",
            "Sentence: je t aime\n",
            "Char Indexes: [[1, 7], [17], [2, 3, 16, 7], [0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEGVlMuW27R4",
        "colab_type": "text"
      },
      "source": [
        "There are a total of `29` unique characters (excluding the EOS_char_token '*' that we added)  in the `French` dataset as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MwV4fb0pAbe",
        "colab_type": "code",
        "outputId": "0208fc1f-92ae-4427-f279-3fe6016ba48c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "print(input_char.char2index)\n",
        "print(input_char.char2count)\n",
        "print(input_char.index2char)\n",
        "print(len(input_char.char2index))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'j': 1, 'a': 2, 'i': 3, 'n': 4, 's': 5, '.': 6, 'e': 7, 'v': 8, 'b': 9, 'c': 10, 'u': 11, 'g': 12, 'r': 13, 'o': 14, 'f': 15, 'm': 16, 't': 17, 'h': 18, '!': 19, 'l': 20, 'd': 21, 'p': 22, 'z': 23, 'x': 24, 'y': 25, 'q': 26, 'k': 27, '?': 28, 'w': 29}\n",
            "{'j': 4774, 'a': 13819, 'i': 15983, 'n': 15628, 's': 27771, '.': 10262, 'e': 40222, 'v': 4630, 'b': 1642, 'c': 5537, 'u': 16566, 'g': 1742, 'r': 12076, 'o': 13499, 'f': 2316, 'm': 6719, 't': 15608, 'h': 1468, '!': 197, 'l': 9856, 'd': 5313, 'p': 6098, 'z': 407, 'x': 747, 'y': 415, 'q': 1192, 'k': 37, '?': 145, 'w': 13}\n",
            "{0: '*', 1: 'j', 2: 'a', 3: 'i', 4: 'n', 5: 's', 6: '.', 7: 'e', 8: 'v', 9: 'b', 10: 'c', 11: 'u', 12: 'g', 13: 'r', 14: 'o', 15: 'f', 16: 'm', 17: 't', 18: 'h', 19: '!', 20: 'l', 21: 'd', 22: 'p', 23: 'z', 24: 'x', 25: 'y', 26: 'q', 27: 'k', 28: '?', 29: 'w'}\n",
            "29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx0-LqqZpAcJ",
        "colab_type": "text"
      },
      "source": [
        "In the `train` method, we modify the encoder's forward pass method (line 24) to take in character indexes for each input word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJhU76afpAcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, input_char_indexes, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        # Modify encoder forward pass method to take in character indexes of the input word\n",
        "        encoder_output, encoder_hidden = encoder(torch.LongTensor([input_char_indexes[ei]]).to(device), input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "        \n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            # Train using DecoderRNN\n",
        "            # decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "            # Train using AttnDecoderRNN\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            # Train using DecoderRNN\n",
        "            # decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "\n",
        "            # Train using AttnDecoderRNN\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            \n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHutNXXIC6j5",
        "colab_type": "text"
      },
      "source": [
        "We define a new method `trainEpochs` to train on `train_pairs` (80% of the data) and evaluate on `test_pairs` (20% of the data) for a given number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NorkrizapAcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainEpochs(encoder, decoder, n_epochs=20, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    # Train over n_epochs\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        # Print start of epoch\n",
        "        print('Epoch %d' % (epoch))\n",
        "\n",
        "        # Shuffle train_pairs\n",
        "        random.shuffle(train_pairs)\n",
        "\n",
        "        # Get tensors from pair\n",
        "        training_pairs = [tensorsFromPair(pair) for pair in train_pairs]\n",
        "        \n",
        "        # Get character indexes\n",
        "        training_char_indexes = [charIndexesFromSentence(input_char, pair[0]) for pair in train_pairs]\n",
        "\n",
        "        # Train all train_pairs\n",
        "        for i in range(1, len(train_pairs) + 1):\n",
        "        # for i in range(1, 2):\n",
        "          training_pair = training_pairs[i - 1]\n",
        "          input_tensor = training_pair[0]\n",
        "          target_tensor = training_pair[1]\n",
        "          input_char_indexes = training_char_indexes[i - 1]\n",
        "\n",
        "          loss = train(input_tensor, target_tensor, input_char_indexes, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "          print_loss_total += loss\n",
        "          plot_loss_total += loss\n",
        "\n",
        "          if i % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, i / len(train_pairs)), i, i / len(train_pairs) * 100, print_loss_avg))\n",
        "            \n",
        "          if i % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2HxgWKNDZuO",
        "colab_type": "text"
      },
      "source": [
        "We modify the `Evaluate` method to perform beam search decoding to generate the output sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01eiA9AKDuSt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "  with torch.no_grad():\n",
        "    input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "    input_length = input_tensor.size()[0]\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "    \n",
        "    input_char_indexes = charIndexesFromSentence(input_char, sentence)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    for ei in range(input_length):\n",
        "      encoder_output, encoder_hidden = encoder(torch.LongTensor([input_char_indexes[ei]]).to(device), input_tensor[ei], encoder_hidden)\n",
        "      encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    decoded_batch = beamDecode(decoder_hidden.unsqueeze(1), decoder, encoder_outputs.unsqueeze(1))\n",
        "    \n",
        "    return decoded_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foxGGXPZ6Q2C",
        "colab_type": "text"
      },
      "source": [
        "### Train our improved model on train data (80% of total data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVooQtHupAck",
        "colab_type": "code",
        "outputId": "184b176d-564b-4c6a-fbc1-f0c3f07548f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "hidden_size = 256\n",
        "char_embedding_dim = 25\n",
        "char_representation_dim = 25\n",
        "\n",
        "encoder1 = EncoderRNN(input_char.n_chars, input_lang.n_words, hidden_size, char_embedding_dim, char_representation_dim).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainEpochs(encoder1, attn_decoder1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA-3ffOspAcV",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate and report BLEU scores for beam size of 5 on test data (20% of total data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5cwQQmqpakb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "candidate_corpus = []\n",
        "references_corpus = []\n",
        "\n",
        "for pair in test_pairs:\n",
        "  # Evaluate pair\n",
        "  print('>', pair[0])\n",
        "  print('=', pair[1])\n",
        "  decoded_batch = evaluate(encoder1, attn_decoder1, pair[0])\n",
        "  output_sentence = ' '.join([output_lang.index2word[index.item()] for index in decoded_batch[0][0][1:-1]])\n",
        "  print('<', output_sentence)\n",
        "  print('')\n",
        "\n",
        "  # Append to corpus\n",
        "  candidate_corpus.append(output_sentence.split(' '))\n",
        "  references_corpus.append(references_corpus.split(' '))\n",
        "\n",
        "score = bleu_score(candidate_corpus, references_corpus)\n",
        "print('Bleu score: %.4f' % (score))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}