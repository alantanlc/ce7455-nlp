{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "nlp-from-scratch-translation-with-a-sequence-to-sequence-network-and-attention.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alanwuha/ce7455-nlp/blob/master/assignment-3/nlp-from-scratch-translation-with-a-sequence-to-sequence-network-and-attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Kf6lYotpAas",
        "colab_type": "text"
      },
      "source": [
        "# NLP FROM SCRATCH: TRANSLATION WITH A SEQUENCE TO SEQUENCE NETWORK AND ATTENTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0ohF3QFc-Sb",
        "colab_type": "text"
      },
      "source": [
        "### Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyVY5MIopAav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0iUnH9VpAa1",
        "colab_type": "text"
      },
      "source": [
        "### Load and Prepare Data\n",
        "\n",
        "The data for this project is a set of many thousands of English to French translation pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmdQD0tQpAa2",
        "colab_type": "code",
        "outputId": "11152c4d-219e-4c56-8fd8-4bf013b218f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "!wget https://download.pytorch.org/tutorial/data.zip\n",
        "!unzip data.zip"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-01 05:54:52--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 54.192.151.21, 54.192.151.109, 54.192.151.68, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|54.192.151.21|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2.7M) [application/zip]\n",
            "Saving to: ‘data.zip.1’\n",
            "\n",
            "data.zip.1          100%[===================>]   2.75M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-04-01 05:54:53 (90.4 MB/s) - ‘data.zip.1’ saved [2882130/2882130]\n",
            "\n",
            "Archive:  data.zip\n",
            "replace data/eng-fra.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybACQRGQpAa6",
        "colab_type": "text"
      },
      "source": [
        "Similar to the character encoding used in the character-level RNN tutorials, we will be representing each word in a language as a one-hot vector, or giant vector of zeros except for a single one (at the index of the word). Compared to the dozens of characters that might exist in a language, there are many many more words, so the encoding vector is much larger. We will however cheat a bit and trim the data to only use a few thousand words per language.\n",
        "\n",
        "![diagram](https://pytorch.org/tutorials/_images/word-encoding.png)\n",
        "\n",
        "We'll need a unique index per word to use as the inputs and targets of the networks later. To keep track of all this we will use a helper class called `Lang` which has word -> index (`word2index`) and index -> word (`index2word`) dictionaries, as well as a count of each word `word2count` to use to later replace rare words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVYaTPe4pAa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = { 0: 'SOS', 1: 'EOS' }\n",
        "        self.n_words = 2 # Count SOS and EOS\n",
        "        \n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "            \n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O27J5z9xh2ey",
        "colab_type": "text"
      },
      "source": [
        "Here we add a new `Char` class that stores the character vocabulary, char2index, index2char dictionaries for a given language."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6qWLGLfpAbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Char:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.char2index = {}\n",
        "        self.char2count = {}\n",
        "        self.index2char = {}\n",
        "        self.n_chars = 0\n",
        "        \n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "            \n",
        "    def addWord(self, word):\n",
        "        for char in word:\n",
        "            self.addChar(char)\n",
        "            \n",
        "    def addChar(self, char):\n",
        "        if char not in self.char2index:\n",
        "            self.char2index[char] = self.n_chars\n",
        "            self.char2count[char] = 1\n",
        "            self.index2char[self.n_chars] = char\n",
        "            self.n_chars += 1\n",
        "        else:\n",
        "            self.char2count[char] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLuIjrBNpAbE",
        "colab_type": "text"
      },
      "source": [
        "The files are all in Unicode, to simplify we will turn Unicode characters to ASCII, make everything lowercase, and trim most punctuation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7QYWG2opAbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e37cc299-d731-4533-b873-4fb3ca4ce4ec",
        "id": "vr8PjTrnh1A1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print(unicodeToAscii(\"À l'aide !\"))\n",
        "print(normalizeString(\"À l'aide !\"))\n",
        "\n",
        "print(normalizeString(\"Go.\"))\n",
        "print(normalizeString(\"Va !\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A l'aide !\n",
            "a l aide !\n",
            "go .\n",
            "va !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhLEvBd-pAbP",
        "colab_type": "text"
      },
      "source": [
        "To read the data file we will split the file into lines, and then split lines into pairs. The files are all English -> Other Language, so if we want to translate from Other Language -> English I added the `reverse` flag to reverse the pairs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQiqvKdSpAbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readLangs(lang1, lang2, reverse=False): # e.g. lang1 = eng, lang2 = fra\n",
        "    print(\"Reading lines...\")\n",
        "    \n",
        "    # Read the file and split into lines\n",
        "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n",
        "    \n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    \n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "        input_char = Char(lang2)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "        input_char = Char(lang1)\n",
        "        \n",
        "    return input_lang, output_lang, input_char, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-08MGdXmpAbU",
        "colab_type": "text"
      },
      "source": [
        "Since there are a _lot_ of example sentences and we want to train something quickly, we'll trim the data set to only relatively short and simple sentence. Here the maximum length is 10 words (that includes ending punctuation) and we're filtering to sentences that translate to the form \"I am\" or \"He is\" etc. (accounting for apostrophes replaced earlier)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q965XNzpAbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p): # e.g. p = ['go.', 'va !']\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRcfjbwtpAbZ",
        "colab_type": "text"
      },
      "source": [
        "The full process for preparing that data is:\n",
        "- Read text file and split into lines, split lines into pairs\n",
        "- Normalize text filter by length and content\n",
        "- Make word lists from sentences in pairs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "YdVVufWmpAba",
        "colab_type": "code",
        "outputId": "1953d4d8-63e7-4fe6-ce41-4a89f22aed90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, input_char, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words and chars...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "        input_char.addSentence(pair[0])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    print(\"Counted chars:\")\n",
        "    print(input_char.name, input_char.n_chars)\n",
        "    \n",
        "    return input_lang, output_lang, input_char, pairs\n",
        "\n",
        "input_lang, output_lang, input_char, pairs = prepareData('eng', 'fra', True)\n",
        "\n",
        "print(random.choice(pairs))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 10599 sentence pairs\n",
            "Counting words and chars...\n",
            "Counted words:\n",
            "fra 4345\n",
            "eng 2803\n",
            "Counted chars:\n",
            "fra 29\n",
            "['vous etes tres marrant .', 'you re very funny .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MwV4fb0pAbe",
        "colab_type": "code",
        "outputId": "f490364f-b1de-44b6-d3d8-20e0eb1aa273",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "print(input_char.char2index)\n",
        "print(input_char.char2count)\n",
        "print(input_char.index2char)\n",
        "print(input_char.n_chars)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'j': 0, 'a': 1, 'i': 2, 'n': 3, 's': 4, '.': 5, 'e': 6, 'v': 7, 'b': 8, 'c': 9, 'u': 10, 'g': 11, 'r': 12, 'o': 13, 'f': 14, 'm': 15, 't': 16, 'h': 17, '!': 18, 'l': 19, 'd': 20, 'p': 21, 'z': 22, 'x': 23, 'y': 24, 'q': 25, 'k': 26, '?': 27, 'w': 28}\n",
            "{'j': 4774, 'a': 13819, 'i': 15983, 'n': 15628, 's': 27771, '.': 10262, 'e': 40222, 'v': 4630, 'b': 1642, 'c': 5537, 'u': 16566, 'g': 1742, 'r': 12076, 'o': 13499, 'f': 2316, 'm': 6719, 't': 15608, 'h': 1468, '!': 197, 'l': 9856, 'd': 5313, 'p': 6098, 'z': 407, 'x': 747, 'y': 415, 'q': 1192, 'k': 37, '?': 145, 'w': 13}\n",
            "{0: 'j', 1: 'a', 2: 'i', 3: 'n', 4: 's', 5: '.', 6: 'e', 7: 'v', 8: 'b', 9: 'c', 10: 'u', 11: 'g', 12: 'r', 13: 'o', 14: 'f', 15: 'm', 16: 't', 17: 'h', 18: '!', 19: 'l', 20: 'd', 21: 'p', 22: 'z', 23: 'x', 24: 'y', 25: 'q', 26: 'k', 27: '?', 28: 'w'}\n",
            "29\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Q5vqey1pAb0",
        "colab_type": "text"
      },
      "source": [
        "### The Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdbqxzumpAb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        \n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UEvTmt8pAb5",
        "colab_type": "text"
      },
      "source": [
        "### Attention Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtPmQBvcpAb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "        \n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "        \n",
        "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
        "        \n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "        \n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        \n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        \n",
        "        return output, hidden, attn_weights\n",
        "    \n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuIByMVfpAca",
        "colab_type": "text"
      },
      "source": [
        "### Plotting results\n",
        "\n",
        "Plotting is done with matplotlib, using the array of loss values `plot_losses` saved while training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuK6-GWApAca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "def showPlot(points):\n",
        "    print('showPlot')\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locater puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gtm7keUhpAcQ",
        "colab_type": "text"
      },
      "source": [
        "This is a helper function to print time elapsed and estimated time remaining given the current time and progress %."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdnEx96qpAcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = s // 60\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULnxkv_TkYmX",
        "colab_type": "text"
      },
      "source": [
        "## Here's What's Changed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WM5RvpMpAcB",
        "colab_type": "text"
      },
      "source": [
        "### Preparing Training Data\n",
        "\n",
        "Here, we add two new methods `charIndexesFromSentence` and `charIndexesFromWord` to get the character indexes for a given sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6n8_cjSEpAcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wordIndexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def charIndexesFromSentence(char, sentence):\n",
        "    return [charIndexesFromWord(char, word) for word in sentence.split(' ')]\n",
        "\n",
        "def charIndexesFromWord(char, word):\n",
        "    return [char.char2index[character] for character in word]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = wordIndexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAsK9XHypAcF",
        "colab_type": "code",
        "outputId": "68daa70d-745b-4c54-9484-4490b2825e3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "sentence = 'je t aime'\n",
        "print(sentence)\n",
        "print(charIndexesFromSentence(input_char, sentence))"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "je t aime\n",
            "[[0, 6], [16], [1, 2, 15, 6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PNMvZqupAbj",
        "colab_type": "text"
      },
      "source": [
        "### The CNN Model for Generating Character Embeddings\n",
        "\n",
        "Consider the word 'cat', we pad it on both ends to get our maximum word length (this is mainly an implementation quirk since we can't have variable length layers at run time, our algorithm will ignore the pads).\n",
        "\n",
        "We then apply a convolution layer on top that generates spatial coherence across characters, we use a maxpool to extract meaningful features out of our convolution layer. This now gives us a dense vector representation of each word.\n",
        "\n",
        "__This representation will then be concatenated with the word embedding to become the input of the GRU layer in EncoderRNN__.\n",
        "\n",
        "![cnn_model](https://raw.githubusercontent.com/TheAnig/NER-LSTM-CNN-Pytorch/master/images/cnn_model.png)\n",
        "\n",
        "This snippet shows us how the CNN is implemented in PyTorch.\n",
        "\n",
        "```\n",
        "self.char_cnn3 = nn.Conv2d(in_channels=1, out_channels=char_representation_dim, kernel_size=(3, char_embedding_dim), padding=(2, 0))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GibMll--lA07",
        "colab_type": "text"
      },
      "source": [
        "### The Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmP2nw1geG-y",
        "colab_type": "text"
      },
      "source": [
        "We modify the original EncoderRNN to include a character-level encoder to generate a character representation given an input word. This character representation will then be concatenated with the word embedding and passed as input into the GRU layer to generate the output and hidden vectors for each word in a sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0rMmhzZpAbx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, n_chars, n_words, hidden_size, char_embedding_dim=25, char_representation_dim=25):\n",
        "        \"\"\"\n",
        "        Input parameters:\n",
        "            n_chars = Number of unique characters in input language\n",
        "            n_words = Number of unique words in input language\n",
        "            hidden_size = Dimension of GRU input and output.\n",
        "            char_embedding_dim = Dimension of the character embeddings\n",
        "            char_representation_dim = Output dimension from the CNN encoder for character\n",
        "        \"\"\"\n",
        "        super(EncoderRNN, self).__init__()\n",
        "\n",
        "        # Parameters\n",
        "        self.n_chars = n_chars\n",
        "        self.char_embedding_dim = char_embedding_dim\n",
        "        self.char_representation_dim = char_representation_dim\n",
        "        self.n_words = n_words\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Character-level encoder\n",
        "        self.char_embedding_layer = nn.Embedding(n_chars, char_embedding_dim)\n",
        "        self.char_cnn3_layer = nn.Conv2d(in_channels=1, out_channels=char_representation_dim, kernel_size=(3, char_embedding_dim), padding=(2, 0))\n",
        "        \n",
        "        # Word embedding layer\n",
        "        self.word_embedding_layer = nn.Embedding(n_words, hidden_size - char_representation_dim) # Dimension of the word embeddings is automatically derived as hidden_size - char_representation_dim\n",
        "        \n",
        "        # GRU layer\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "        # Weights\n",
        "        self.initEmbedding(self.char_embedding_layer.weight)\n",
        "        \n",
        "    def forward(self, char_indexes, word_index, hidden):\n",
        "        # Get char representation\n",
        "        char_embedding = self.char_embedding_layer(char_indexes).unsqueeze(1)\n",
        "        char_cnn3 = self.char_cnn3_layer(char_embedding).squeeze(-1).unsqueeze(1)\n",
        "        char_representation = F.max_pool2d(char_cnn3, kernel_size=(1, char_cnn3.size(-1))).squeeze(-1)\n",
        "\n",
        "        # Get word embedding\n",
        "        word_embedding = self.word_embedding_layer(word_index).view(1, 1, -1)\n",
        "\n",
        "        # Concatenate char representation with word embedding\n",
        "        combined = torch.cat((char_representation, word_embedding), dim=2)\n",
        "\n",
        "        # Feed combined and hidden to GRU\n",
        "        output, hidden = self.gru(combined, hidden)\n",
        "\n",
        "        return output, hidden\n",
        "    \n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "    def initEmbedding(self, input_embedding):\n",
        "      \"\"\"\n",
        "      Initialize embedding\n",
        "      \"\"\"\n",
        "      bias = np.sqrt(3.0 / input_embedding.size(1))\n",
        "      nn.init.uniform_(input_embedding, -bias, bias)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA-3ffOspAcV",
        "colab_type": "text"
      },
      "source": [
        "### Training the Model\n",
        "\n",
        "Here we modify `TrainIter` to train on 80% of the training data for a given number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NorkrizapAcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainIters(encoder, char_cnn3, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    char_cnn3_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    \n",
        "    training_pairs = []\n",
        "    training_char_indexes = []\n",
        "    for i in range(n_iters):\n",
        "        pair = random.choice(pairs)\n",
        "        training_pairs.append(tensorsFromPair(pair))\n",
        "        training_char_indexes.append(charIndexesFromSentence(input_char, pair[0]))\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "        input_char_indexes = training_char_indexes[iter - 1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, input_char_indexes, encoder, char_cnn3,\n",
        "                     decoder, encoder_optimizer, char_cnn3_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    # print(plot_losses)\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx0-LqqZpAcJ",
        "colab_type": "text"
      },
      "source": [
        "Here we modify the encoder's forward pass method (line 24) to take in character indexes for each input word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJhU76afpAcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, input_char_indexes, encoder, char_cnn3, decoder, encoder_optimizer, char_cnn3_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    char_cnn3_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    # Pad input_char_indexes with zero indexes if sequence has fewer than max_length words\n",
        "    if len(input_char_indexes) < max_length:\n",
        "      input_char_indexes += [[0]] * (max_length - len(input_char_indexes))\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        # Modify encoder forward pass method to take in character indexes of the input word\n",
        "        encoder_output, encoder_hidden = encoder(torch.LongTensor([input_char_indexes[ei]]).to(device), input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "        \n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "\n",
        "            # Train using DecoderRNN\n",
        "            # decoder_output, decoder_hidden = decoder(\n",
        "            #     decoder_input, decoder_hidden)\n",
        "\n",
        "            # Train using AttnDecoderRNN\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            \n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "\n",
        "            # Train using DecoderRNN\n",
        "            # decoder_output, decoder_hidden = decoder(\n",
        "            #     decoder_input, decoder_hidden)\n",
        "\n",
        "            # Train using AttnDecoderRNN\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            \n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    char_cnn3_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LOiKHL0jN-B",
        "colab_type": "text"
      },
      "source": [
        "### Training and Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVooQtHupAck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_size = 256\n",
        "char_embedding_dim = 25\n",
        "char_representation_dim = 25\n",
        "\n",
        "encoder1 = EncoderRNN(input_char.n_chars, input_lang.n_words, hidden_size, char_embedding_dim, char_representation_dim).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, char_cnn3, attn_decoder1, 75000, print_every=5000)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}