{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Logistic_regression.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alanwuha/ce7455-nlp/blob/master/Logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWLtk1Yv6N3b",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression Sentiment Analysis\n",
        "\n",
        "In this series we'll be building a machine learning model to detect sentiment (i.e. detect if a sentence is positive or negative) using PyTorch and TorchText. This will be done on movie reviews, using the [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
        "\n",
        "In this first notebook, we'll start very simple to understand the general concepts whilst not really caring about good results. Further notebooks will build on this knowledge and we'll actually get good results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVgGQezXYbcA",
        "colab_type": "text"
      },
      "source": [
        "## 0. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HszuNYWAYdzs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "3206378b-5286-49f6-9384-93a26d9e0440"
      },
      "source": [
        "!pip install torch>=1.2.0\n",
        "!pip install torchtext==0.4.0\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n",
            "\r\u001b[K     |██████▏                         | 10kB 16.9MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 20kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 30kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 40kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 51kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4.0) (1.17.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4.0) (4.28.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4.0) (1.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4.0) (1.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4.0) (2.21.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4.0) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4.0) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4.0) (3.0.4)\n",
            "Installing collected packages: torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed torchtext-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc8tF2CKYjJ6",
        "colab_type": "text"
      },
      "source": [
        "## 1. Preparing Data\n",
        "\n",
        "One of the main concepts of __TorchText__ is the `Field`. These define how your data should be processed. In our sentiment classification task the data consists of both the __raw string__ of the review and the sentiment, either __\"pos\" or \"neg\"__.\n",
        "\n",
        "The parameters of a `Field` specify __how the data should be processed__.\n",
        "\n",
        "We use the `TEXT` field to define __how the review should be processed__, and the `LABEL` field to process the sentiment.\n",
        "\n",
        "Our `TEST` field has `tokenize='spacy'` as an argument. This defines that the \"tokenization\" (the act of splitting the string into discrete \"tokens\") should be done using the [spaCy](https://spacy.io/) tokenizer. If no `tokenizer` argument is passed, the __default is simply splitting the string on spaces__.\n",
        "\n",
        "`LABEL` is defined by a `LabelField`, a special subset of the `Field` class specifically used for handling labels. We will explain the `dtype` argument later.\n",
        "\n",
        "For more on `Fields`, go [here](https://github.com/pytorch/text/blob/master/torchtext/data/field.py).\n",
        "\n",
        "We also set the random seeds for reproducibility.\n",
        "\n",
        "Another handy feature of TorchText is that it has support for common datasets used in natural language processing (NLP).\n",
        "\n",
        "The following code automatically downloads the IMDb dataset and splits it into the canonical train/test splits as `torchtext.datasets` objects. It process the data using the `Fields` we have previously defined. The IMDb dataset consists of 50,000 movie reviews, each marked as being a positive or negative review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0OCNxhJaVsP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "aed287b0-ecc9-4930-daef-1b3d7ae52003"
      },
      "source": [
        "import torch\n",
        "from torchtext import data, datasets\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(tokenize = 'spacy')\n",
        "LABEL = data.LabelField(dtype = torch.float)\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:07<00:00, 11.2MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLiq_onqblIS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "b7bdfe0f-00f0-4b1e-9809-5dfca541f395"
      },
      "source": [
        "print(type(data))\n",
        "print(type(train_data))\n",
        "\n",
        "sample = train_data[0]\n",
        "\n",
        "print(sample)\n",
        "print(sample.__dict__)\n",
        "print(sample.__dict__.keys())\n",
        "\n",
        "print('Text: ', ' '.join(sample.text))\n",
        "print('Label: ', sample.label, type(sample.label))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'module'>\n",
            "<class 'torchtext.datasets.imdb.IMDB'>\n",
            "<torchtext.data.example.Example object at 0x7f3c1645e390>\n",
            "{'text': ['\"', 'A', 'Mouse', 'in', 'the', 'House', '\"', 'is', 'a', 'very', 'classic', 'cartoon', 'by', 'Tom', '&', 'Jerry', ',', 'faithful', 'to', 'their', 'tradition', 'but', 'with', 'jokes', 'of', 'its', 'own', '.', 'It', 'is', 'hysterical', ',', 'hilarious', ',', 'very', 'entertaining', 'and', 'quite', 'amusing', '.', 'Artwork', 'is', 'of', 'good', 'quality', 'either.<br', '/><br', '/>This', 'short', 'is', \"n't\", 'just', 'about', 'Tom', 'trying', 'to', 'catch', 'Jerry', '.', 'Butch', 'lives', 'in', 'the', 'same', 'house', 'and', 'he', \"'s\", 'trying', 'to', 'catch', 'the', 'mouse', 'too', ',', 'because', '«', 'there', \"'s\", 'only', 'going', 'to', 'be', 'one', 'cat', 'in', 'this', 'house', 'in', 'the', 'morning', '--', 'and', 'that', \"'s\", 'the', 'cat', 'that', 'catches', 'the', 'mouse».<br', '/><br', '/>If', 'you', 'ask', 'me', ',', 'there', 'are', 'lots', 'of', 'funny', 'gags', 'in', 'this', 'cartoon', '.', 'The', 'funniest', 'for', 'me', 'are', ',', 'for', 'example', ',', 'when', 'Mammy', 'Two', 'Shoes', 'sees', 'the', 'two', 'lazy', 'cats', 'sleeping', 'and', 'says', 'sarcastically', '«', 'I', \"'m\", 'glad', 'you', \"'re\", 'enjoying', 'the', 'siesta', '»', 'and', 'that', 'she', 'hopes', 'they', \"'re\", 'satisfied', 'because', 'she', 'ai', \"n't\", ',', 'making', 'the', 'two', 'cats', 'gasp', '.', 'Another', 'funny', 'gag', 'is', 'when', 'Tom', 'disguises', 'himself', 'as', 'Mammy', 'Two', 'Shoes', 'and', 'slams', 'Butch', 'with', 'a', 'frying', 'pan', 'and', 'then', 'Butch', 'does', 'the', 'same', 'trick', 'to', 'Tom', '.', 'Of', 'course', 'that', ',', 'even', 'funnier', 'than', 'this', ',', 'is', 'when', 'the', 'real', 'Mammy', 'Two', 'Shoes', 'appears', 'and', 'both', '(', 'dumb', '!', ')', 'cats', 'think', 'they', 'are', 'seeing', 'each', 'other', 'disguised', 'as', 'Mammy', 'and', 'then', 'they', 'both', 'attack', 'her', 'on', 'the', '\"', 'rear', '\"', '-', 'lol', '.', 'Naturally', 'that', 'she', 'gets', 'mad', 'and', 'once', 'she', 'gets', 'mad', ',', 'she', 'is', \"n't\", 'someone', 'to', 'mess', 'with', '.', 'But', 'even', 'Jerry', 'does', \"n't\", 'win', 'this', 'time', ',', 'because', 'he', 'is', 'expelled', 'by', 'her', 'too', '.'], 'label': 'pos'}\n",
            "dict_keys(['text', 'label'])\n",
            "Text:  \" A Mouse in the House \" is a very classic cartoon by Tom & Jerry , faithful to their tradition but with jokes of its own . It is hysterical , hilarious , very entertaining and quite amusing . Artwork is of good quality either.<br /><br />This short is n't just about Tom trying to catch Jerry . Butch lives in the same house and he 's trying to catch the mouse too , because « there 's only going to be one cat in this house in the morning -- and that 's the cat that catches the mouse».<br /><br />If you ask me , there are lots of funny gags in this cartoon . The funniest for me are , for example , when Mammy Two Shoes sees the two lazy cats sleeping and says sarcastically « I 'm glad you 're enjoying the siesta » and that she hopes they 're satisfied because she ai n't , making the two cats gasp . Another funny gag is when Tom disguises himself as Mammy Two Shoes and slams Butch with a frying pan and then Butch does the same trick to Tom . Of course that , even funnier than this , is when the real Mammy Two Shoes appears and both ( dumb ! ) cats think they are seeing each other disguised as Mammy and then they both attack her on the \" rear \" - lol . Naturally that she gets mad and once she gets mad , she is n't someone to mess with . But even Jerry does n't win this time , because he is expelled by her too .\n",
            "Label:  pos <class 'str'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB80m3Z0bK8r",
        "colab_type": "text"
      },
      "source": [
        "We can see length of datasets and show some examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxK_0FavbRHS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "35c65ed1-fd19-4273-d682-79d0c54ea41b"
      },
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')\n",
        "print(vars(train_data.examples[0])) # vars(object) is equivalent to object.__dict__"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 25000\n",
            "Number of testing examples: 25000\n",
            "{'text': ['\"', 'A', 'Mouse', 'in', 'the', 'House', '\"', 'is', 'a', 'very', 'classic', 'cartoon', 'by', 'Tom', '&', 'Jerry', ',', 'faithful', 'to', 'their', 'tradition', 'but', 'with', 'jokes', 'of', 'its', 'own', '.', 'It', 'is', 'hysterical', ',', 'hilarious', ',', 'very', 'entertaining', 'and', 'quite', 'amusing', '.', 'Artwork', 'is', 'of', 'good', 'quality', 'either.<br', '/><br', '/>This', 'short', 'is', \"n't\", 'just', 'about', 'Tom', 'trying', 'to', 'catch', 'Jerry', '.', 'Butch', 'lives', 'in', 'the', 'same', 'house', 'and', 'he', \"'s\", 'trying', 'to', 'catch', 'the', 'mouse', 'too', ',', 'because', '«', 'there', \"'s\", 'only', 'going', 'to', 'be', 'one', 'cat', 'in', 'this', 'house', 'in', 'the', 'morning', '--', 'and', 'that', \"'s\", 'the', 'cat', 'that', 'catches', 'the', 'mouse».<br', '/><br', '/>If', 'you', 'ask', 'me', ',', 'there', 'are', 'lots', 'of', 'funny', 'gags', 'in', 'this', 'cartoon', '.', 'The', 'funniest', 'for', 'me', 'are', ',', 'for', 'example', ',', 'when', 'Mammy', 'Two', 'Shoes', 'sees', 'the', 'two', 'lazy', 'cats', 'sleeping', 'and', 'says', 'sarcastically', '«', 'I', \"'m\", 'glad', 'you', \"'re\", 'enjoying', 'the', 'siesta', '»', 'and', 'that', 'she', 'hopes', 'they', \"'re\", 'satisfied', 'because', 'she', 'ai', \"n't\", ',', 'making', 'the', 'two', 'cats', 'gasp', '.', 'Another', 'funny', 'gag', 'is', 'when', 'Tom', 'disguises', 'himself', 'as', 'Mammy', 'Two', 'Shoes', 'and', 'slams', 'Butch', 'with', 'a', 'frying', 'pan', 'and', 'then', 'Butch', 'does', 'the', 'same', 'trick', 'to', 'Tom', '.', 'Of', 'course', 'that', ',', 'even', 'funnier', 'than', 'this', ',', 'is', 'when', 'the', 'real', 'Mammy', 'Two', 'Shoes', 'appears', 'and', 'both', '(', 'dumb', '!', ')', 'cats', 'think', 'they', 'are', 'seeing', 'each', 'other', 'disguised', 'as', 'Mammy', 'and', 'then', 'they', 'both', 'attack', 'her', 'on', 'the', '\"', 'rear', '\"', '-', 'lol', '.', 'Naturally', 'that', 'she', 'gets', 'mad', 'and', 'once', 'she', 'gets', 'mad', ',', 'she', 'is', \"n't\", 'someone', 'to', 'mess', 'with', '.', 'But', 'even', 'Jerry', 'does', \"n't\", 'win', 'this', 'time', ',', 'because', 'he', 'is', 'expelled', 'by', 'her', 'too', '.'], 'label': 'pos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ0GwzojcTPk",
        "colab_type": "text"
      },
      "source": [
        "Generate the validation set with a `split_ratio` of 0.8 would mean 80% of the examples make up the training set and 20% make up the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1Is5m4pcdEf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4a91ef55-d724-4700-fe53-211006b18ba0"
      },
      "source": [
        "import random\n",
        "\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED), split_ratio = 0.8)\n",
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 20000\n",
            "Number of validation examples: 5000\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jug56cBOc0_5",
        "colab_type": "text"
      },
      "source": [
        "Next we have to build a _vocabulary_. This is effectively a look up table where every unique work in your data in your data set has a corresponding _index_ (an integer).\n",
        "\n",
        "We do this as our machine learning model cannot operate on strings, only numbers. Each _index_ is used to construct a _one-hot_ vector for each word. A one-hot vector is a vector where all of the elements are 0, except one, which is 1, and dimensionality is the total number of unique words in your vocabulary, commonly denoted by V.\n",
        "\n",
        "![alt text](https://doc-0s-4g-docs.googleusercontent.com/docs/securesc/ldgmc9f1rnrbpb7r2nci7mdkujir7e1k/sttjcf30i03i1ummpenuql67vsscr25u/1580385600000/15602990810144463660/04768977881078875371/1lrne4KntVuYW7SW-V_sP_Xk8y95vswO1?authuser=0)\n",
        "\n",
        "The number of unique words in our training set is over 100,000, which means that our one-hot vectors will have over 100,000 dimensions! This will make training slow and possibly won't fit onto your GPU (if you're using one).\n",
        "\n",
        "There are two ways to effectively cut down our vocabulary, we can either only take the top _n_ most common words or ignore words that appear less than _m_ times. We'll do the former, only keeping the top 25,000 words.\n",
        "\n",
        "What do we do with words that appear in examples but we have cut from the vocabulary? We replace them with a special _unknown_ or `<unk>` token. For example, if the sentence was \"This film is great and I love it\" but the word \"love\" was not in the vocabulary, it would become \"This film is great and I `<unk>` it\".\n",
        "\n",
        "The following builds the vocabulary, only keeping the most common `max_size` tokens.\n",
        "\n",
        "![alt text](https://doc-0s-4g-docs.googleusercontent.com/docs/securesc/ldgmc9f1rnrbpb7r2nci7mdkujir7e1k/thivloa32v6eegel2v5rksgm9fh6253a/1580385600000/15602990810144463660/04768977881078875371/1FybOlHRx0ayGZp5hWxuu3WuYaHMboV3I?authuser=0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjRJ0rKcedss",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0cbaf112-b21b-4191-95ec-391a0d809ed4"
      },
      "source": [
        "MAX_VOCAB_SIZE = 25000\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "print(f'Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}')\n",
        "print(f'Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}')\n",
        "print(TEXT.vocab.itos[:10])\n",
        "print(LABEL.vocab.stoi)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 25002\n",
            "Unique tokens in LABEL vocabulary: 2\n",
            "['<unk>', '<pad>', 'the', ',', '.', 'a', 'and', 'of', 'to', 'is']\n",
            "defaultdict(None, {'neg': 0, 'pos': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py0dYuHxhK02",
        "colab_type": "text"
      },
      "source": [
        "The final step of preparing the data is creating the iterators. We iterate over these in the training/evaluation loop, and they return a batch of examples (indexed and converted into tensors) at each iteration.\n",
        "\n",
        "We'll use a `BucketIterator` which is a special type of iterator that will return a batch of examples where each example is of a similar length, minimizing the amount of padding per example.\n",
        "\n",
        "We also want to place the tensors returned by the iterator on the GPU (if you're using one). PyTorch handles this using `torch.device`, we then pass this device to the iterator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhoKCdhfhqPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3bCfRzPh54n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "de746a02-dcc5-4423-cd5a-f7a8e01eb1f9"
      },
      "source": [
        "print(device)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}