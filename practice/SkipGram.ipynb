{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "SkipGram.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alanwuha/ce7455-nlp/blob/master/practice/SkipGram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5u9NLEgqwK9",
        "colab_type": "text"
      },
      "source": [
        "# Skip Gram\n",
        "- [Ref](https://blog.cambridgespark.com/tutorial-build-your-own-embedding-and-use-it-in-a-neural-network-e9cde4a81296?gi=1cb19fad9b7a)\n",
        "- [Code](https://github.com/DSKSD/DeepNLP-models-Pytorch/blob/master/notebooks/02.Skip-gram-Negative-Sampling.ipynb)\n",
        "- Edited by Hancheol Moon\n",
        "\n",
        "![skip-gram](https://cdn-images-1.medium.com/max/800/1*SR6l59udY05_bUICAjb6-w.png)\n",
        "\n",
        "- Skip-gram's objective is to predict the contexts, given a target word: V_t -> V_c\n",
        "- The contexts are immediate neighbours of the target and are retrieved using a window of an arbitrary size _n_\n",
        "  - Capturing _n_ words to the left of the target and _n_ words to its right.\n",
        "- In a two-gram example:\n",
        "\n",
        "![two-gram](https://nbviewer.jupyter.org/github/DSKSD/DeepNLP-models-Pytorch/blob/master/images/01.skipgram-prepare-data.png)\n",
        "\n",
        "- The original Skip-grams' objective is to maximize P(V_c|V_c): The probability of V_c being predicted as V_t's context for all training pairs.\n",
        "- To calculate P(V_c|V_t) we need a way to quantify the __closeness__ of the target-word around the context-word.\n",
        "- In Skip-gram, this closeness is computed using the __dot product between the input-embedding of the target and the output-embedding of the context__.\n",
        "\n",
        "Now, if we define u_t,c to be the measure of words' closeness between the target word and context word, _E_ to be the embedding matrix holding input-embeddings and _O_ to be the output-embedding matrix we get:\n",
        "\n",
        "u_t,c = E_t O_c\n",
        "\n",
        ", where _c_ is the context and _t_ is the target. With softmax,\n",
        "\n",
        "![architecture](https://cdn-images-1.medium.com/max/1600/1*4Viy_LvP6jLIWSvB9-Fk-Q.png)\n",
        "\n",
        "# Negative Sampling\n",
        "\n",
        "So far, we have studied the basics of Skip-gram, but there is an issue with the __original softmax objective of Skip-gram__. It is __highly computationally expensive__:\n",
        "- It requires scanning through the output-embeddings of all words in the vocabulary in order to calculate the sum from the __denominator__.\n",
        "- Typically such vocabularies contain hundreds of thousands of words.\n",
        "Because of this inefficiency, most implementations use an alternative, _negative-sampling objective_, which rephrases the problem as a set of independent binary classification tasks.\n",
        "\n",
        "Instead of defining the complete probability distribution over words, __the model learns to distinguish the correct training pairs from incorrect pairs, which are randomly generated pairs.__\n",
        "- Negative pair: Keep V_t and sample V_c from noise distribution\n",
        "- D: correct pairs\n",
        "- D': all negatively sampled |D| x m pairs\n",
        "- P(C = 1 | V_t, V_c): the probability of (V_t, V_t) being a correct pair\n",
        "\n",
        "For each sample we are making a binary decision we define P(C = 1|V_t, V_c) using the sigmoid function.\n",
        "\n",
        "Negative (context) samples are drawn from uniform distribution raised to the power of 3/4. Why? If you play with some sample values, you'll find that, compared to the simpler equation, this one has the tendency to increase the probability for less frequent words and decrease the probability for more frequent words.\n",
        "\n",
        "P(w) = Unif(W)^3/4 / Z,\n",
        "\n",
        "where Z is the normalization factor.\n",
        "\n",
        "__Sampling-based approaches completely do away with the softmax layer.__ They do this by approximating the normalization in the denominator of the softmax with some other loss that is cheap to compute. __However, sampling-based approaches are only useful at training time - - during inference, the full softmax still needs to be computed to obtain a normalised probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "827gXD6IBnfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import nltk\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "import pdb\n",
        "\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "random.seed(1024)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HStmj0FzCSaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getBatch(batch_size, train_data):\n",
        "  random.shuffle(train_data) # Shuffling is necessary. Why?\n",
        "  sindex = 0\n",
        "  eindex = batch_size\n",
        "  while eindex < len(train_data):\n",
        "    batch = train_data[sindex:eindex]\n",
        "    temp = eindex\n",
        "    eindex += batch_size\n",
        "    sindex = temp\n",
        "    yield batch  # yield is a keyword that is used like return, except the function will return a generator\n",
        "\n",
        "  if eindex >= len(train_data):\n",
        "    batch = train_data[sindex:]\n",
        "    yield batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1T0OSd_GfCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_sequence(seq, word2index):\n",
        "  l = lambda w: word2index[w] if word2index.get(w) is not None else word2index['<UNK>']\n",
        "  idxs = list(map(l, seq))\n",
        "  return torch.Tensor(idxs).type(torch.LongTensor)\n",
        "\n",
        "def prepare_word(word, word2index):\n",
        "  return torch.Tensor([word2index[word]]).type(torch.LongTensor) if word2index.get(word) is not None else LongTensor([word2index['<UNK>']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS1PZRnUIZDj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "027d2d45-c4cc-44f8-e559-a9adf4aa2058"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('gutenberg')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEp5i0f2IdIv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "d7c3626b-963c-4f28-d685-17e2b9ff513c"
      },
      "source": [
        "nltk.corpus.gutenberg.fileids()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo1b3osEIqAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = list(nltk.corpus.gutenberg.sents('melville-moby_dick.txt'))[:500]  # sampling sentences for test\n",
        "corpus = [[word.lower() for word in sent] for sent in corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w59U-sCWNnwi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "863b55b9-c4ed-48e2-bd60-4ca746d176ec"
      },
      "source": [
        "print(len(corpus))\n",
        "print(corpus[0], len(corpus[0]))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n",
            "['[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']'] 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thvILeW6NsWn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "45cad533-cd71-4daf-9959-e268a36085bf"
      },
      "source": [
        "# Exclude sparse words\n",
        "vocab = {}\n",
        "MIN_COUNT = 3\n",
        "\n",
        "for sentence in corpus:\n",
        "  for word in sentence:\n",
        "    if word not in vocab:\n",
        "      vocab[word] = 1\n",
        "    else:\n",
        "      vocab[word] += 1\n",
        "\n",
        "remove = []\n",
        "for word, count in vocab.items():\n",
        "  if count < MIN_COUNT:\n",
        "    remove.append(word)\n",
        "\n",
        "for word in remove:\n",
        "  vocab.pop(word)\n",
        "\n",
        "vocab['<UNK>'] = 0\n",
        "\n",
        "len(vocab)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "479"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3GHbQJ-PLSA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "da845a83-350b-47c0-9913-61fec155f10e"
      },
      "source": [
        "# Exclude sparse words\n",
        "MIN_COUNT = 3\n",
        "word_count = Counter(flatten(corpus))\n",
        "\n",
        "exclude = []\n",
        "for w, c in word_count.items():\n",
        "  if c < MIN_COUNT:\n",
        "    exclude.append(w)\n",
        "\n",
        "vocab = list(set(flatten(corpus)) - set(exclude))\n",
        "vocab.append('<UNK>')\n",
        "\n",
        "len(vocab)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "479"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkeo17C8P0-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2index = {'<UNK>': 0}\n",
        "\n",
        "for vo in vocab:\n",
        "  if word2index.get(vo) is None:\n",
        "    word2index[vo] = len(word2index)\n",
        "\n",
        "index2word = {v:k for k, v in word2index.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cheMUy6hQvIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WINDOW_SIZE = 5 # Range of context\n",
        "windows = flatten([list(nltk.ngrams(['<DUMMY>'] * WINDOW_SIZE + c + ['DUMMY'] * WINDOW_SIZE,\n",
        "                                  WINDOW_SIZE * 2 + 1)) for c in corpus])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OevybFX4Q-Og",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "ec11e307-c613-46d7-b7fa-25b2ccd51840"
      },
      "source": [
        "print(windows[0])\n",
        "print(windows[1])"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('<DUMMY>', '<DUMMY>', '<DUMMY>', '<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by', 'herman', 'melville')\n",
            "('<DUMMY>', '<DUMMY>', '<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by', 'herman', 'melville', '1851')\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}