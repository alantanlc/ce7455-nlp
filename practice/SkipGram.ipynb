{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "SkipGram.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alanwuha/ce7455-nlp/blob/master/practice/SkipGram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5u9NLEgqwK9",
        "colab_type": "text"
      },
      "source": [
        "# Skip Gram\n",
        "- [Ref](https://blog.cambridgespark.com/tutorial-build-your-own-embedding-and-use-it-in-a-neural-network-e9cde4a81296?gi=1cb19fad9b7a)\n",
        "- [Code](https://github.com/DSKSD/DeepNLP-models-Pytorch/blob/master/notebooks/02.Skip-gram-Negative-Sampling.ipynb)\n",
        "- Edited by Hancheol Moon\n",
        "\n",
        "![skip-gram](https://cdn-images-1.medium.com/max/800/1*SR6l59udY05_bUICAjb6-w.png)\n",
        "\n",
        "- Skip-gram's objective is to predict the contexts, given a target word: V_t -> V_c\n",
        "- The contexts are immediate neighbours of the target and are retrieved using a window of an arbitrary size _n_\n",
        "  - Capturing _n_ words to the left of the target and _n_ words to its right.\n",
        "- In a two-gram example:\n",
        "\n",
        "![two-gram](https://nbviewer.jupyter.org/github/DSKSD/DeepNLP-models-Pytorch/blob/master/images/01.skipgram-prepare-data.png)\n",
        "\n",
        "- The original Skip-grams' objective is to maximize P(V_c|V_c): The probability of V_c being predicted as V_t's context for all training pairs.\n",
        "- To calculate P(V_c|V_t) we need a way to quantify the __closeness__ of the target-word around the context-word.\n",
        "- In Skip-gram, this closeness is computed using the __dot product between the input-embedding of the target and the output-embedding of the context__.\n",
        "\n",
        "Now, if we define u_t,c to be the measure of words' closeness between the target word and context word, _E_ to be the embedding matrix holding input-embeddings and _O_ to be the output-embedding matrix we get:\n",
        "\n",
        "u_t,c = E_t O_c\n",
        "\n",
        ", where _c_ is the context and _t_ is the target. With softmax,\n",
        "\n",
        "![architecture](https://cdn-images-1.medium.com/max/1600/1*4Viy_LvP6jLIWSvB9-Fk-Q.png)\n",
        "\n",
        "# Negative Sampling\n",
        "\n",
        "So far, we have studied the basics of Skip-gram, but there is an issue with the __original softmax objective of Skip-gram__. It is __highly computationally expensive__:\n",
        "- It requires scanning through the output-embeddings of all words in the vocabulary in order to calculate the sum from the __denominator__.\n",
        "- Typically such vocabularies contain hundreds of thousands of words.\n",
        "Because of this inefficiency, most implementations use an alternative, _negative-sampling objective_, which rephrases the problem as a set of independent binary classification tasks.\n",
        "\n",
        "Instead of defining the complete probability distribution over words, __the model learns to distinguish the correct training pairs from incorrect pairs, which are randomly generated pairs.__\n",
        "- Negative pair: Keep V_t and sample V_c from noise distribution\n",
        "- D: correct pairs\n",
        "- D': all negatively sampled |D| x m pairs\n",
        "- P(C = 1 | V_t, V_c): the probability of (V_t, V_t) being a correct pair\n",
        "\n",
        "For each sample we are making a binary decision we define P(C = 1|V_t, V_c) using the sigmoid function.\n",
        "\n",
        "Negative (context) samples are drawn from uniform distribution raised to the power of 3/4. Why? If you play with some sample values, you'll find that, compared to the simpler equation, this one has the tendency to increase the probability for less frequent words and decrease the probability for more frequent words.\n",
        "\n",
        "P(w) = Unif(W)^3/4 / Z,\n",
        "\n",
        "where Z is the normalization factor.\n",
        "\n",
        "__Sampling-based approaches completely do away with the softmax layer.__ They do this by approximating the normalization in the denominator of the softmax with some other loss that is cheap to compute. __However, sampling-based approaches are only useful at training time - - during inference, the full softmax still needs to be computed to obtain a normalised probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSOeCNWGiNdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional\n",
        "\n",
        "import nltk\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "import pdb\n",
        "\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "random.seed(1024)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZHMuScZihA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getBatch(batch_size, train_data):\n",
        "  random.shuffle(train_data) # Shuffling is necessary. Why?\n",
        "  sindex = 0\n",
        "  eindex = batch_size\n",
        "  while eindex < len(train_data):\n",
        "    batch = train_data[sindex:eindex]\n",
        "    temp = eindex\n",
        "    eindex = eindex + batch_size\n",
        "    sindex = temp\n",
        "    yield batch\n",
        "\n",
        "  if eindex >= len(train_data):\n",
        "    batch = train_data[sindex:]\n",
        "    yield batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLZlEtR2mUQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_sequence(seq, word2index):\n",
        "  l = lambda w: word2index[w] if word2index.get(w) is not None else word2index['<UNK>']\n",
        "  idxs = list(map(l, seq))\n",
        "  return torch.Tensor(idxs).type(torch.LongTensor)\n",
        "\n",
        "def prepare_word(word, word2index):\n",
        "  return torch.Tensor([word2index[word]]).type(torch.LongTensor) if word2index.get(word) is not None else torch.LongTensor([word2index['<UNK>']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Idcm6jF0oSGd",
        "colab_type": "code",
        "outputId": "561f338f-8075-4ee6-c9d3-6708e2a4fff9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('gutenberg')"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGYhsYM0oqDe",
        "colab_type": "code",
        "outputId": "ace5fd4e-eabb-4d99-9ccb-5313399b6791",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "nltk.corpus.gutenberg.fileids()"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO8wr1ZAotaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = list(nltk.corpus.gutenberg.sents('melville-moby_dick.txt'))[:500] # sampling sentences for test\n",
        "corpus = [[word.lower() for word in sent] for sent in corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RlOV7nepKRu",
        "colab_type": "code",
        "outputId": "b31aaad9-b84b-41db-edb7-4cff821acdbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(len(corpus))\n",
        "print(corpus[0], len(corpus[0]))"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n",
            "['[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']'] 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eyi7hDE8rIUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Exclude sparse words\n",
        "MIN_COUNT = 3\n",
        "word_count = Counter(flatten(corpus))\n",
        "exclude = []\n",
        "for w, c in word_count.items():\n",
        "  if c < MIN_COUNT:\n",
        "    exclude.append(w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c73uKKLrsLmU",
        "colab_type": "code",
        "outputId": "00b5f117-7c75-489f-dec4-3c35a3bc5b34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab = list(set(flatten(corpus)) - set(exclude))\n",
        "vocab.append('<UNK>')\n",
        "len(vocab)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "479"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQaDrQ-rseDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2index = {'<UNK>': 0}\n",
        "\n",
        "for vo in vocab:\n",
        "  if word2index.get(vo) is None:\n",
        "    word2index[vo] = len(word2index)\n",
        "\n",
        "  index2word = {v:k for k, v in word2index.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBMeGloPs2CF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WINDOW_SIZE = 5 # Range of contexts\n",
        "windows = flatten([list(nltk.ngrams(['<DUMMY>'] * WINDOW_SIZE + c + ['<DUMMY>'] * WINDOW_SIZE,\n",
        "                                    WINDOW_SIZE * 2 + 1)) for c in corpus])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLrnhh9ItPDn",
        "colab_type": "code",
        "outputId": "2565d9c1-d3bc-4b04-f665-20c897d8f93d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(windows[0])\n",
        "print(windows[1])"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('<DUMMY>', '<DUMMY>', '<DUMMY>', '<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by', 'herman', 'melville')\n",
            "('<DUMMY>', '<DUMMY>', '<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by', 'herman', 'melville', '1851')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGZXviK0uOvJ",
        "colab_type": "code",
        "outputId": "bd095f56-814f-4be1-f485-e8b6a5f30165",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "list(nltk.ngrams(['<DUMMY>'] * WINDOW_SIZE + corpus[0] + ['<DUMMY>'] * WINDOW_SIZE, WINDOW_SIZE * 2 + 1))"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '[',\n",
              "  'moby',\n",
              "  'dick',\n",
              "  'by',\n",
              "  'herman',\n",
              "  'melville'),\n",
              " ('<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '[',\n",
              "  'moby',\n",
              "  'dick',\n",
              "  'by',\n",
              "  'herman',\n",
              "  'melville',\n",
              "  '1851'),\n",
              " ('<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '[',\n",
              "  'moby',\n",
              "  'dick',\n",
              "  'by',\n",
              "  'herman',\n",
              "  'melville',\n",
              "  '1851',\n",
              "  ']'),\n",
              " ('<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '[',\n",
              "  'moby',\n",
              "  'dick',\n",
              "  'by',\n",
              "  'herman',\n",
              "  'melville',\n",
              "  '1851',\n",
              "  ']',\n",
              "  '<DUMMY>'),\n",
              " ('<DUMMY>',\n",
              "  '[',\n",
              "  'moby',\n",
              "  'dick',\n",
              "  'by',\n",
              "  'herman',\n",
              "  'melville',\n",
              "  '1851',\n",
              "  ']',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>'),\n",
              " ('[',\n",
              "  'moby',\n",
              "  'dick',\n",
              "  'by',\n",
              "  'herman',\n",
              "  'melville',\n",
              "  '1851',\n",
              "  ']',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>'),\n",
              " ('moby',\n",
              "  'dick',\n",
              "  'by',\n",
              "  'herman',\n",
              "  'melville',\n",
              "  '1851',\n",
              "  ']',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>'),\n",
              " ('dick',\n",
              "  'by',\n",
              "  'herman',\n",
              "  'melville',\n",
              "  '1851',\n",
              "  ']',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssY5gPwMtTvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Training Set\n",
        "train_data = []\n",
        "for window in windows:\n",
        "  for i in range(WINDOW_SIZE * 2 + 1):\n",
        "    if window[i] in exclude or window[WINDOW_SIZE] in exclude:\n",
        "      continue  # min_count\n",
        "\n",
        "    if i == WINDOW_SIZE or window[i] == '<DUMMY>':\n",
        "      continue\n",
        "\n",
        "    # window[WINDOW_SIZE] : target word\n",
        "    # window[i]           : context word\n",
        "    train_data.append((window[WINDOW_SIZE], window[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj6eUGmY6S_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "ff14268b-b919-48dc-bae1-be5773410c55"
      },
      "source": [
        "print(windows[0])\n",
        "for w in windows[0]:\n",
        "  print(w in exclude)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('<DUMMY>', '<DUMMY>', '<DUMMY>', '<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by', 'herman', 'melville')\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "True\n",
            "True\n",
            "False\n",
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRfBOav2yaL0",
        "colab_type": "code",
        "outputId": "a941dd19-53d8-40ce-bbed-379dcf81c7bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# 2-Gram dataset\n",
        "train_data[:10]"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('(', 'supplied'),\n",
              " ('(', 'by'),\n",
              " ('(', 'a'),\n",
              " ('(', 'late'),\n",
              " ('supplied', '('),\n",
              " ('supplied', 'by'),\n",
              " ('supplied', 'a'),\n",
              " ('supplied', 'late'),\n",
              " ('by', '('),\n",
              " ('by', 'supplied')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZQscOIdyt-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_p = []\n",
        "y_p = []\n",
        "\n",
        "for tr in train_data:\n",
        "  X_p.append(prepare_word(tr[0], word2index).view(1, -1))\n",
        "  y_p.append(prepare_word(tr[1], word2index).view(1, -1))\n",
        "\n",
        "train_data = list(zip(X_p, y_p))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bxtTIO77jIT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "c1e10e9e-046d-431e-fea0-ec7aa2fad52c"
      },
      "source": [
        "train_data[:10]"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(tensor([[411]]), tensor([[1]])),\n",
              " (tensor([[411]]), tensor([[356]])),\n",
              " (tensor([[411]]), tensor([[237]])),\n",
              " (tensor([[411]]), tensor([[262]])),\n",
              " (tensor([[1]]), tensor([[411]])),\n",
              " (tensor([[1]]), tensor([[356]])),\n",
              " (tensor([[1]]), tensor([[237]])),\n",
              " (tensor([[1]]), tensor([[262]])),\n",
              " (tensor([[356]]), tensor([[411]])),\n",
              " (tensor([[356]]), tensor([[1]]))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDbFXmTp78vr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e6dd04c4-6f27-409b-9b60-bc28f480e103"
      },
      "source": [
        "index2word[411]"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'('"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrSeYqfq8ZYB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b2c3199-55ac-4f8a-f40f-31dfe0e0dec5"
      },
      "source": [
        "len(train_data)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50242"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glaWZvF08doR",
        "colab_type": "text"
      },
      "source": [
        "### Unigram Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAXqeHAs8fu5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_count = Counter(flatten(corpus))\n",
        "num_total_words = sum([c for w, c in word_count.items() if w not in exclude])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCJnH6jE8olT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha = 3/4\n",
        "noise_dist = {key: val / num_total_words ** alpha for key, val in word_count.items() if key not in exclude}\n",
        "Z = sum(noise_dist.values())\n",
        "\n",
        "noise_dist_normalized = {key: val / Z for key, val in noise_dist.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "fa4f8304-9808-4d60-c730-846343bef209",
        "id": "O3TGdlecBee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(f\"Normalization factor Z:           {Z:.7}\")\n",
        "print(f'Noise distribution:               {noise_dist[\"by\"]:.6}')\n",
        "print(f'Normalized noise distribution:    {noise_dist_normalized[\"by\"]:.6}')"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalization factor Z:           9.397142\n",
            "Noise distribution:               0.0566383\n",
            "Normalized noise distribution:    0.00602719\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbFNiKU8BfvA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f4c289f0-e4a6-470d-d6a5-1aeb4a8e6c54"
      },
      "source": [
        "K = 10\n",
        "np.random.choice(list(noise_dist_normalized.keys()), size=K, p=list(noise_dist_normalized.values()), replace=True)"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\"'\", 'the', '.', 'the', 'that', 'he', 'we', 'city', ';', 'the'],\n",
              "      dtype='<U11')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46uueHsTCdYG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "36247421-55a5-4383-a8cd-4743dd0c0f8f"
      },
      "source": [
        "np.random.choice(['a', 'b', 'c', 'd'], size=4, p=[0.5, 0.5, 0, 0])"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['a', 'a', 'a', 'a'], dtype='<U1')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpguQn6gC4vT",
        "colab_type": "text"
      },
      "source": [
        "### Simple Question: What is the difference between nn.Embedding() and nn.Linear()?\n",
        "- Word embedding doesn't assume any bias. It assumes a zero-centered distribution\n",
        "- Following codes represent the same network structure:\n",
        "  - `nn.Linear(vocab_size, embed_dim, bias=False)`\n",
        "  - `nn.Embedding(vocab_size, embed_dim)`\n",
        "- However, `nn.Embedding()` is more efficient, why?\n",
        "  - Input of `nn.Embedding()` is integer: index of one-hot-vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU8rbjjBDU-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def negative_sampling(targets, noise_dist_normalized, k):\n",
        "  batch_size = targets.size(0)\n",
        "  neg_samples = []\n",
        "  for i in range(batch_size):\n",
        "    nsample = []\n",
        "    if device == 'cuda':\n",
        "      # GPU -> CPU\n",
        "      target_index = targets[i].data.cpu().tolist()[0]  # PyTorch Tensor -> List\n",
        "    else:\n",
        "      target_index = targets[i].data.tolist()[0]\n",
        "\n",
        "    while len(nsample) < k: # num of sampling\n",
        "      neg = np.random.choice(list(noise_dist_normalized.keys()),\n",
        "                             size=1, p=list(noise_dist_normalized.values()))\n",
        "      \n",
        "      neg_word = neg[0]\n",
        "      if word2index[neg_word] == target_index:\n",
        "        continue\n",
        "      nsample.append(neg_word)\n",
        "    neg_samples.append(prepare_sequence(nsample, word2index).view(1, -1))\n",
        "  return torch.cat(neg_samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EHOY3DwEr6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SkipgramNegSampling(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embed_dim):\n",
        "    super(SkipgramNegSampling, self).__init__()\n",
        "    self.embedding_v = nn.Embedding(vocab_size, embed_dim)  # center embedding\n",
        "    self.embedding_u = nn.Embedding(vocab_size, embed_dim)  # out embedding\n",
        "    self.logsigmoid = nn.LogSigmoid()\n",
        "\n",
        "    nn.init.xavier_normal_(self.embedding_v.weight)\n",
        "    nn.init.xavier_normal(self.embedding_u.weight)\n",
        "\n",
        "  def forward(self, center_words, target_words, negative_words):\n",
        "    center_embeds = self.embedding_v(center_words)  # B x 1 x D\n",
        "    target_embeds = self.embedding_u(target_words)  # B x 1 x D\n",
        "\n",
        "    neg_embeds = - self.embedding_u(negative_words) # B x K x D\n",
        "\n",
        "    positive_score = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2) # Bx1\n",
        "    negative_score = torch.sum(neg_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2), 1).view(negs.size(0), -1)  # BxK -> Bx1\n",
        "\n",
        "    loss = self.logsigmoid(positive_score) + self.logsigmoid(negative_score)\n",
        "\n",
        "    return -torch.mean(loss)\n",
        "\n",
        "  def prediction(self, inputs):\n",
        "    embeds = self.embedding_v(inputs)\n",
        "    return embeds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTAOUZMyGzJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_SIZE = 30\n",
        "BATCH_SIZE = 256\n",
        "EPOCH = 30\n",
        "NEG = 10 # Num of Negative Sampling"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}