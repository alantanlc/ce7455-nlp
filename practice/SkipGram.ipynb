{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "SkipGram.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alanwuha/ce7455-nlp/blob/master/practice/SkipGram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5u9NLEgqwK9",
        "colab_type": "text"
      },
      "source": [
        "# Skip Gram\n",
        "- [Ref](https://blog.cambridgespark.com/tutorial-build-your-own-embedding-and-use-it-in-a-neural-network-e9cde4a81296?gi=1cb19fad9b7a)\n",
        "- [Code](https://github.com/DSKSD/DeepNLP-models-Pytorch/blob/master/notebooks/02.Skip-gram-Negative-Sampling.ipynb)\n",
        "- Edited by Hancheol Moon\n",
        "\n",
        "![skip-gram](https://cdn-images-1.medium.com/max/800/1*SR6l59udY05_bUICAjb6-w.png)\n",
        "\n",
        "- Skip-gram's objective is to predict the contexts, given a target word: V_t -> V_c\n",
        "- The contexts are immediate neighbours of the target and are retrieved using a window of an arbitrary size _n_\n",
        "  - Capturing _n_ words to the left of the target and _n_ words to its right.\n",
        "- In a two-gram example:\n",
        "\n",
        "![two-gram](https://nbviewer.jupyter.org/github/DSKSD/DeepNLP-models-Pytorch/blob/master/images/01.skipgram-prepare-data.png)\n",
        "\n",
        "- The original Skip-grams' objective is to maximize P(V_c|V_c): The probability of V_c being predicted as V_t's context for all training pairs.\n",
        "- To calculate P(V_c|V_t) we need a way to quantify the __closeness__ of the target-word around the context-word.\n",
        "- In Skip-gram, this closeness is computed using the __dot product between the input-embedding of the target and the output-embedding of the context__.\n",
        "\n",
        "Now, if we define u_t,c to be the measure of words' closeness between the target word and context word, _E_ to be the embedding matrix holding input-embeddings and _O_ to be the output-embedding matrix we get:\n",
        "\n",
        "u_t,c = E_t O_c\n",
        "\n",
        ", where _c_ is the context and _t_ is the target. With softmax,\n",
        "\n",
        "![architecture](https://cdn-images-1.medium.com/max/1600/1*4Viy_LvP6jLIWSvB9-Fk-Q.png)\n",
        "\n",
        "# Negative Sampling\n",
        "\n",
        "So far, we have studied the basics of Skip-gram, but there is an issue with the __original softmax objective of Skip-gram__. It is __highly computationally expensive__:\n",
        "- It requires scanning through the output-embeddings of all words in the vocabulary in order to calculate the sum from the __denominator__.\n",
        "- Typically such vocabularies contain hundreds of thousands of words.\n",
        "Because of this inefficiency, most implementations use an alternative, _negative-sampling objective_, which rephrases the problem as a set of independent binary classification tasks.\n",
        "\n",
        "Instead of defining the complete probability distribution over words, __the model learns to distinguish the correct training pairs from incorrect pairs, which are randomly generated pairs.__\n",
        "- Negative pair: Keep V_t and sample V_c from noise distribution\n",
        "- D: correct pairs\n",
        "- D': all negatively sampled |D| x m pairs\n",
        "- P(C = 1 | V_t, V_c): the probability of (V_t, V_t) being a correct pair\n",
        "\n",
        "For each sample we are making a binary decision we define P(C = 1|V_t, V_c) using the sigmoid function.\n",
        "\n",
        "Negative (context) samples are drawn from uniform distribution raised to the power of 3/4. Why? If you play with some sample values, you'll find that, compared to the simpler equation, this one has the tendency to increase the probability for less frequent words and decrease the probability for more frequent words.\n",
        "\n",
        "P(w) = Unif(W)^3/4 / Z,\n",
        "\n",
        "where Z is the normalization factor.\n",
        "\n",
        "__Sampling-based approaches completely do away with the softmax layer.__ They do this by approximating the normalization in the denominator of the softmax with some other loss that is cheap to compute. __However, sampling-based approaches are only useful at training time - - during inference, the full softmax still needs to be computed to obtain a normalised probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSOeCNWGiNdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional\n",
        "\n",
        "import nltk\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "import pdb\n",
        "\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]\n",
        "random.seed(1024)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZHMuScZihA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getBatch(batch_size, train_data):\n",
        "  random.shuffle(train_data) # Shuffling is necessary. Why?\n",
        "  sindex = 0\n",
        "  eindex = batch_size\n",
        "  while eindex < len(train_data):\n",
        "    batch = train_data[sindex:eindex]\n",
        "    temp = eindex\n",
        "    eindex = eindex + batch_size\n",
        "    sindex = temp\n",
        "    yield batch\n",
        "\n",
        "  if eindex >= len(train_data):\n",
        "    batch = train_data[sindex:]\n",
        "    yield batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLZlEtR2mUQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_sequence(seq, word2index):\n",
        "  l = lambda w: word2index[w] if word2index.get(w) is not None else word2index['<UNK>']\n",
        "  idxs = list(map(l, seq))\n",
        "  return torch.Tensor(idxs).type(torch.LongTensor)\n",
        "\n",
        "def prepare_word(word, word2index):\n",
        "  return torch.Tensor([word2index[word]]).type(torch.LongTensor) if word2index.get(word) is not None else torch.LongTensor([word2index['<UNK>']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Idcm6jF0oSGd",
        "colab_type": "code",
        "outputId": "32a25b75-45b1-4b93-a0d2-e0d96c99d106",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('gutenberg')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGYhsYM0oqDe",
        "colab_type": "code",
        "outputId": "cb54efea-f66c-4517-c3a5-403a02fde2b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "nltk.corpus.gutenberg.fileids()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO8wr1ZAotaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = list(nltk.corpus.gutenberg.sents('melville-moby_dick.txt'))[:500] # sampling sentences for test\n",
        "corpus = [[word.lower() for word in sent] for sent in corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RlOV7nepKRu",
        "colab_type": "code",
        "outputId": "c862926c-9d65-4389-a3cf-38af4ceeb16a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(len(corpus))\n",
        "print(corpus[0], len(corpus[0]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n",
            "['[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']'] 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eyi7hDE8rIUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Exclude sparse words\n",
        "MIN_COUNT = 3\n",
        "word_count = Counter(flatten(corpus))\n",
        "exclude = []\n",
        "for w, c in word_count.items():\n",
        "  if c < MIN_COUNT:\n",
        "    exclude.append(w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c73uKKLrsLmU",
        "colab_type": "code",
        "outputId": "060fbc47-21d1-46b1-e34e-13ee7cae87ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab = list(set(flatten(corpus)) - set(exclude))\n",
        "vocab.append('<UNK>')\n",
        "len(vocab)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "479"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQaDrQ-rseDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2index = {'<UNK>': 0}\n",
        "\n",
        "for vo in vocab:\n",
        "  if word2index.get(vo) is None:\n",
        "    word2index[vo] = len(word2index)\n",
        "\n",
        "  index2word = {v:k for k, v in word2index.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBMeGloPs2CF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WINDOW_SIZE = 5 # Range of contexts\n",
        "windows = flatten([list(nltk.ngrams(['<DUMMY>'] * WINDOW_SIZE + c + ['<DUMMY>'] * WINDOW_SIZE,\n",
        "                                    WINDOW_SIZE * 2 + 1)) for c in corpus])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLrnhh9ItPDn",
        "colab_type": "code",
        "outputId": "425ca73d-6c7d-4bc3-9452-36f5bf6deb89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(windows[0])\n",
        "print(windows[1])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('<DUMMY>', '<DUMMY>', '<DUMMY>', '<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by', 'herman', 'melville')\n",
            "('<DUMMY>', '<DUMMY>', '<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by', 'herman', 'melville', '1851')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGZXviK0uOvJ",
        "colab_type": "code",
        "outputId": "ac63cf53-266f-4ede-bcf6-ff57d5e767de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "list(nltk.ngrams(['<DUMMY>'] * WINDOW_SIZE + corpus[0] + ['<DUMMY>'] * WINDOW_SIZE, WINDOW_SIZE * 2 + 1))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '[',\n",
              "  'moby',\n",
              "  'dick',\n",
              "  'by',\n",
              "  'herman',\n",
              "  'melville'),\n",
              " ('<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '[',\n",
              "  'moby',\n",
              "  'dick',\n",
              "  'by',\n",
              "  'herman',\n",
              "  'melville',\n",
              "  '1851'),\n",
              " ('<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '[',\n",
              "  'moby',\n",
              "  'dick',\n",
              "  'by',\n",
              "  'herman',\n",
              "  'melville',\n",
              "  '1851',\n",
              "  ']'),\n",
              " ('<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '[',\n",
              "  'moby',\n",
              "  'dick',\n",
              "  'by',\n",
              "  'herman',\n",
              "  'melville',\n",
              "  '1851',\n",
              "  ']',\n",
              "  '<DUMMY>'),\n",
              " ('<DUMMY>',\n",
              "  '[',\n",
              "  'moby',\n",
              "  'dick',\n",
              "  'by',\n",
              "  'herman',\n",
              "  'melville',\n",
              "  '1851',\n",
              "  ']',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>'),\n",
              " ('[',\n",
              "  'moby',\n",
              "  'dick',\n",
              "  'by',\n",
              "  'herman',\n",
              "  'melville',\n",
              "  '1851',\n",
              "  ']',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>'),\n",
              " ('moby',\n",
              "  'dick',\n",
              "  'by',\n",
              "  'herman',\n",
              "  'melville',\n",
              "  '1851',\n",
              "  ']',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>'),\n",
              " ('dick',\n",
              "  'by',\n",
              "  'herman',\n",
              "  'melville',\n",
              "  '1851',\n",
              "  ']',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>',\n",
              "  '<DUMMY>')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssY5gPwMtTvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create Training Set\n",
        "train_data = []\n",
        "for window in windows:\n",
        "  for i in range(WINDOW_SIZE * 2 + 1):\n",
        "    if window[i] in exclude or window[WINDOW_SIZE] in exclude:\n",
        "      continue  # min_count\n",
        "\n",
        "    if i == WINDOW_SIZE or window[i] == '<DUMMY>':\n",
        "      continue\n",
        "\n",
        "    # window[WINDOW_SIZE] : target word\n",
        "    # window[i]           : context word\n",
        "    train_data.append((window[WINDOW_SIZE], window[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj6eUGmY6S_H",
        "colab_type": "code",
        "outputId": "1f808764-4669-4cb8-ce2e-d8007753bf2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "print(windows[0])\n",
        "for w in windows[0]:\n",
        "  print(w in exclude)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('<DUMMY>', '<DUMMY>', '<DUMMY>', '<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by', 'herman', 'melville')\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "False\n",
            "True\n",
            "True\n",
            "True\n",
            "False\n",
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRfBOav2yaL0",
        "colab_type": "code",
        "outputId": "6e8c8085-ead1-4454-a345-885c7be343f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# 2-Gram dataset\n",
        "train_data[:10]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('(', 'supplied'),\n",
              " ('(', 'by'),\n",
              " ('(', 'a'),\n",
              " ('(', 'late'),\n",
              " ('supplied', '('),\n",
              " ('supplied', 'by'),\n",
              " ('supplied', 'a'),\n",
              " ('supplied', 'late'),\n",
              " ('by', '('),\n",
              " ('by', 'supplied')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZQscOIdyt-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_p = []\n",
        "y_p = []\n",
        "\n",
        "for tr in train_data:\n",
        "  X_p.append(prepare_word(tr[0], word2index).view(1, -1))\n",
        "  y_p.append(prepare_word(tr[1], word2index).view(1, -1))\n",
        "\n",
        "train_data = list(zip(X_p, y_p))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bxtTIO77jIT",
        "colab_type": "code",
        "outputId": "4315dc4b-70ff-4a6c-9f65-6cda003352aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "train_data[:10]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(tensor([[34]]), tensor([[3]])),\n",
              " (tensor([[34]]), tensor([[100]])),\n",
              " (tensor([[34]]), tensor([[64]])),\n",
              " (tensor([[34]]), tensor([[159]])),\n",
              " (tensor([[3]]), tensor([[34]])),\n",
              " (tensor([[3]]), tensor([[100]])),\n",
              " (tensor([[3]]), tensor([[64]])),\n",
              " (tensor([[3]]), tensor([[159]])),\n",
              " (tensor([[100]]), tensor([[34]])),\n",
              " (tensor([[100]]), tensor([[3]]))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDbFXmTp78vr",
        "colab_type": "code",
        "outputId": "dac7abfd-6489-4e8d-d6c7-f5e42c99d405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "index2word[411]"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'then'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrSeYqfq8ZYB",
        "colab_type": "code",
        "outputId": "69763147-d23d-4dc3-f486-5edbb28fc811",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_data)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50242"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glaWZvF08doR",
        "colab_type": "text"
      },
      "source": [
        "### Unigram Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAXqeHAs8fu5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_count = Counter(flatten(corpus))\n",
        "num_total_words = sum([c for w, c in word_count.items() if w not in exclude])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCJnH6jE8olT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha = 3/4\n",
        "noise_dist = {key: val / num_total_words ** alpha for key, val in word_count.items() if key not in exclude}\n",
        "Z = sum(noise_dist.values())\n",
        "\n",
        "noise_dist_normalized = {key: val / Z for key, val in noise_dist.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3d02f8f5-5dfb-4920-9bfa-a6f65c27859b",
        "id": "O3TGdlecBee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(f\"Normalization factor Z:           {Z:.7}\")\n",
        "print(f'Noise distribution:               {noise_dist[\"by\"]:.6}')\n",
        "print(f'Normalized noise distribution:    {noise_dist_normalized[\"by\"]:.6}')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalization factor Z:           9.397142\n",
            "Noise distribution:               0.0566383\n",
            "Normalized noise distribution:    0.00602719\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KElf_LXclF_K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "outputId": "6c6baba6-7876-4127-b647-e10145b748a5"
      },
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "total = 100\n",
        "alpha = 3/4\n",
        "raised = total ** alpha\n",
        "\n",
        "counts = pd.Series(list(range(0, 110, 10)))\n",
        "dist = counts.apply(lambda x: x/total)\n",
        "noise_dist = counts.apply(lambda x: x/raised)\n",
        "norm_factor = sum(noise_dist)\n",
        "noise_dist_norm = noise_dist.apply(lambda x: x/norm_factor)\n",
        "\n",
        "df = pd.DataFrame({'counts': counts,\n",
        "                   'dist': dist,\n",
        "                   'noise_dist': noise_dist,\n",
        "                   'noise_dist_norm': noise_dist_norm})\n",
        "\n",
        "display(df)\n",
        "print('total:', total)\n",
        "print('raised:', raised)\n",
        "print('norm_factor:', norm_factor)\n",
        "print('sum', sum(noise_dist_norm))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>counts</th>\n",
              "      <th>dist</th>\n",
              "      <th>noise_dist</th>\n",
              "      <th>noise_dist_norm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.316228</td>\n",
              "      <td>0.018182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.632456</td>\n",
              "      <td>0.036364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>30</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.948683</td>\n",
              "      <td>0.054545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>40</td>\n",
              "      <td>0.4</td>\n",
              "      <td>1.264911</td>\n",
              "      <td>0.072727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>50</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.581139</td>\n",
              "      <td>0.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>60</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1.897367</td>\n",
              "      <td>0.109091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>70</td>\n",
              "      <td>0.7</td>\n",
              "      <td>2.213594</td>\n",
              "      <td>0.127273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>80</td>\n",
              "      <td>0.8</td>\n",
              "      <td>2.529822</td>\n",
              "      <td>0.145455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>90</td>\n",
              "      <td>0.9</td>\n",
              "      <td>2.846050</td>\n",
              "      <td>0.163636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>100</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.162278</td>\n",
              "      <td>0.181818</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    counts  dist  noise_dist  noise_dist_norm\n",
              "0        0   0.0    0.000000         0.000000\n",
              "1       10   0.1    0.316228         0.018182\n",
              "2       20   0.2    0.632456         0.036364\n",
              "3       30   0.3    0.948683         0.054545\n",
              "4       40   0.4    1.264911         0.072727\n",
              "5       50   0.5    1.581139         0.090909\n",
              "6       60   0.6    1.897367         0.109091\n",
              "7       70   0.7    2.213594         0.127273\n",
              "8       80   0.8    2.529822         0.145455\n",
              "9       90   0.9    2.846050         0.163636\n",
              "10     100   1.0    3.162278         0.181818"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "total: 100\n",
            "raised: 31.622776601683793\n",
            "norm_factor: 17.392527130926087\n",
            "sum 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Vs0S7Bmr5KT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7cf2d77c-5f91-40b2-c1b9-99f7154e33ff"
      },
      "source": [
        "list(range(10))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7nt1Y3KrGQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbFNiKU8BfvA",
        "colab_type": "code",
        "outputId": "183c03e5-eac7-467c-aafc-fc943bd26663",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "K = 10\n",
        "np.random.choice(list(noise_dist_normalized.keys()), size=K, p=list(noise_dist_normalized.values()), replace=True)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['in', '.', 'over', 'of', 'from', 'no', 'the', '--', 'have', 'the'],\n",
              "      dtype='<U11')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46uueHsTCdYG",
        "colab_type": "code",
        "outputId": "f6014eae-1f04-45b2-d6f2-cd241d610a1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.random.choice(['a', 'b', 'c', 'd'], size=4, p=[0.5, 0.5, 0, 0])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['b', 'b', 'a', 'b'], dtype='<U1')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpguQn6gC4vT",
        "colab_type": "text"
      },
      "source": [
        "### Simple Question: What is the difference between nn.Embedding() and nn.Linear()?\n",
        "- Word embedding doesn't assume any bias. It assumes a zero-centered distribution\n",
        "- Following codes represent the same network structure:\n",
        "  - `nn.Linear(vocab_size, embed_dim, bias=False)`\n",
        "  - `nn.Embedding(vocab_size, embed_dim)`\n",
        "- However, `nn.Embedding()` is more efficient, why?\n",
        "  - Input of `nn.Embedding()` is integer: index of one-hot-vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU8rbjjBDU-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def negative_sampling(targets, noise_dist_normalized, k):\n",
        "  batch_size = targets.size(0)\n",
        "  neg_samples = []\n",
        "  for i in range(batch_size):\n",
        "    nsample = []\n",
        "    if device == 'cuda':\n",
        "      # GPU -> CPU\n",
        "      target_index = targets[i].data.cpu().tolist()[0]  # PyTorch Tensor -> List\n",
        "    else:\n",
        "      target_index = targets[i].data.tolist()[0]\n",
        "\n",
        "    while len(nsample) < k: # num of sampling\n",
        "      neg = np.random.choice(list(noise_dist_normalized.keys()),\n",
        "                             size=1, p=list(noise_dist_normalized.values()))\n",
        "      neg_word = neg[0]\n",
        "      if word2index[neg_word] == target_index:\n",
        "        continue\n",
        "      nsample.append(neg_word)\n",
        "    neg_samples.append(prepare_sequence(nsample, word2index).view(1, -1))\n",
        "\n",
        "  return torch.cat(neg_samples)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EHOY3DwEr6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SkipgramNegSampling(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embed_dim):\n",
        "    super(SkipgramNegSampling, self).__init__()\n",
        "    self.embedding_v = nn.Embedding(vocab_size, embed_dim)  # center embedding\n",
        "    self.embedding_u = nn.Embedding(vocab_size, embed_dim)  # output embedding\n",
        "    self.logsigmoid = nn.LogSigmoid()\n",
        "\n",
        "    nn.init.xavier_normal_(self.embedding_v.weight)\n",
        "    nn.init.xavier_normal_(self.embedding_u.weight)\n",
        "\n",
        "  def forward(self, center_words, target_words, negative_words):\n",
        "    center_embeds = self.embedding_v(center_words)\n",
        "    target_embeds = self.embedding_u(target_words)\n",
        "\n",
        "    neg_embeds = - self.embedding_u(negative_words)\n",
        "\n",
        "    positive_score = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
        "    negative_score = torch.sum(neg_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2), 1).view(negs.size(0), -1)\n",
        "\n",
        "    loss = self.logsigmoid(positive_score) + self.logsigmoid(negative_score)\n",
        "\n",
        "    return -torch.mean(loss)\n",
        "\n",
        "  def prediction(self, inputs):\n",
        "    embeds = self.embedding_v(inputs)\n",
        "\n",
        "    return embeds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTAOUZMyGzJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_SIZE = 30\n",
        "BATCH_SIZE = 256\n",
        "EPOCH = 30\n",
        "NEG = 10 # Num of Negative Sampling"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EKlmIM3yfg1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "4604a297-616c-48f8-babf-09c6b3166f74"
      },
      "source": [
        "import torch\n",
        "help(torch.bmm)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on built-in function bmm:\n",
            "\n",
            "bmm(...)\n",
            "    bmm(input, mat2, out=None) -> Tensor\n",
            "    \n",
            "    Performs a batch matrix-matrix product of matrices stored in :attr:`input`\n",
            "    and :attr:`mat2`.\n",
            "    \n",
            "    :attr:`input` and :attr:`mat2` must be 3-D tensors each containing\n",
            "    the same number of matrices.\n",
            "    \n",
            "    If :attr:`input` is a :math:`(b \\times n \\times m)` tensor, :attr:`mat2` is a\n",
            "    :math:`(b \\times m \\times p)` tensor, :attr:`out` will be a\n",
            "    :math:`(b \\times n \\times p)` tensor.\n",
            "    \n",
            "    .. math::\n",
            "        \\text{out}_i = \\text{input}_i \\mathbin{@} \\text{mat2}_i\n",
            "    \n",
            "    .. note:: This function does not :ref:`broadcast <broadcasting-semantics>`.\n",
            "              For broadcasting matrix products, see :func:`torch.matmul`.\n",
            "    \n",
            "    Args:\n",
            "        input (Tensor): the first batch of matrices to be multiplied\n",
            "        mat2 (Tensor): the second batch of matrices to be multiplied\n",
            "        out (Tensor, optional): the output tensor.\n",
            "    \n",
            "    Example::\n",
            "    \n",
            "        >>> input = torch.randn(10, 3, 4)\n",
            "        >>> mat2 = torch.randn(10, 4, 5)\n",
            "        >>> res = torch.bmm(input, mat2)\n",
            "        >>> res.size()\n",
            "        torch.Size([10, 3, 5])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81tfaXvb0AKq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a931684-f271-45f5-d859-c1273e6d3b98"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Device: {device}')\n",
        "\n",
        "losses = []\n",
        "model = SkipgramNegSampling(len(word2index), EMBEDDING_SIZE)\n",
        "if device == 'cuda':\n",
        "  model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJrwuKmm4dxr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "930d37c6-8e19-45b2-b8f0-7965e24f00d7"
      },
      "source": [
        "model"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SkipgramNegSampling(\n",
              "  (embedding_v): Embedding(479, 30)\n",
              "  (embedding_u): Embedding(479, 30)\n",
              "  (logsigmoid): LogSigmoid()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPcw_qTe4fZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_lr(optimizer):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    return param_group['lr']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAOmtx904mLZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "30196403-cbb0-4654-f6bc-ff0ebe5ce4ae"
      },
      "source": [
        "for epoch in range(EPOCH):\n",
        "  scheduler.step()\n",
        "  for i, batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
        "    inputs, targets = zip(*batch)\n",
        "\n",
        "    inputs = torch.cat(inputs).to(device)\n",
        "    targets = torch.cat(targets).to(device)\n",
        "    negs = negative_sampling(targets, noise_dist_normalized, NEG).to(device)\n",
        "\n",
        "    model.zero_grad()\n",
        "\n",
        "    loss = model(inputs, targets, negs)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i % 20 == 0:\n",
        "      lr = get_lr(optimizer)\n",
        "      print(f'Epoch: {epoch} || Iter: {i} || Loss: {loss:.2f} || LR: {lr:.6f}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 || Iter: 0 || Loss: 1.39 || LR: 0.001000\n",
            "Epoch: 0 || Iter: 20 || Loss: 1.37 || LR: 0.001000\n",
            "Epoch: 0 || Iter: 40 || Loss: 1.32 || LR: 0.001000\n",
            "Epoch: 0 || Iter: 60 || Loss: 1.22 || LR: 0.001000\n",
            "Epoch: 0 || Iter: 80 || Loss: 1.10 || LR: 0.001000\n",
            "Epoch: 0 || Iter: 100 || Loss: 1.00 || LR: 0.001000\n",
            "Epoch: 0 || Iter: 120 || Loss: 0.96 || LR: 0.001000\n",
            "Epoch: 0 || Iter: 140 || Loss: 0.93 || LR: 0.001000\n",
            "Epoch: 0 || Iter: 160 || Loss: 0.92 || LR: 0.001000\n",
            "Epoch: 0 || Iter: 180 || Loss: 0.92 || LR: 0.001000\n",
            "Epoch: 1 || Iter: 0 || Loss: 0.91 || LR: 0.001000\n",
            "Epoch: 1 || Iter: 20 || Loss: 0.91 || LR: 0.001000\n",
            "Epoch: 1 || Iter: 40 || Loss: 0.90 || LR: 0.001000\n",
            "Epoch: 1 || Iter: 60 || Loss: 0.90 || LR: 0.001000\n",
            "Epoch: 1 || Iter: 80 || Loss: 0.91 || LR: 0.001000\n",
            "Epoch: 1 || Iter: 100 || Loss: 0.90 || LR: 0.001000\n",
            "Epoch: 1 || Iter: 120 || Loss: 0.91 || LR: 0.001000\n",
            "Epoch: 1 || Iter: 140 || Loss: 0.90 || LR: 0.001000\n",
            "Epoch: 1 || Iter: 160 || Loss: 0.90 || LR: 0.001000\n",
            "Epoch: 1 || Iter: 180 || Loss: 0.90 || LR: 0.001000\n",
            "Epoch: 2 || Iter: 0 || Loss: 0.90 || LR: 0.001000\n",
            "Epoch: 2 || Iter: 20 || Loss: 0.89 || LR: 0.001000\n",
            "Epoch: 2 || Iter: 40 || Loss: 0.90 || LR: 0.001000\n",
            "Epoch: 2 || Iter: 60 || Loss: 0.90 || LR: 0.001000\n",
            "Epoch: 2 || Iter: 80 || Loss: 0.90 || LR: 0.001000\n",
            "Epoch: 2 || Iter: 100 || Loss: 0.90 || LR: 0.001000\n",
            "Epoch: 2 || Iter: 120 || Loss: 0.89 || LR: 0.001000\n",
            "Epoch: 2 || Iter: 140 || Loss: 0.89 || LR: 0.001000\n",
            "Epoch: 2 || Iter: 160 || Loss: 0.89 || LR: 0.001000\n",
            "Epoch: 2 || Iter: 180 || Loss: 0.90 || LR: 0.001000\n",
            "Epoch: 3 || Iter: 0 || Loss: 0.90 || LR: 0.001000\n",
            "Epoch: 3 || Iter: 20 || Loss: 0.89 || LR: 0.001000\n",
            "Epoch: 3 || Iter: 40 || Loss: 0.89 || LR: 0.001000\n",
            "Epoch: 3 || Iter: 60 || Loss: 0.90 || LR: 0.001000\n",
            "Epoch: 3 || Iter: 80 || Loss: 0.89 || LR: 0.001000\n",
            "Epoch: 3 || Iter: 100 || Loss: 0.89 || LR: 0.001000\n",
            "Epoch: 3 || Iter: 120 || Loss: 0.88 || LR: 0.001000\n",
            "Epoch: 3 || Iter: 140 || Loss: 0.89 || LR: 0.001000\n",
            "Epoch: 3 || Iter: 160 || Loss: 0.89 || LR: 0.001000\n",
            "Epoch: 3 || Iter: 180 || Loss: 0.89 || LR: 0.001000\n",
            "Epoch: 4 || Iter: 0 || Loss: 0.89 || LR: 0.000100\n",
            "Epoch: 4 || Iter: 20 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 4 || Iter: 40 || Loss: 0.90 || LR: 0.000100\n",
            "Epoch: 4 || Iter: 60 || Loss: 0.89 || LR: 0.000100\n",
            "Epoch: 4 || Iter: 80 || Loss: 0.89 || LR: 0.000100\n",
            "Epoch: 4 || Iter: 100 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 4 || Iter: 120 || Loss: 0.89 || LR: 0.000100\n",
            "Epoch: 4 || Iter: 140 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 4 || Iter: 160 || Loss: 0.89 || LR: 0.000100\n",
            "Epoch: 4 || Iter: 180 || Loss: 0.89 || LR: 0.000100\n",
            "Epoch: 5 || Iter: 0 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 5 || Iter: 20 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 5 || Iter: 40 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 5 || Iter: 60 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 5 || Iter: 80 || Loss: 0.89 || LR: 0.000100\n",
            "Epoch: 5 || Iter: 100 || Loss: 0.89 || LR: 0.000100\n",
            "Epoch: 5 || Iter: 120 || Loss: 0.89 || LR: 0.000100\n",
            "Epoch: 5 || Iter: 140 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 5 || Iter: 160 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 5 || Iter: 180 || Loss: 0.89 || LR: 0.000100\n",
            "Epoch: 6 || Iter: 0 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 6 || Iter: 20 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 6 || Iter: 40 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 6 || Iter: 60 || Loss: 0.89 || LR: 0.000100\n",
            "Epoch: 6 || Iter: 80 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 6 || Iter: 100 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 6 || Iter: 120 || Loss: 0.89 || LR: 0.000100\n",
            "Epoch: 6 || Iter: 140 || Loss: 0.89 || LR: 0.000100\n",
            "Epoch: 6 || Iter: 160 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 6 || Iter: 180 || Loss: 0.87 || LR: 0.000100\n",
            "Epoch: 7 || Iter: 0 || Loss: 0.89 || LR: 0.000100\n",
            "Epoch: 7 || Iter: 20 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 7 || Iter: 40 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 7 || Iter: 60 || Loss: 0.89 || LR: 0.000100\n",
            "Epoch: 7 || Iter: 80 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 7 || Iter: 100 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 7 || Iter: 120 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 7 || Iter: 140 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 7 || Iter: 160 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 7 || Iter: 180 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 8 || Iter: 0 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 8 || Iter: 20 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 8 || Iter: 40 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 8 || Iter: 60 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 8 || Iter: 80 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 8 || Iter: 100 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 8 || Iter: 120 || Loss: 0.88 || LR: 0.000100\n",
            "Epoch: 8 || Iter: 140 || Loss: 0.87 || LR: 0.000100\n",
            "Epoch: 8 || Iter: 160 || Loss: 0.87 || LR: 0.000100\n",
            "Epoch: 8 || Iter: 180 || Loss: 0.89 || LR: 0.000100\n",
            "Epoch: 9 || Iter: 0 || Loss: 0.89 || LR: 0.000010\n",
            "Epoch: 9 || Iter: 20 || Loss: 0.89 || LR: 0.000010\n",
            "Epoch: 9 || Iter: 40 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 9 || Iter: 60 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 9 || Iter: 80 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 9 || Iter: 100 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 9 || Iter: 120 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 9 || Iter: 140 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 9 || Iter: 160 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 9 || Iter: 180 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 10 || Iter: 0 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 10 || Iter: 20 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 10 || Iter: 40 || Loss: 0.87 || LR: 0.000010\n",
            "Epoch: 10 || Iter: 60 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 10 || Iter: 80 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 10 || Iter: 100 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 10 || Iter: 120 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 10 || Iter: 140 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 10 || Iter: 160 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 10 || Iter: 180 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 11 || Iter: 0 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 11 || Iter: 20 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 11 || Iter: 40 || Loss: 0.89 || LR: 0.000010\n",
            "Epoch: 11 || Iter: 60 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 11 || Iter: 80 || Loss: 0.89 || LR: 0.000010\n",
            "Epoch: 11 || Iter: 100 || Loss: 0.89 || LR: 0.000010\n",
            "Epoch: 11 || Iter: 120 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 11 || Iter: 140 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 11 || Iter: 160 || Loss: 0.89 || LR: 0.000010\n",
            "Epoch: 11 || Iter: 180 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 12 || Iter: 0 || Loss: 0.89 || LR: 0.000010\n",
            "Epoch: 12 || Iter: 20 || Loss: 0.87 || LR: 0.000010\n",
            "Epoch: 12 || Iter: 40 || Loss: 0.89 || LR: 0.000010\n",
            "Epoch: 12 || Iter: 60 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 12 || Iter: 80 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 12 || Iter: 100 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 12 || Iter: 120 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 12 || Iter: 140 || Loss: 0.87 || LR: 0.000010\n",
            "Epoch: 12 || Iter: 160 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 12 || Iter: 180 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 13 || Iter: 0 || Loss: 0.89 || LR: 0.000010\n",
            "Epoch: 13 || Iter: 20 || Loss: 0.89 || LR: 0.000010\n",
            "Epoch: 13 || Iter: 40 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 13 || Iter: 60 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 13 || Iter: 80 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 13 || Iter: 100 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 13 || Iter: 120 || Loss: 0.87 || LR: 0.000010\n",
            "Epoch: 13 || Iter: 140 || Loss: 0.89 || LR: 0.000010\n",
            "Epoch: 13 || Iter: 160 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 13 || Iter: 180 || Loss: 0.88 || LR: 0.000010\n",
            "Epoch: 14 || Iter: 0 || Loss: 0.88 || LR: 0.000001\n",
            "Epoch: 14 || Iter: 20 || Loss: 0.88 || LR: 0.000001\n",
            "Epoch: 14 || Iter: 40 || Loss: 0.87 || LR: 0.000001\n",
            "Epoch: 14 || Iter: 60 || Loss: 0.88 || LR: 0.000001\n",
            "Epoch: 14 || Iter: 80 || Loss: 0.89 || LR: 0.000001\n",
            "Epoch: 14 || Iter: 100 || Loss: 0.89 || LR: 0.000001\n",
            "Epoch: 14 || Iter: 120 || Loss: 0.88 || LR: 0.000001\n",
            "Epoch: 14 || Iter: 140 || Loss: 0.88 || LR: 0.000001\n",
            "Epoch: 14 || Iter: 160 || Loss: 0.87 || LR: 0.000001\n",
            "Epoch: 14 || Iter: 180 || Loss: 0.88 || LR: 0.000001\n",
            "Epoch: 15 || Iter: 0 || Loss: 0.88 || LR: 0.000001\n",
            "Epoch: 15 || Iter: 20 || Loss: 0.88 || LR: 0.000001\n",
            "Epoch: 15 || Iter: 40 || Loss: 0.88 || LR: 0.000001\n",
            "Epoch: 15 || Iter: 60 || Loss: 0.88 || LR: 0.000001\n",
            "Epoch: 15 || Iter: 80 || Loss: 0.88 || LR: 0.000001\n",
            "Epoch: 15 || Iter: 100 || Loss: 0.89 || LR: 0.000001\n",
            "Epoch: 15 || Iter: 120 || Loss: 0.88 || LR: 0.000001\n",
            "Epoch: 15 || Iter: 140 || Loss: 0.88 || LR: 0.000001\n",
            "Epoch: 15 || Iter: 160 || Loss: 0.88 || LR: 0.000001\n",
            "Epoch: 15 || Iter: 180 || Loss: 0.88 || LR: 0.000001\n",
            "Epoch: 16 || Iter: 0 || Loss: 0.88 || LR: 0.000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57ai4Yfm6OvO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_similarity(target, vocab):\n",
        "  target_V = model.prediction(prepare_word(target, word2index).to(device))\n",
        "\n",
        "  similarities = []\n",
        "  for i in range(len(vocab)):\n",
        "    if vocab[i] == target:\n",
        "      continue\n",
        "\n",
        "    vector = model.prediction(prepare_word(list(vocab)[i], word2index).to(device))\n",
        "\n",
        "    cosine_sim = F.cosine_similarity(target_V, vector).data.tolist()[0]\n",
        "    similarities.append([vocab[i], cosine_sim])\n",
        "  return sorted(similarities, key=lambda x: x[1], reverse=True)[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz2pecA-6r6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = random.choice(list(vocab))\n",
        "print(test)\n",
        "word_similarity(test, vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}